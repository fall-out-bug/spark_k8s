# Почему spark_k8s?

> **Видение:** Production-ready, а не production-hostile.

## Проблема: Два мира, которые не говорят друг с другом

DevOps-команды понимают Kubernetes. Они знают поды, сервисы, ingress, RBAC. Инженеры данных понимают Spark. Они знают RDD, DataFrame, экзекьюторы, shuffle. Но когда Spark разворачивают на Kubernetes, что-то ломается: **разрыв коммуникации**.

Платформенные команды выделяют кластеры. Команды данных пишут джобы. Никто полностью не понимает ограничений другой стороны. Результат: деплои, работающие на ноутбуке, падают в продакшене. Конфигурации, проходящие `helm install`, вызывают OOMKill в 3 ночи. Каждый инцидент превращается в пожарную тревогу — нет общего языка.

**Стоимость реальна:** недели переписки по тикетам, расхождение окружений dev и prod, команды, которые учатся обходить платформу вместо работы с ней.

## Глубокий разбор проблемы: Когда разрыв бьёт

Типичный сценарий. Инженеру данных нужно протестировать изменение пайплайна. Он запрашивает Spark-окружение у платформенной команды. Тикет создан. Через два дня появляется кластер — но настроенный под другой сценарий. Лимиты памяти не соответствуют джобу. S3-бакет не примонтирован. Инженер тратит день на отладку инфраструктуры вместо логики данных.

**Количественная боль:**
- **Время:** 2–5 дней от запроса до рабочего окружения. Циклы итерации растягиваются на недели.
- **Деньги:** Часы платформенного инженера, часы инженера данных, оба ждут. Упущенная выгода от задержки инсайтов.
- **Фрустрация:** «Почему я не могу просто запустить сам?» — вопрос, который мы слышали многократно.

**Реальный паттерн:** Команды данных прибегают к «теневому IT» — запускают Spark на ноутбуках или в неконтролируемых облачных аккаунтах — потому что официальный путь слишком медленный. Платформенная команда потом наследует поддержку конфигураций, которые не проектировала.

## Решение: Подход Lego-конструктора

spark_k8s выбирает другой путь. Вместо монолитного «вот ваш кластер» мы даём **пресетные строительные блоки**, кодирующие лучшие практики.

**Модульность как Lego:** Команды сами комбинируют компоненты. Нужен Jupyter + Spark Connect? Выберите пресет. Нужны Airflow + MLflow? Другой пресет. Нужен OpenShift с PSS restricted? Есть и такой. Каждый пресет протестирован, задокументирован и готов к продакшену.

**Почему важны пресеты:** Они мостят разрыв. Инженер данных разворачивает известную рабочую конфигурацию за минуты. DevOps-инженер видит стандартную, аудируемую настройку. Никакой археологии кастомного YAML. Никакого «у меня работало» — потому что машина определена пресетом.

**Баланс:** Гибкость там, где важно (размер ресурсов, флаги фич), ограничения там, где не важно (дефолты безопасности, наблюдаемость). Self-service без хаоса.

## Видение и дорожная карта

**За пределами Helm-чартов:** spark_k8s стремится быть **Production Operations Framework**. Не только «как развернуть», но и «как оперировать». Day-2 операции: ранбуки, реагирование на инциденты, backup/DR, SLI/SLO мониторинг. См. [ROADMAP.md](../../../ROADMAP.md) для текущего прогресса.

**Сообщество:** Open source, двуязычная (EN/RU) документация, приверженность production-grade дефолтам. Эволюция с обратной связью сообщества — пресеты расширяются, ранбуки растут, платформа зреет.

**Устойчивость:** Поддерживается как демонстрация экспертизы в production data platforms. Контрибьюции приветствуются. Дорожная карта прозрачна; код открыт.

---

**Резюме:** spark_k8s существует, потому что разрыв DevOps/Data реален и дорог. Мы решаем его пресетным деплоем, модульностью и фокусом на production operations. Результат: меньше тикетов, быстрее итерации, команды, которые могут фокусироваться на инсайтах вместо инфраструктуры.
