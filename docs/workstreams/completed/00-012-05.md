---
ws_id: 00-012-05
feature: F12
status: backlog
size: MEDIUM
project_id: 00
github_issue: null
assignee: null
depends_on:
  - 00-006-01  # Phase 0: Helm charts
  - 00-022-01  # Phase 1: Security templates
---

## WS-00-012-05: Standalone E2E (8 scenarios)

### ðŸŽ¯ Goal

**What must WORK after completing this WS:**
- 8 standalone cluster E2E test scenarios with NYC Taxi dataset
- Tests validate Spark 3.5.7, 3.5.8 Ã— Jupyter, Airflow Ã— standalone-submit mode
- Standalone cluster deployment and job submission validation

**Acceptance Criteria:**
- [ ] AC1: 8 standalone E2E scenarios created (Jupyter/Airflow Ã— 3.5.7/3.5.8 Ã— standalone-submit)
- [ ] AC2: Standalone Master/Worker pods deploy correctly
- [ ] AC3: Jobs submit to standalone cluster successfully
- [ ] AC4: Worker scaling validated (dynamic worker addition)
- [ ] AC5: All scenarios complete with timeout < 1200s

**âš ï¸ WS is NOT complete until Goal is achieved (all AC âœ…).**

---

### Context

Standalone E2E tests validate Spark standalone cluster deployment on Kubernetes. This is the traditional Spark deployment mode with Master/Worker architecture, as opposed to the native Kubernetes scheduler mode.

### Dependencies

- Phase 0 (F06): Helm charts (spark-standalone chart)
- Phase 1 (F07): Security templates
- F01: Spark Standalone chart (WS-001-01 through WS-001-12)

### Input Files

- `scripts/tests/e2e/conftest.py` â€” Base E2E fixtures
- `charts/spark-standalone/` â€” Standalone chart
- `scripts/tests/smoke/scenarios/*standalone*.sh` â€” Existing standalone smoke tests

### Steps

1. **Create standalone-specific fixtures:**
   - Add standalone cluster deployment fixture
   - Add Master/Worker pod status check fixture
   - Add worker scaling fixture

2. **Implement standalone E2E test scenarios:**
   - `test_jupyter_standalone_357.py` â€” Jupyter standalone-submit, Spark 3.5.7
   - `test_jupyter_standalone_358.py` â€” Jupyter standalone-submit, Spark 3.5.8
   - `test_airflow_standalone_357.py` â€” Airflow standalone-submit, Spark 3.5.7
   - `test_airflow_standalone_358.py` â€” Airflow standalone-submit, Spark 3.5.8

3. **Add standalone-specific test cases:**
   - Deploy standalone cluster (Master + Workers)
   - Verify Master is ready
   - Verify Workers register with Master
   - Submit job to standalone cluster
   - Verify job executes on Workers
   - Test worker scaling (add worker, verify it registers)

4. **Implement standalone metrics collection:**
   - Master URL connectivity
   - Worker count
   - Executor distribution across workers

5. **Validate locally:**
   - Run pytest on standalone scenarios
   - Verify standalone cluster deploys
   - Check jobs submit and complete

### Code

**Standalone fixture example:**

```python
@pytest.fixture(scope="session")
def standalone_cluster(request, release_name="spark-standalone-e2e"):
    """Deploy Spark standalone cluster."""
    import subprocess
    import time

    # Deploy standalone cluster
    subprocess.run([
        "helm", "upgrade", "--install", release_name,
        "charts/spark-standalone",
        "--namespace", "default",
        "--create-namespace",
        "--set", "spark.worker.replicas=2",
        "--wait", "--timeout", "300s"
    ], check=True)

    # Wait for Master ready
    subprocess.run([
        "kubectl", "wait", "--for=condition=ready",
        "pod", "-l", f"app=spark-master,app.kubernetes.io/instance={release_name}",
        "--namespace", "default", "--timeout=120s"
    ], check=True)

    # Wait for Workers ready
    subprocess.run([
        "kubectl", "wait", "--for=condition=ready",
        "pod", "-l", f"app=spark-worker,app.kubernetes.io/instance={release_name}",
        "--namespace", "default", "--timeout=120s"
    ], check=True)

    yield {
        "master_url": f"spark://{release_name}-spark-master:7077",
        "release": release_name
    }

    # Cleanup
    subprocess.run([
        "helm", "uninstall", release_name,
        "--namespace", "default"
    ], check=False)

@pytest.fixture(scope="function")
def standalone_metrics(standalone_cluster):
    """Collect standalone cluster metrics."""
    import subprocess
    import json

    def get_worker_count():
        result = subprocess.run([
            "kubectl", "get", "pods",
            "-l", f"app=spark-worker,app.kubernetes.io/instance={standalone_cluster['release']}",
            "-o", "json"
        ], capture_output=True, text=True)
        pods = json.loads(result.stdout)
        return len(pods.get("items", []))

    yield {}
    return {"worker_count": get_worker_count()}
```

### Expected Outcome

- Files: 4 test modules
- All 8 scenarios (4 modules Ã— 2 test cases) pass
- Standalone cluster deployment validated

### Scope Estimate

- Files: ~5
- Lines: ~500 (MEDIUM)
- Tokens: ~2000

### Completion Criteria

```bash
# Run standalone E2E tests
pytest scripts/tests/e2e/ -v --timeout=1200 -k "standalone"

# Verify standalone cluster is running
kubectl get pods -l app=spark-worker
```

### Constraints

- DO NOT modify existing standalone chart
- Tests cleanup standalone cluster after completion
- MUST handle resource limits (may need increased Minikube resources)

---

## Execution Report

**Executed by:** ______
**Date:** ______
**Duration:** ______ minutes

### Goal Status
- [ ] AC1-AC5 â€” âœ…

**Goal Achieved:** ______

### Files Changed
| File | Action | LOC |
|------|--------|-----|
|      |        |     |

### Statistics
- **Files Changed:** ______
- **Lines Added:** ______
- **Tests Passed:** ______
- **Tests Failed:** ______

### Deviations from Plan
- ______

### Commit
______
