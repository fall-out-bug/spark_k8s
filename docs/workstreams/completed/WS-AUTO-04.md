# WS-AUTO-04: Helm Values Generator

## Summary
Generate Helm values overlay files from autotuning recommendations.

## Scope
- Convert recommendations to Helm values format
- Generate preset files for different scenarios
- Support both full values and overlays
- Include metadata and provenance

## Acceptance Criteria
- [ ] Generator takes RecommendationSet as input
- [ ] Outputs valid Helm values YAML
- [ ] Supports overlay format (only changed values)
- [ ] Supports full preset format
- [ ] Includes metadata: app_id, confidence, timestamp
- [ ] Validates output with `helm lint`
- [ ] Unit tests for YAML generation
- [ ] CLI interface: `python -m autotuning.applier --recommendations-file <path>`

## Technical Design

### Output Formats

**Overlay Format (recommended):**
```yaml
# charts/spark-3.5/presets/autotuned/nyc-taxi-etl-2026-02-22.yaml
# Generated by spark-autotuner v1.0
# App ID: app-20260222120000-0001
# Confidence: 0.85
# Source: metrics analysis (14.2M records, 45s duration)

# Overrides for NYC Taxi ETL workload
connect:
  executor:
    memory: "6Gi"
    memoryOverhead: "2Gi"
    cores: 3

  sparkConf:
    spark.sql.shuffle.partitions: "400"
    spark.sql.adaptive.advisoryPartitionSizeInBytes: "128MB"
    spark.sql.autoBroadcastJoinThreshold: "104857600"
    spark.memory.fraction: "0.7"
    spark.memory.storageFraction: "0.3"

# Metadata for tracking
autotuning:
  generatedAt: "2026-02-22T12:00:00Z"
  appId: "app-20260222120000-0001"
  confidence: 0.85
  basedOn:
    - "gc_ratio: 0.08 (ok)"
    - "cpu_util: 0.55 (low -> reduced cores)"
    - "shuffle_bytes: 2.1GB (increased partitions)"
```

**Full Preset Format:**
```yaml
# charts/spark-3.5/presets/autotuned/full/nyc-taxi-etl-optimized.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-autotuned-values
  labels:
    autotuning/generated: "true"
    autotuning/app-id: "app-20260222120000-0001"
data:
  values.yaml: |
    # ... full Helm values
```

### Mapping Rules

```python
RECOMMENDATION_TO_HELM = {
    # Spark config -> Helm path
    "spark.executor.memory": "connect.executor.memory",
    "spark.executor.memoryOverhead": "connect.executor.memoryOverhead",
    "spark.executor.cores": "connect.executor.cores",
    "spark.executor.instances": "connect.replicas",
    "spark.sql.shuffle.partitions": "connect.sparkConf.spark.sql.shuffle.partitions",
    "spark.sql.autoBroadcastJoinThreshold": "connect.sparkConf.spark.sql.autoBroadcastJoinThreshold",
    "spark.memory.fraction": "connect.sparkConf.spark.memory.fraction",
    "spark.memory.storageFraction": "connect.sparkConf.spark.memory.storageFraction",
    "spark.sql.adaptive.enabled": "connect.sparkConf.spark.sql.adaptive.enabled",
    "spark.sql.adaptive.skewJoin.enabled": "connect.sparkConf.spark.sql.adaptive.skewJoin.enabled",
}
```

### Validation

```python
def validate_helm_values(values_path: str) -> ValidationResult:
    """Validate generated Helm values."""
    # 1. YAML syntax check
    # 2. Required fields check
    # 3. Type validation
    # 4. Helm lint (optional, requires chart)

    result = ValidationResult()

    try:
        with open(values_path) as f:
            values = yaml.safe_load(f)
    except yaml.YAMLError as e:
        result.add_error(f"YAML syntax error: {e}")
        return result

    # Check required structure
    if "connect" not in values:
        result.add_warning("Missing 'connect' section")

    # Validate memory format
    if memory := values.get("connect", {}).get("executor", {}).get("memory"):
        if not re.match(r"^\d+[GM]i$", memory):
            result.add_error(f"Invalid memory format: {memory}")

    return result
```

### File Structure
```
scripts/autotuning/
├── applier.py
├── templates/
│   ├── overlay.yaml.j2
│   └── full-preset.yaml.j2
└── tests/
    ├── test_applier.py
    └── fixtures/
        └── sample_recommendations.json

charts/spark-3.5/presets/autotuned/
├── README.md
├── overlays/
│   ├── nyc-taxi-etl-2026-02-22.yaml
│   └── ...
└── full/
    └── ...
```

### CLI Interface

```bash
# Generate overlay
python -m autotuning.applier \
  --recommendations-file /tmp/recommendations.json \
  --output-format overlay \
  --output-dir charts/spark-3.5/presets/autotuned/overlays/

# Generate full preset
python -m autotuning.applier \
  --recommendations-file /tmp/recommendations.json \
  --output-format full \
  --chart-path charts/spark-3.5 \
  --validate

# Dry-run with helm template
python -m autotuning.applier \
  --recommendations-file /tmp/recommendations.json \
  --dry-run \
  --helm-cmd "helm template test charts/spark-3.5 -f -"
```

## Definition of Done
- Code reviewed and merged
- Tests passing (coverage >= 80%)
- Both output formats working
- Validation implemented
- Documentation in docstrings

## Estimated Effort
Small (YAML generation, straightforward mapping)

## Blocked By
- WS-AUTO-03 (Rule-based Recommender)

## Blocks
- WS-AUTO-05 (Grafana Dashboard)
