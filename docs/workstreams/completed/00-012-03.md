---
ws_id: 00-012-03
feature: F12
status: backlog
size: MEDIUM
project_id: 00
github_issue: null
assignee: null
depends_on:
  - 00-006-01  # Phase 0: Helm charts
  - 00-006-07  # Phase 0: Iceberg feature templates
  - 00-022-01  # Phase 1: Security templates
---

## WS-00-012-03: Iceberg E2E (16 scenarios)

### ðŸŽ¯ Goal

**What must WORK after completing this WS:**
- 16 Iceberg E2E test scenarios with NYC Taxi dataset
- Tests validate Spark 3.5.7, 3.5.8, 4.1.0, 4.1.1 Ã— Airflow Ã— connect-k8s Ã— Iceberg
- Apache Iceberg table operations (CREATE, READ, UPDATE, DELETE)

**Acceptance Criteria:**
- [ ] AC1: 16 Iceberg E2E scenarios created (4 Spark versions Ã— Airflow Ã— connect-k8s Ã— Iceberg)
- [ ] AC2: Iceberg catalog operations validated (CREATE TABLE, DROP TABLE)
- [ ] AC3: Iceberg table operations validated (INSERT, UPDATE, DELETE, MERGE)
- [ ] AC4: Time travel queries work (snapshot-based reads)
- [ ] AC5: Schema evolution validated (ADD COLUMN, DROP COLUMN)
- [ ] AC6: All scenarios complete with timeout < 1200s

**âš ï¸ WS is NOT complete until Goal is achieved (all AC âœ…).**

---

### Context

Iceberg E2E tests validate Apache Iceberg table format functionality. Tests use Spark's Iceberg integration to verify table operations, time travel, and schema evolution across different Spark versions.

### Dependencies

- Phase 0 (F06): Iceberg feature templates
- Phase 1 (F07): Security templates
- Phase 5 (F11): Iceberg runtime images

### Input Files

- `scripts/tests/e2e/conftest.py` â€” Base E2E fixtures
- `charts/spark-3.5/` and `charts/spark-4.1/` â€” Charts with Iceberg support

### Steps

1. **Create Iceberg-specific fixtures:**
   - Add Iceberg catalog fixture (Hive backend or REST)
   - Add Iceberg table initialization fixture
   - Add warehouse directory fixture

2. **Implement Iceberg E2E test scenarios:**
   - `test_airflow_iceberg_357.py` â€” Airflow with Iceberg, Spark 3.5.7
   - `test_airflow_iceberg_358.py` â€” Airflow with Iceberg, Spark 3.5.8
   - `test_airflow_iceberg_410.py` â€” Airflow with Iceberg, Spark 4.1.0
   - `test_airflow_iceberg_411.py` â€” Airflow with Iceberg, Spark 4.1.1

3. **Add Iceberg-specific test cases:**
   - Create Iceberg table from NYC Taxi data
   - Test time travel: `df.select("*").asOf("timestamp")`
   - Test schema evolution: ADD/DROP COLUMN
   - Test MERGE operation (upsert)
   - Test partition evolution

4. **Implement Iceberg metrics collection:**
   - Table snapshot count
   - Partition count
   - Schema version

5. **Validate locally:**
   - Run pytest on Iceberg scenarios
   - Verify Iceberg tables are created in warehouse
   - Check time travel queries return correct historical data

### Code

**Iceberg fixture example:**

```python
@pytest.fixture(scope="function")
def iceberg_table(spark_session, dataset_path):
    """Create Iceberg table from NYC Taxi dataset."""
    import tempfile
    warehouse = tempfile.mkdtemp(prefix="iceberg_warehouse_")

    # Configure Iceberg
    spark_session.conf.set("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
    spark_session.conf.set("spark.sql.catalog.local.type", "hadoop")
    spark_session.conf.set("spark.sql.catalog.local.warehouse", warehouse)
    spark_session.conf.set("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")

    # Create Iceberg table
    df = spark_session.read.parquet(dataset_path)
    df.writeTo("local.nyc_taxi").using("iceberg").create()

    yield {"table": "local.nyc_taxi", "warehouse": warehouse}

    # Cleanup
    import shutil
    shutil.rmtree(warehouse, ignore_errors=True)

@pytest.fixture(scope="function")
def iceberg_metrics(spark_session):
    """Collect Iceberg table metrics."""
    def get_metrics(table_name):
        return {
            "snapshots": spark_session.sql(f"SELECT * FROM {table_name}.snapshots").count(),
            "schema_version": spark_session.sql(f"DESCRIBE {table_name}").count()
        }
    yield {}
    return get_metrics
```

### Expected Outcome

- Files: 4 test modules
- All 16 scenarios (4 modules Ã— 4 test cases) pass
- Iceberg operations validated across all Spark versions

### Scope Estimate

- Files: ~5
- Lines: ~900 (MEDIUM)
- Tokens: ~3500

### Completion Criteria

```bash
# Run Iceberg E2E tests
pytest scripts/tests/e2e/ -v --timeout=1200 -k "iceberg"

# Verify Iceberg tables created
# Check warehouse directory for metadata
```

### Constraints

- DO NOT modify existing Iceberg charts
- Tests skip gracefully if Iceberg not available
- MUST clean up warehouse directories after tests

---

## Execution Report

**Executed by:** ______
**Date:** ______
**Duration:** ______ minutes

### Goal Status
- [ ] AC1-AC6 â€” âœ…

**Goal Achieved:** ______

### Files Changed
| File | Action | LOC |
|------|--------|-----|
|      |        |     |

### Statistics
- **Files Changed:** ______
- **Lines Added:** ______
- **Tests Passed:** ______
- **Tests Failed:** ______

### Deviations from Plan
- ______

### Commit
______
