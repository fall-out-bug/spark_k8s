---
ws_id: 026-03
feature: F26
status: completed
size: MEDIUM
project_id: 00
github_issue: null
assignee: null
depends_on: ["026-01"]
---

## WS-026-03: Structured Streaming Example (Kafka to Iceberg)

### Goal

**What must WORK after completing this WS:**
- Working Structured Streaming example: Kafka source → transformation → Iceberg sink
- Docker Compose addition for local Kafka broker
- Jupyter notebook demonstrating streaming query
- Documentation for real-time use case

**Acceptance Criteria:**
- [ ] AC1: `examples/streaming/kafka_to_iceberg.py` PySpark streaming job created
- [ ] AC2: `examples/streaming/docker-compose.yaml` adds Kafka + Schema Registry
- [ ] AC3: `examples/streaming/README.md` documents setup and execution
- [ ] AC4: Jupyter notebook `notebooks/recipes/streaming-quickstart.ipynb` created
- [ ] AC5: Streaming job runs for 60s, processes ≥100 records, writes to Iceberg
- [ ] AC6: Checkpoint location configured for exactly-once semantics
- [ ] AC7: Example works with both Spark 3.5 and 4.1

### Files Changed

- `examples/streaming/kafka_to_iceberg.py` (NEW)
- `examples/streaming/docker-compose.yaml` (NEW)
- `examples/streaming/README.md` (NEW)
- `notebooks/recipes/streaming-quickstart.ipynb` (NEW)
