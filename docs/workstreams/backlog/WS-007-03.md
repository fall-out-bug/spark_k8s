# WS-007-03: Iceberg Load Tests

## Goal
Создать 4 Iceberg load тестов для проверки производительности Apache Iceberg table operations при sustained load (30 минут).

### Acceptance Criteria
1. 4 Iceberg load тестов созданы в `tests/load/phase07/test_07_03_iceberg_load.py`
2. Sustained load duration: 30 минут
3. Iceberg table operations проверены при нагрузке
4. Load profile: sustained INSERT, UPDATE, DELETE, MERGE
5. Iceberg метрики собираются (scan time, snapshot size, file pruning)
6. Snapshot stability проверяется (no corruption over 30 min)

## Context

**Существующая инфраструктура:**
- `tests/load/phase07/test_07_01_baseline_load.py` — baseline load тесты
- `tests/e2e/phase06/test_06_03_iceberg_e2e.py` — Iceberg E2E тесты

**Что нужно добавить:**
- Iceberg-specific load testing
- Sustained Iceberg write operations
- Snapshot management under load
- File pruning metrics
- Отдельный файл `tests/load/phase07/test_07_03_iceberg_load.py`

## Dependency
Phase 0 (Helm Charts), Phase 5 (Iceberg Final Images), Phase 6 (E2E Tests)

## Input Files
- `tests/load/phase07/conftest.py` (shared fixtures)
- `tests/load/phase07/test_07_01_baseline_load.py` (reference)
- `charts/spark-3.5/values.yaml`
- `charts/spark-4.1/values.yaml`

## Steps

### 1. Create test_07_03_iceberg_load.py
- Создать `tests/load/phase07/test_07_03_iceberg_load.py`
- Implement 4 test scenarios

### 2. Implement test scenarios

**Iceberg Load Scenarios:**

1. **Spark 3.5.7 Iceberg — Sustained INSERT (30 min)**
   - Continuous INSERT INTO Iceberg table
   - 10 inserts/second for 30 minutes
   - Validate: snapshot consistency, file growth

2. **Spark 3.5.8 Iceberg — Sustained MERGE (UPSERT) (30 min)**
   - Continuous MERGE INTO Iceberg table
   - 5 merges/second for 30 minutes
   - Validate: no duplicate keys, correct updates

3. **Spark 4.1.0 Iceberg — Sustained UPDATE/DELETE (30 min)**
   - Alternate UPDATE and DELETE operations
   - 5 operations/second for 30 minutes
   - Validate: snapshot management, rollback available

4. **Spark 4.1.1 Iceberg — Sustained TIME TRAVEL (30 min)**
   - Continuous queries with historical snapshots
   - 10 time-travel queries/second for 30 minutes
   - Validate: snapshot availability, consistent results

### 3. Iceberg Metrics Collection
- iceberg_snapshot_count: количество snapshots
- iceberg_scan_time: время scan операций
- iceberg_files_pruned: количество пропущенных файлов
- iceberg_table_size: размер таблицы
- iceberg_writes_per_second: скорость записи

### 4. Run tests
- `pytest tests/load/phase07/test_07_03_iceberg_load.py -v --timeout=2400`

## Code

### tests/load/phase07/test_07_03_iceberg_load.py

```python
"""Iceberg Load Tests for Phase 7

Load tests for Apache Iceberg table operations under sustained load.
"""

import pytest
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

class TestIcebergLoad:
    """Iceberg load tests for sustained performance"""

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    @pytest.mark.iceberg
    def test_01_iceberg_sustained_insert(self, spark_load_session, load_metrics, load_duration_seconds):
        """Spark 3.5.7 Iceberg — Sustained INSERT (30 min)"""
        # Configure Iceberg
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog.type", "hadoop")
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog.warehouse", "/tmp/iceberg/warehouse")

        # Create source data
        source_df = spark_load_session.range(1, 1000000)
        source_df.createOrReplaceTempView("source_data")

        # Create Iceberg table
        spark_load_session.sql("""
            CREATE TABLE IF NOT EXISTS spark_catalog.load_test.insert_table
            USING iceberg
            AS SELECT * FROM source_data
        """)

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        insert_count = 0
        error_count = 0

        batch_size = 10000
        batch_id = 0

        while time.time() < end_time:
            try:
                # INSERT batch
                batch_df = spark_load_session.range(batch_id * batch_size, (batch_id + 1) * batch_size)
                batch_df.writeTo("spark_catalog.load_test.insert_table") \
                    .append()
                insert_count += 1
                batch_id += 1

                load_metrics("test_iceberg_01_inserts", insert_count)
                time.sleep(0.1)  # 10 inserts/second

            except Exception as e:
                error_count += 1
                load_metrics("test_iceberg_01_error", str(e))
                if error_count > 50:
                    pytest.fail(f"Too many errors: {error_count}")

        # Verify table
        final_count = spark_load_session.sql("SELECT COUNT(*) FROM spark_catalog.load_test.insert_table")
        rows = final_count.collect()

        assert insert_count > 10000, f"Should have at least 10000 inserts, got {insert_count}"
        assert rows[0]['count'] > 0, "Table should have data"

        load_metrics("test_iceberg_01_total_inserts", insert_count)
        load_metrics("test_iceberg_01_final_row_count", rows[0]['count'])

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    @pytest.mark.iceberg
    def test_02_iceberg_sustained_merge(self, spark_load_session, load_metrics, load_duration_seconds):
        """Spark 3.5.8 Iceberg — Sustained MERGE/UPSERT (30 min)"""
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog.type", "hadoop")

        # Create Iceberg table with primary key
        spark_load_session.sql("""
            CREATE TABLE IF NOT EXISTS spark_catalog.load_test.merge_table
            (id BIGINT, value BIGINT)
            USING iceberg
            PARTITIONED BY (value)
        """)

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        merge_count = 0
        error_count = 0

        while time.time() < end_time:
            try:
                # MERGE (UPSERT)
                update_data = spark_load_session.range(merge_count * 100, (merge_count + 1) * 100)
                update_data.createOrReplaceTempView("updates")

                spark_load_session.sql("""
                    MERGE INTO spark_catalog.load_test.merge_table AS target
                    USING updates AS source
                    ON target.id = source.id
                    WHEN MATCHED THEN UPDATE SET target.value = source.value
                    WHEN NOT MATCHED THEN INSERT *
                """)

                merge_count += 1
                load_metrics("test_iceberg_02_merges", merge_count)
                time.sleep(0.2)  # 5 merges/second

            except Exception as e:
                error_count += 1
                load_metrics("test_iceberg_02_error", str(e))
                if error_count > 30:
                    pytest.fail(f"Too many errors: {error_count}")

        assert merge_count > 5000, f"Should have at least 5000 merges, got {merge_count}"

        load_metrics("test_iceberg_02_total_merges", merge_count)

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    @pytest.mark.iceberg
    def test_03_iceberg_sustained_update_delete(self, spark_load_session, load_metrics, load_duration_seconds):
        """Spark 4.1.0 Iceberg — Sustained UPDATE/DELETE (30 min)"""
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog.type", "hadoop")

        # Create table with data
        spark_load_session.range(1, 100000).writeTo("spark_catalog.load_test.ud_table").append()

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        operation_count = 0
        error_count = 0

        while time.time() < end_time:
            try:
                # UPDATE
                if operation_count % 2 == 0:
                    spark_load_session.sql("""
                        UPDATE spark_catalog.load_test.ud_table
                        SET id = id * 2
                        WHERE id < 1000
                    """)
                # DELETE
                else:
                    spark_load_session.sql("""
                        DELETE FROM spark_catalog.load_test.ud_table
                        WHERE id < 100
                    """)

                operation_count += 1
                load_metrics("test_iceberg_03_operations", operation_count)
                time.sleep(0.2)  # 5 operations/second

            except Exception as e:
                error_count += 1
                load_metrics("test_iceberg_03_error", str(e))
                if error_count > 30:
                    pytest.fail(f"Too many errors: {error_count}")

        assert operation_count > 5000

        load_metrics("test_iceberg_03_total_operations", operation_count)

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    @pytest.mark.iceberg
    def test_04_iceberg_sustained_time_travel(self, spark_load_session, load_metrics, load_duration_seconds):
        """Spark 4.1.1 Iceberg — Sustained TIME TRAVEL (30 min)"""
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog.type", "hadoop")

        # Create table
        spark_load_session.range(1, 10000).writeTo("spark_catalog.load_test.tt_table").append()

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        query_count = 0
        error_count = 0

        # Get snapshots
        snapshots_df = spark_load_session.sql("""
            SELECT snapshot_id FROM spark_catalog.load_test.tt_table.snapshots
            ORDER BY committed_at
        """)
        snapshots = snapshots_df.collect()

        while time.time() < end_time:
            try:
                # Query current snapshot
                df = spark_load_session.table("spark_catalog.load_test.tt_table")
                count = df.count()

                # If multiple snapshots, query historical
                if len(snapshots) > 1:
                    snapshot_id = snapshots[-1]['snapshot_id']
                    historical_df = spark_load_session.table(f"spark_catalog.load_test.tt_table VERSION AS {snapshot_id}")
                    historical_count = historical_df.count()

                query_count += 1
                load_metrics("test_iceberg_04_queries", query_count)
                time.sleep(0.1)  # 10 queries/second

            except Exception as e:
                error_count += 1
                load_metrics("test_iceberg_04_error", str(e))
                if error_count > 50:
                    pytest.fail(f"Too many errors: {error_count}")

        assert query_count > 10000

        load_metrics("test_iceberg_04_total_queries", query_count)
        load_metrics("test_iceberg_04_snapshot_count", len(snapshots))
```

## Scope Estimate
- Files: 1 (test_07_03_iceberg_load.py)
- LOC: ~600 (tests + Iceberg logic)
- Scenarios: 4
- Duration: 30 min × 4 = 2 hours execution time
- Size: MEDIUM
