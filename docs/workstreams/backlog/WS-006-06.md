# WS-006-06: Library Compatibility Tests

## Goal
Создать 8 library compatibility E2E тестов для проверки совместимости популярных Python библиотек с Spark на Kubernetes.

### Acceptance Criteria
1. 8 library compatibility тестов созданы в `tests/e2e/phase06/test_06_06_library_compat.py`
2. Библиотеки проверены: pandas, numpy, scikit-learn, pyarrow, delta-lake, hudi
3. Spark + pandas interoperability проверен (pandas API on Spark)
4. Spark + MLlib (scikit-learn compatible API)
5. Arrow serialization проверена
6. Delta Lake operations проверены
7. Apache Hudi operations проверены
8. Version compatibility matrix создана

## Context

**Существующая инфраструктура:**
- `tests/e2e/phase06/conftest.py` — shared fixtures
- `charts/spark-3.5/templates/jupyter.yaml` — Jupyter с библиотеками
- Docker images с Python библиотеками из Phase 5

**Что нужно добавить:**
- Library compatibility E2E тесты
- Pandas API on Spark тесты
- MLlib compatibility тесты
- Delta Lake/Hudi тесты
- Version matrix
- Отдельный файл `tests/e2e/phase06/test_06_06_library_compat.py`

## Dependency
Phase 0 (Helm Charts), Phase 5 (Final Images with libraries)

## Input Files
- `tests/e2e/phase06/conftest.py` (shared fixtures)
- `charts/spark-3.5/values.yaml`
- `charts/spark-4.1/values.yaml`

## Steps

### 1. Create test_06_06_library_compat.py
- Создать `tests/e2e/phase06/test_06_06_library_compat.py`
- Implement 8 test scenarios

### 2. Implement test scenarios

**Library Compatibility Scenarios:**

**Pandas API on Spark:**
1. Spark 3.5.7 — pandas API compatibility
2. Spark 3.5.8 — pandas-on-Spark operations

**Arrow Serialization:**
3. Spark 4.1.0 — Arrow serialization (Pandas <-> Spark)
4. Spark 4.1.1 — Arrow memory format

**MLlib + Scikit-learn:**
5. Spark 3.5.7 — MLlib basic operations

**Delta Lake:**
6. Spark 3.5.8 — Delta Lake CRUD operations

**Apache Hudi:**
7. Spark 4.1.0 — Hudi table operations

**Version Matrix:**
8. All versions — library version compatibility

**Operations:**
- pandas_api: ps.DataFrame vs pd.DataFrame
- arrow_serde: Pandas <-> Spark conversion
- mllib_basic: LinearRegression, LogisticRegression
- delta_lake: CREATE, READ, UPDATE, DELETE
- hudi_table: INSERT, UPGRADE, COMPACT
- version_check: проверка версий библиотек

### 3. Library Metrics Collection
- library_version: версии проверенных библиотек
- compat_score: оценка совместимости (pass/fail)
- conversion_time: время преобразования Pandas <-> Spark
- serialization_size: размер serialized данных

### 4. Run tests
- `pytest tests/e2e/phase06/test_06_06_library_compat.py -v`
- Убедиться что все library тесты проходят

## Code

### tests/e2e/phase06/test_06_06_library_compat.py

```python
"""Library Compatibility Tests for Phase 6

Tests for Python library compatibility with Spark on Kubernetes.
"""

import pytest
import time
import pyspark.pandas as ps
import pandas as pd
import numpy as np
from pyspark.sql import SparkSession

class TestLibraryCompatibility:
    """Library compatibility tests for Spark ecosystem"""

    @pytest.mark.timeout(600)
    def test_01_spark_357_pandas_api_compatibility(self, spark_connect_session, metrics_collector):
        """Spark 3.5.7 — Pandas API on Spark compatibility"""
        start_time = time.time()

        # Create pandas-on-Spark DataFrame
        psdf = ps.DataFrame({
            'passenger_count': [1, 2, 3, 4, 5],
            'trip_distance': [1.5, 2.5, 3.5, 4.5, 5.5],
            'total_amount': [10.0, 20.0, 30.0, 40.0, 50.0]
        })

        # Execute pandas-like operations
        result = psdf.groupby('passenger_count').agg({
            'trip_distance': 'mean',
            'total_amount': 'sum'
        })

        # Convert to pandas
        pdf_result = result.to_pandas()

        execution_time = time.time() - start_time

        assert len(pdf_result) > 0, "Pandas API should work"
        assert 'passenger_count' in pdf_result.columns, "GroupBy result should have columns"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "pandas_on_spark", True)

    @pytest.mark.timeout(600)
    def test_02_spark_358_pandas_operations(self, spark_connect_session, metrics_collector):
        """Spark 3.5.8 — Pandas-on-Spark operations"""
        start_time = time.time()

        # Test various pandas operations
        psdf = ps.DataFrame({
            'a': range(1000),
            'b': range(1000, 2000)
        })

        # Filter
        filtered = psdf[psdf['a'] > 500]

        # Sort
        sorted_df = psdf.sort_values(by='a')

        # Merge
        psdf2 = ps.DataFrame({'a': range(500), 'c': range(2000, 2500)})
        merged = psdf.merge(psdf2, on='a', how='inner')

        execution_time = time.time() - start_time

        assert len(merged) > 0, "Merge should work"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "operations_count", 4)

    @pytest.mark.timeout(600)
    def test_03_spark_410_arrow_serialization(self, spark_connect_session, metrics_collector):
        """Spark 4.1.0 — Arrow serialization (Pandas <-> Spark)"""
        start_time = time.time()

        # Enable Arrow
        spark_connect_session.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")

        # Create Spark DataFrame
        df = spark_connect_session.createDataFrame([
            (1, 10.0, "a"),
            (2, 20.0, "b"),
            (3, 30.0, "c")
        ], ["id", "value", "label"])

        # Convert to Pandas with Arrow
        pdf = df.toPandas()

        # Convert back to Spark
        df_back = spark_connect_session.createDataFrame(pdf)

        execution_time = time.time() - start_time

        assert df_back.count() == 3, "Round-trip conversion should work"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "arrow_enabled", True)

    @pytest.mark.timeout(600)
    def test_04_spark_411_arrow_memory_format(self, spark_connect_session, metrics_collector):
        """Spark 4.1.1 — Arrow memory format"""
        start_time = time.time()

        spark_connect_session.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")

        # Create large DataFrame
        import pandas as pd
        pdf = pd.DataFrame({
            'id': range(100000),
            'value': np.random.rand(100000)
        })

        # Convert with Arrow
        df = spark_connect_session.createDataFrame(pdf)
        count = df.count()

        execution_time = time.time() - start_time

        assert count == 100000, "Arrow should handle large data"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "row_count", count)

    @pytest.mark.timeout(600)
    def test_05_spark_357_mllib_basic(self, spark_connect_session, metrics_collector):
        """Spark 3.5.7 — MLlib basic operations"""
        start_time = time.time()

        from pyspark.ml.regression import LinearRegression
        from pyspark.ml.feature import VectorAssembler
        from pyspark.sql.linalg import Vectors

        # Create training data
        data = [(Vectors.dense([1.0, 2.0]), 3.0),
                (Vectors.dense([2.0, 3.0]), 5.0),
                (Vectors.dense([3.0, 4.0]), 7.0)]
        df = spark_connect_session.createDataFrame(data, ["features", "label"])

        # Train model
        lr = LinearRegression(featuresCol="features", labelCol="label")
        model = lr.fit(df)

        # Predict
        test_data = spark_connect_session.createDataFrame(
            [(Vectors.dense([4.0, 5.0]),)],
            ["features"]
        )
        predictions = model.transform(test_data)

        execution_time = time.time() - start_time

        assert predictions.count() == 1, "MLlib should work"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "mllib", True)

    @pytest.mark.timeout(900)
    def test_06_spark_358_delta_lake_crud(self, spark_connect_session, metrics_collector):
        """Spark 3.5.8 — Delta Lake CRUD operations"""
        start_time = time.time()

        # Configure Delta Lake
        spark_connect_session.conf.set("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        spark_connect_session.conf.set("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

        # CREATE
        data = spark_connect_session.range(0, 1000)
        data.write.format("delta").save("/tmp/delta_table")

        # READ
        df = spark_connect_session.read.format("delta").load("/tmp/delta_table")
        count = df.count()

        # UPDATE
        df.filter("id > 500").write.format("delta").mode("overwrite").save("/tmp/delta_table")

        # DELETE (via vacuum)
        spark_connect_session.sql("VACUUM `/tmp/delta_table` RETAIN 0 HOURS")

        execution_time = time.time() - start_time

        assert count > 0, "Delta Lake CRUD should work"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "delta_lake", True)

    @pytest.mark.timeout(900)
    def test_07_spark_410_hudi_table_operations(self, spark_connect_session, metrics_collector):
        """Spark 4.1.0 — Apache Hudi table operations"""
        start_time = time.time()

        # Configure Hudi
        spark_connect_session.conf.set("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension")
        spark_connect_session.conf.set("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.hudi.catalog.HoodieCatalog")

        # CREATE Hudi table
        data = spark_connect_session.range(0, 1000)
        data.write.format("hudi") \
            .option("primaryKey", "id") \
            .option("preCombineField", "id") \
            .mode("overwrite") \
            .save("/tmp/hudi_table")

        # READ
        df = spark_connect_session.read.format("hudi").load("/tmp/hudi_table")
        count = df.count()

        # INSERT (UPSERT)
        new_data = spark_connect_session.range(1000, 1500)
        new_data.write.format("hudi") \
            .option("primaryKey", "id") \
            .option("recordKeyField", "id") \
            .mode("append") \
            .save("/tmp/hudi_table")

        execution_time = time.time() - start_time

        assert count > 0, "Hudi operations should work"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "hudi", True)

    @pytest.mark.timeout(600)
    def test_08_library_version_matrix(self, spark_connect_session, metrics_collector):
        """All versions — Library version compatibility matrix"""
        import importlib

        start_time = time.time()

        # Check library versions
        libraries = {
            'pandas': None,
            'numpy': None,
            'pyarrow': None,
            'pyspark': None,
            'sklearn': 'scikit-learn'
        }

        versions = {}
        for lib_name, import_name in libraries.items():
            try:
                module = importlib.import_module(import_name or lib_name)
                version = getattr(module, '__version__', 'unknown')
                versions[lib_name] = version
            except ImportError:
                versions[lib_name] = 'not installed'

        execution_time = time.time() - start_time

        # Log versions
        for lib, ver in versions.items():
            metrics_collector("version_matrix", lib, ver)

        # Check minimum requirements
        assert versions['pyspark'] != 'not installed', "PySpark must be installed"
        assert versions['pandas'] != 'not installed', "Pandas must be installed"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "libraries_checked", len(versions))
```

### Version Compatibility Matrix

**Expected Library Versions:**

| Library | Spark 3.5.7 | Spark 3.5.8 | Spark 4.1.0 | Spark 4.1.1 |
|---------|-------------|-------------|-------------|-------------|
| pandas | 2.0.x | 2.0.x | 2.1.x | 2.1.x |
| numpy | 1.24.x | 1.24.x | 1.26.x | 1.26.x |
| pyarrow | 12.x | 14.x | 14.x | 15.x |
| scikit-learn | 1.3.x | 1.3.x | 1.4.x | 1.5.x |
| delta-lake | 2.4.x | 3.0.x | 3.1.x | 3.2.x |
| hudi | 0.14.x | 0.15.x | 0.15.x | 0.16.x |

## Scope Estimate
- Files: 1 (test_06_06_library_compat.py)
- LOC: ~600 (tests + library-specific logic)
- Scenarios: 8
- Size: MEDIUM
