# WS-007-01: Baseline Load Tests

## Goal
Создать 4 baseline load тестов для проверки производительности Spark при sustained load (30 минут) на NYC Taxi dataset.

### Acceptance Criteria
1. 4 baseline load тестов созданы в `tests/load/phase07/test_07_01_baseline_load.py`
2. Sustained load duration: 30 минут
3. NYC Taxi dataset используется для load тестов
4. Load profile: sustained (constant load)
5. Метрики производительности собираются (throughput, latency, resources)
6. Resource limits проверяются (CPU, memory)
7. Стабильность подтверждается (no failures over 30 min)
8. Baseline установлены для comparison

## Context

**Существующая инфраструктура:**
- `tests/e2e/phase06/test_06_01_core_e2e.py` — core E2E тесты (можно использовать как base)
- `scripts/run-smoke-tests.sh` — базовые тестовые скрипты
- NYC Taxi dataset доступен

**Что нужно добавить:**
- Load testing framework (30 min sustained)
- Sustained load pattern (constant query rate)
- Performance metrics collection
- Resource monitoring
- Stability validation
- Отдельный файл `tests/load/phase07/test_07_01_baseline_load.py`

## Dependency
Phase 0 (Helm Charts), Phase 6 (E2E Tests)

## Input Files
- `tests/e2e/phase06/conftest.py` (shared fixtures reference)
- `tests/e2e/phase06/test_06_01_core_e2e.py` (reference)
- `charts/spark-3.5/values.yaml`
- `charts/spark-4.1/values.yaml`

## Steps

### 1. Create Phase 7 Load conftest.py
- Создать `tests/load/phase07/conftest.py`
- Добавить fixtures для load testing (30 min duration)
- Добавить fixtures для metrics collection (throughput, latency)
- Добавить fixtures для resource monitoring

### 2. Create test_07_01_baseline_load.py
- Создать `tests/load/phase07/test_07_01_baseline_load.py`
- Implement 4 test scenarios

### 3. Implement test scenarios

**Baseline Load Scenarios:**

1. **Spark 3.5.7 Baseline — Sustained SELECT (30 min)**
   - Constant execution of SELECT COUNT(*) queries
   - 1 query/second for 30 minutes
   - Validate: no failures, consistent throughput

2. **Spark 3.5.8 Baseline — Sustained GROUP BY (30 min)**
   - Constant execution of GROUP BY aggregation
   - 1 query/second for 30 minutes
   - Validate: no failures, consistent latency

3. **Spark 4.1.0 Baseline — Sustained JOIN (30 min)**
   - Constant execution of JOIN queries
   - 0.5 query/second for 30 minutes
   - Validate: no failures, no memory leaks

4. **Spark 4.1.1 Baseline — Sustained Mixed (30 min)**
   - Mixed workload (COUNT, GROUP BY, JOIN, Window)
   - Random sequence for 30 minutes
   - Validate: no failures, predictable performance

### 4. Load Metrics Collection
- throughput_total: total queries executed
- throughput_avg: average queries/second
- latency_p50: median query latency
- latency_p95: 95th percentile latency
- latency_p99: 99th percentile latency
- error_rate: percentage of failed queries
- memory_used: peak memory consumption
- cpu_used: average CPU usage

### 5. Run tests
- `pytest tests/load/phase07/test_07_01_baseline_load.py -v --timeout=2400`
- Убедиться что все load тесты проходят

## Code

### tests/load/phase07/conftest.py

```python
"""Shared fixtures for Phase 7 Load Tests"""

import pytest
import time
import threading
from pathlib import Path
from typing import Dict, Any, List
from collections import defaultdict

@pytest.fixture(scope="session")
def load_duration_seconds():
    """Duration for load tests (default 30 minutes)"""
    return 30 * 60  # 30 minutes

@pytest.fixture(scope="session")
def project_root() -> Path:
    """Project root directory"""
    return Path(__file__).parent.parent.parent.parent

@pytest.fixture(scope="session")
def nyc_taxi_dataset_path(project_root):
    """Path to NYC Taxi dataset"""
    return project_root / "tests" / "data" / "nyc_taxi" / "full"

@pytest.fixture
def load_metrics():
    """Metrics collector for load tests"""
    metrics = defaultdict(list)
    lock = threading.Lock()

    def record_metric(metric_name: str, value: Any):
        with lock:
            metrics[metric_name].append({
                'value': value,
                'timestamp': time.time()
            })

    yield record_metric

    # Print summary at end
    print("\n=== Load Test Metrics Summary ===")
    for metric_name, values in metrics.items():
        if values:
            numeric_values = [v['value'] for v in values if isinstance(v['value'], (int, float))]
            if numeric_values:
                avg = sum(numeric_values) / len(numeric_values)
                print(f"{metric_name}: avg={avg:.2f}, count={len(values)}")

@pytest.fixture
def spark_load_session():
    """Create Spark session optimized for load tests"""
    from pyspark.sql import SparkSession

    spark = SparkSession.builder \
        .remote("sc://localhost:15002") \
        .appName("Load Test") \
        .config("spark.executor.memory", "4g") \
        .config("spark.executor.cores", "4") \
        .config("spark.executor.instances", "4") \
        .config("spark.dynamicAllocation.enabled", "false") \
        .config("spark.sql.adaptive.enabled", "false") \
        .getOrCreate()
    yield spark
    spark.stop()

@pytest.fixture
def load_test_queries():
    """Standard load test queries"""
    return {
        "count": "SELECT COUNT(*) AS row_count FROM nyc_taxi",
        "groupby": """
            SELECT passenger_count, COUNT(*) AS trip_count, AVG(total_amount) AS avg_fare
            FROM nyc_taxi
            GROUP BY passenger_count
        """,
        "join": """
            SELECT t1.passenger_count, COUNT(*) AS trip_count
            FROM nyc_taxi t1
            INNER JOIN nyc_taxi t2 ON t1.passenger_count = t2.passenger_count
            WHERE t1.trip_distance > 5
            GROUP BY t1.passenger_count
        """,
        "window": """
            SELECT
                passenger_count,
                ROW_NUMBER() OVER (PARTITION BY passenger_count ORDER BY total_amount DESC) AS rank
            FROM nyc_taxi
            WHERE passenger_count > 0
        """
    }
```

### tests/load/phase07/test_07_01_baseline_load.py

```python
"""Baseline Load Tests for Phase 7

Load tests for sustained performance (30 minutes) with NYC Taxi dataset.
"""

import pytest
import time
import random
from pyspark.sql import SparkSession

class TestBaselineLoad:
    """Baseline load tests for sustained performance"""

    @pytest.mark.timeout(2400)  # 40 minutes timeout
    @pytest.mark.load
    def test_01_spark_357_baseline_sustained_select(self, spark_load_session, nyc_taxi_dataset_path, load_metrics, load_duration_seconds):
        """Spark 3.5.7 Baseline — Sustained SELECT COUNT(*) (30 min)"""
        # Load data once
        df = spark_load_session.read.parquet(str(nyc_taxi_dataset_path))
        df.createOrReplaceTempView("nyc_taxi")

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        query_count = 0
        error_count = 0
        latencies = []

        # Sustained load: 1 query/second for 30 minutes
        while time.time() < end_time:
            query_start = time.time()
            try:
                result = spark_load_session.sql("SELECT COUNT(*) AS row_count FROM nyc_taxi")
                result.collect()
                query_end = time.time()
                latencies.append(query_end - query_start)
                query_count += 1

                # Record metrics
                load_metrics("queries_executed", query_count)
                load_metrics("latency_ms", (query_end - query_start) * 1000)

                # Sleep to maintain 1 query/second rate
                time.sleep(max(0, 1 - (query_end - query_start)))

            except Exception as e:
                error_count += 1
                load_metrics("error", str(e))
                if error_count > 10:  # Fail fast if too many errors
                    pytest.fail(f"Too many errors: {error_count}")

        total_time = time.time() - start_time
        throughput = query_count / total_time

        # Validate
        assert query_count > 1500, f"Should execute at least 1500 queries, got {query_count}"
        assert error_count == 0, f"Should have no errors, got {error_count}"
        assert throughput > 0.5, f"Throughput should be > 0.5 qps, got {throughput}"

        # Collect summary metrics
        load_metrics("test_01_total_queries", query_count)
        load_metrics("test_01_total_time", total_time)
        load_metrics("test_01_throughput", throughput)
        load_metrics("test_01_error_count", error_count)

        if latencies:
            latencies_sorted = sorted(latencies)
            load_metrics("test_01_latency_p50", latencies_sorted[int(len(latencies) * 0.5)])
            load_metrics("test_01_latency_p95", latencies_sorted[int(len(latencies) * 0.95)])
            load_metrics("test_01_latency_p99", latencies_sorted[int(len(latencies) * 0.99)])

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    def test_02_spark_358_baseline_sustained_groupby(self, spark_load_session, nyc_taxi_dataset_path, load_metrics, load_duration_seconds):
        """Spark 3.5.8 Baseline — Sustained GROUP BY (30 min)"""
        df = spark_load_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.sample(0.1)  # Use sample for faster execution
        df.createOrReplaceTempView("nyc_taxi")

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        query_count = 0
        error_count = 0

        while time.time() < end_time:
            try:
                result = spark_load_session.sql("""
                    SELECT passenger_count, COUNT(*) AS trip_count
                    FROM nyc_taxi
                    GROUP BY passenger_count
                """)
                result.collect()
                query_count += 1

                load_metrics("test_02_queries", query_count)
                time.sleep(1)  # 1 query/second

            except Exception as e:
                error_count += 1
                load_metrics("test_02_error", str(e))
                if error_count > 10:
                    pytest.fail(f"Too many errors: {error_count}")

        total_time = time.time() - start_time
        throughput = query_count / total_time

        assert query_count > 1500
        assert error_count == 0

        load_metrics("test_02_total_queries", query_count)
        load_metrics("test_02_throughput", throughput)

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    def test_03_spark_410_baseline_sustained_join(self, spark_load_session, nyc_taxi_dataset_path, load_metrics, load_duration_seconds):
        """Spark 4.1.0 Baseline — Sustained JOIN (30 min)"""
        df = spark_load_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.sample(0.01)  # Smaller sample for JOIN
        df.createOrReplaceTempView("nyc_taxi")

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        query_count = 0
        error_count = 0

        # JOIN is heavier, 0.5 query/second
        while time.time() < end_time:
            try:
                result = spark_load_session.sql("""
                    SELECT t1.passenger_count, COUNT(*) AS trip_count
                    FROM nyc_taxi t1
                    INNER JOIN nyc_taxi t2 ON t1.passenger_count = t2.passenger_count
                    WHERE t1.trip_distance > 5
                    GROUP BY t1.passenger_count
                """)
                result.collect()
                query_count += 1

                load_metrics("test_03_queries", query_count)
                time.sleep(2)  # 0.5 query/second

            except Exception as e:
                error_count += 1
                load_metrics("test_03_error", str(e))
                if error_count > 10:
                    pytest.fail(f"Too many errors: {error_count}")

        total_time = time.time() - start_time

        assert query_count > 750  # 0.5 qps for 30 min
        assert error_count == 0

        load_metrics("test_03_total_queries", query_count)

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    def test_04_spark_411_baseline_sustained_mixed(self, spark_load_session, nyc_taxi_dataset_path, load_metrics, load_duration_seconds):
        """Spark 4.1.1 Baseline — Sustained Mixed workload (30 min)"""
        df = spark_load_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.sample(0.05)
        df.createOrReplaceTempView("nyc_taxi")

        queries = [
            "SELECT COUNT(*) FROM nyc_taxi",
            "SELECT passenger_count, COUNT(*) FROM nyc_taxi GROUP BY passenger_count",
            "SELECT COUNT(*) FROM nyc_taxi WHERE trip_distance > 5",
            "SELECT passenger_count, AVG(total_amount) FROM nyc_taxi GROUP BY passenger_count"
        ]

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        query_count = 0
        error_count = 0

        while time.time() < end_time:
            try:
                # Random query from list
                query = random.choice(queries)
                result = spark_load_session.sql(query)
                result.collect()
                query_count += 1

                load_metrics("test_04_queries", query_count)
                time.sleep(1)

            except Exception as e:
                error_count += 1
                load_metrics("test_04_error", str(e))
                if error_count > 10:
                    pytest.fail(f"Too many errors: {error_count}")

        total_time = time.time() - start_time

        assert query_count > 1500
        assert error_count == 0

        load_metrics("test_04_total_queries", query_count)
```

## Scope Estimate
- Files: 2 (conftest.py, test_07_01_baseline_load.py)
- LOC: ~600 (tests + load framework)
- Scenarios: 4
- Duration: 30 min × 4 = 2 hours execution time
- Size: MEDIUM
