---
ws_id: 010-02
feature: F10
status: backlog
size: MEDIUM
project_id: spark_k8s
github_issue: null
assignee: null
depends_on:
  - 009-02  # Python 3.10 base layer
---

## WS-010-02: Python Dependencies Layer

### ðŸŽ¯ Goal

**What must WORK after completing this WS:**
- Python dependencies intermediate layer for PySpark applications
- Support for base, GPU, and Iceberg variants
- Layer extends custom Spark builds or standalone JDK base

**Acceptance Criteria:**
- [ ] AC1: docker/docker-intermediate/python-deps/Dockerfile created
- [ ] AC2: requirements-base.txt for core PySpark libraries
- [ ] AC3: requirements-gpu.txt for NVIDIA RAPIDS (cudf, cuml)
- [ ] AC4: requirements-iceberg.txt for pyiceberg support
- [ ] AC5: Build script supports variant selection
- [ ] AC6: Tests validate library imports

**âš ï¸ WS is NOT complete until Goal is achieved (all AC âœ…).**

### Context
- Custom Spark builds already include basic Python (pyspark, pandas, numpy, etc.)
- This layer adds optional GPU libraries (cudf, cuml) and Iceberg support (pyiceberg)
- Previously started in commit a03fae3 but incomplete

### Dependency
- WS-009-02 (Python 3.10 base layer) - if standalone
- Independent if extending custom Spark builds

### Input Files
- Commit a03fae3 work (if available)
- `docker/docker-intermediate/python-deps/` (if exists)

### Steps

#### Phase 1: Create directory structure
1. Create `docker/docker-intermediate/python-deps/` directory:
```bash
mkdir -p /home/fall_out_bug/work/s7/spark_k8s/docker/docker-intermediate/python-deps
```

#### Phase 2: Create requirements files
2. Create `docker/docker-intermediate/python-deps/requirements-base.txt`:
```txt
# Core PySpark and data science libraries
# Note: Custom Spark builds already include these
# This file is for extending other base images

pyspark>=3.5.0
pandas>=2.0.0
numpy>=1.24.0
pyarrow>=14.0.0
findspark
```

3. Create `docker/docker-intermediate/python-deps/requirements-gpu.txt`:
```txt
# NVIDIA RAPIDS GPU libraries for PySpark
# Compatible with CUDA 12.x

cudf-cu12>=24.10.0
cuml-cu12>=24.10.0
cupy-cuda12x>=13.0.0
 Rapids
```

4. Create `docker/docker-intermediate/python-deps/requirements-iceberg.txt`:
```txt
# Apache Iceberg Python support for PySpark

pyiceberg>=0.6.0
fsspec>=2024.0.0
```

#### Phase 3: Create Dockerfile
5. Create `docker/docker-intermediate/python-deps/Dockerfile`:
```dockerfile
# Python Dependencies Intermediate Layer for Spark K8s
# Extends either custom Spark build or JDK base
# Supports base, GPU, and Iceberg variants via build args

ARG BASE_IMAGE=localhost/spark-k8s:3.5.7-hadoop3.4.2
FROM ${BASE_IMAGE}

# Labels for metadata
LABEL maintainer="spark-k8s" \
      description="Python dependencies intermediate layer for Spark K8s" \
      version="1.0.0"

# Build arguments for variant selection
ARG BUILD_GPU_DEPS=false
ARG BUILD_ICEBERG_DEPS=false
ARG PYTHON_VERSION=3.11

# Switch to root for installation
USER root

# Set pip configuration for faster installs
ENV PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PYTHONUNBUFFERED=1

# Install base dependencies (if not already in base image)
COPY requirements-base.txt /tmp/
RUN if ! pip show pyspark >/dev/null 2>&1; then \
        pip install --no-cache-dir -r /tmp/requirements-base.txt; \
    else \
        echo "PySpark already installed in base image"; \
    fi && \
    rm /tmp/requirements-base.txt

# Install GPU dependencies if requested
COPY requirements-gpu.txt /tmp/
RUN if [ "$BUILD_GPU_DEPS" = "true" ]; then \
        echo "Installing GPU dependencies..." && \
        pip install --no-cache-dir -r /tmp/requirements-gpu.txt && \
        echo "GPU dependencies installed: cudf, cuml, cupy"; \
    else \
        echo "Skipping GPU dependencies"; \
    fi && \
    rm /tmp/requirements-gpu.txt

# Install Iceberg dependencies if requested
COPY requirements-iceberg.txt /tmp/
RUN if [ "$BUILD_ICEBERG_DEPS" = "true" ]; then \
        echo "Installing Iceberg dependencies..." && \
        pip install --no-cache-dir -r /tmp/requirements-iceberg.txt && \
        echo "Iceberg dependencies installed: pyiceberg, fsspec"; \
    else \
        echo "Skipping Iceberg dependencies"; \
    fi && \
    rm /tmp/requirements-iceberg.txt

# Verify installations
RUN python -c "import pyspark; print(f'PySpark {pyspark.__version__} OK')" && \
    if [ "$BUILD_GPU_DEPS" = "true" ]; then \
        python -c "import cudf; print(f'cuDF {cudf.__version__} OK')"; \
    fi && \
    if [ "$BUILD_ICEBERG_DEPS" = "true" ]; then \
        python -c "import pyiceberg; print(f'PyIceberg {pyiceberg.__version__} OK')"; \
    fi

# Switch back to non-root user (matches custom Spark build)
USER 185

# Working directory
WORKDIR /opt/spark/work-dir

# Health check to verify Python packages
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD python -c "import pyspark, pandas, numpy" || exit 1

# Default command (show installed packages)
CMD ["bash", "-c", "pip list | grep -E 'pyspark|pandas|numpy|cudf|pyiceberg' || echo 'Python deps layer ready'"]
```

#### Phase 4: Create build script
6. Create `docker/docker-intermediate/python-deps/build.sh`:
```bash
#!/usr/bin/env bash
set -euo pipefail

# Configuration
BASE_IMAGE=${BASE_IMAGE:-localhost/spark-k8s:3.5.7-hadoop3.4.2}
BUILD_GPU=${BUILD_GPU_DEPS:-false}
BUILD_ICEBERG=${BUILD_ICEBERG_DEPS:-false}
TAG_SUFFIX=""

# Add suffixes for variants
if [ "$BUILD_GPU" = "true" ]; then
    TAG_SUFFIX="-gpu"
fi
if [ "$BUILD_ICEBERG" = "true" ]; then
    TAG_SUFFIX="${TAG_SUFFIX}-iceberg"
fi

IMAGE_NAME="spark-k8s-python-deps:latest${TAG_SUFFIX}"

echo "Building Python dependencies layer..."
echo "  Base image: $BASE_IMAGE"
echo "  GPU deps: $BUILD_GPU"
echo "  Iceberg deps: $BUILD_ICEBERG"
echo "  Output: $IMAGE_NAME"

# Build image
docker build \
    --build-arg "BASE_IMAGE=${BASE_IMAGE}" \
    --build-arg "BUILD_GPU_DEPS=${BUILD_GPU}" \
    --build-arg "BUILD_ICEBERG_DEPS=${BUILD_ICEBERG}" \
    -t "$IMAGE_NAME" \
    .

echo "Built: $IMAGE_NAME"
```

#### Phase 5: Create test script
7. Create `docker/docker-intermediate/python-deps/test.sh`:
```bash
#!/usr/bin/env bash
set -euo pipefail

IMAGE_NAME=${IMAGE_NAME:-spark-k8s-python-deps:latest}
BUILD_GPU=${BUILD_GPU_DEPS:-false}
BUILD_ICEBERG=${BUILD_ICEBERG_DEPS:-false}

echo "=== Testing Python Dependencies Layer ==="
echo "Image: $IMAGE_NAME"

# Test 1: Verify image exists
echo "Test 1: Image exists"
docker image inspect "$IMAGE_NAME" >/dev/null
echo "PASS: Image exists"

# Test 2: Verify PySpark
echo "Test 2: PySpark installation"
PYSPARK_VERSION=$(docker run --rm "$IMAGE_NAME" python -c "import pyspark; print(pyspark.__version__)" 2>/dev/null || echo "")
if [[ -n "$PYSPARK_VERSION" ]]; then
    echo "PASS: PySpark $PYSPARK_VERSION installed"
else
    echo "FAIL: PySpark not found"
    exit 1
fi

# Test 3: Verify core data science packages
echo "Test 3: Core packages (pandas, numpy, pyarrow)"
PACKAGES=$(docker run --rm "$IMAGE_NAME" python -c "import pandas, numpy, pyarrow; print('OK')" 2>/dev/null || echo "")
if [[ "$PACKAGES" == "OK" ]]; then
    echo "PASS: Core packages installed"
else
    echo "FAIL: Core packages missing"
    exit 1
fi

# Test 4: Verify GPU packages if requested
if [ "$BUILD_GPU" = "true" ]; then
    echo "Test 4: GPU packages (cudf, cuml)"
    GPU_PACKAGES=$(docker run --rm "$IMAGE_NAME" python -c "import cudf, cuml; print('OK')" 2>/dev/null || echo "")
    if [[ "$GPU_PACKAGES" == "OK" ]]; then
        echo "PASS: GPU packages installed"
    else
        echo "FAIL: GPU packages missing"
        exit 1
    fi
else
    echo "Test 4: Skipped (GPU not requested)"
fi

# Test 5: Verify Iceberg packages if requested
if [ "$BUILD_ICEBERG" = "true" ]; then
    echo "Test 5: Iceberg packages (pyiceberg)"
    ICEBERG_PACKAGES=$(docker run --rm "$IMAGE_NAME" python -c "import pyiceberg; print('OK')" 2>/dev/null || echo "")
    if [[ "$ICEBERG_PACKAGES" == "OK" ]]; then
        echo "PASS: Iceberg packages installed"
    else
        echo "FAIL: Iceberg packages missing"
        exit 1
    fi
else
    echo "Test 5: Skipped (Iceberg not requested)"
fi

# Test 6: Verify Spark integration
echo "Test 6: Spark integration"
SPARK_TEST=$(docker run --rm "$IMAGE_NAME" bash -c 'cd /opt/spark && ./bin/pyspark --version 2>&1 | head -1' 2>/dev/null || echo "")
if [[ -n "$SPARK_TEST" ]]; then
    echo "PASS: Spark integration works"
else
    echo "FAIL: Spark integration failed"
    exit 1
fi

echo ""
echo "=== All tests passed ==="
```

#### Phase 6: Create README
8. Create `docker/docker-intermediate/python-deps/README.md`:
```markdown
# Python Dependencies Intermediate Layer

## Description
Provides Python dependencies for PySpark applications with support for GPU and Iceberg variants.

## Variants

| Variant | Build Arg | Content |
|---------|-----------|---------|
| base | (default) | pyspark, pandas, numpy, pyarrow |
| gpu | BUILD_GPU_DEPS=true | + cudf, cuml, cupy (RAPIDS) |
| iceberg | BUILD_ICEBERG_DEPS=true | + pyiceberg, fsspec |
| gpu-iceberg | Both=true | All of the above |

## Base Images

Compatible with:
- Custom Spark builds: `localhost/spark-k8s:3.5.7-hadoop3.4.2`
- Custom Spark builds: `localhost/spark-k8s:4.1.0-hadoop3.4.2`
- JDK 17 base: `localhost/spark-k8s-jdk-17:latest`

## Build

```bash
# Base variant
./build.sh

# GPU variant
BUILD_GPU_DEPS=true ./build.sh

# Iceberg variant
BUILD_ICEBERG_DEPS=true ./build.sh

# GPU + Iceberg variant
BUILD_GPU_DEPS=true BUILD_ICEBERG_DEPS=true ./build.sh

# With custom base image
BASE_IMAGE=localhost/spark-k8s:4.1.0-hadoop3.4.2 ./build.sh
```

## Test

```bash
# Test base variant
./test.sh

# Test GPU variant
BUILD_GPU_DEPS=true IMAGE_NAME=spark-k8s-python-deps:latest-gpu ./test.sh
```

## Package Versions

| Package | Version |
|---------|---------|
| pyspark | >=3.5.0 |
| pandas | >=2.0.0 |
| numpy | >=1.24.0 |
| pyarrow | >=14.0.0 |
| cudf-cu12 | >=24.10.0 (GPU variant) |
| cuml-cu12 | >=24.10.0 (GPU variant) |
| pyiceberg | >=0.6.0 (Iceberg variant) |
```

### Code

#### docker/docker-intermediate/python-deps/Dockerfile
```dockerfile
# Python Dependencies Intermediate Layer for Spark K8s

ARG BASE_IMAGE=localhost/spark-k8s:3.5.7-hadoop3.4.2
FROM ${BASE_IMAGE}

LABEL maintainer="spark-k8s" \
      description="Python dependencies intermediate layer for Spark K8s" \
      version="1.0.0"

ARG BUILD_GPU_DEPS=false
ARG BUILD_ICEBERG_DEPS=false
ARG PYTHON_VERSION=3.11

USER root

ENV PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PYTHONUNBUFFERED=1

COPY requirements-base.txt /tmp/
RUN if ! pip show pyspark >/dev/null 2>&1; then \
        pip install --no-cache-dir -r /tmp/requirements-base.txt; \
    else \
        echo "PySpark already installed in base image"; \
    fi && \
    rm /tmp/requirements-base.txt

COPY requirements-gpu.txt /tmp/
RUN if [ "$BUILD_GPU_DEPS" = "true" ]; then \
        pip install --no-cache-dir -r /tmp/requirements-gpu.txt; \
    fi && \
    rm /tmp/requirements-gpu.txt

COPY requirements-iceberg.txt /tmp/
RUN if [ "$BUILD_ICEBERG_DEPS" = "true" ]; then \
        pip install --no-cache-dir -r /tmp/requirements-iceberg.txt; \
    fi && \
    rm /tmp/requirements-iceberg.txt

RUN python -c "import pyspark; print(f'PySpark {pyspark.__version__} OK')" && \
    if [ "$BUILD_GPU_DEPS" = "true" ]; then \
        python -c "import cudf; print(f'cuDF {cudf.__version__} OK')"; \
    fi && \
    if [ "$BUILD_ICEBERG_DEPS" = "true" ]; then \
        python -c "import pyiceberg; print(f'PyIceberg {pyiceberg.__version__} OK')"; \
    fi

USER 185

WORKDIR /opt/spark/work-dir

HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD python -c "import pyspark, pandas, numpy" || exit 1

CMD ["bash", "-c", "pip list | grep -E 'pyspark|pandas|numpy' || echo 'Python deps ready'"]
```

### Scope Estimate
- Files: ~6 (Dockerfile, 3 requirements.txt, build.sh, test.sh, README.md)
- LOC: ~400 (SMALL to MEDIUM)

### Acceptance Criteria
1. Python dependencies layer created with 3 variants
2. Requirements files defined for base, GPU, Iceberg
3. Build script supports variant selection via build args
4. Tests verify package installation for each variant
5. Layer extends custom Spark builds successfully

### Notes
- Custom Spark builds already include base Python deps
- This layer is most useful for GPU/Iceberg variants
- For extending JDK base (not custom Spark), installs base deps too
