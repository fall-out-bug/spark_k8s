## WS-001-11: Prod-like Airflow tests (KubernetesExecutor + Spark Standalone)

### üéØ –¶–µ–ª—å (Goal)

**What should WORK after WS completion:**
- A ‚Äúprod-like‚Äù Helm values profile that enables Spark Standalone HA (PVC) and runs Airflow reliably under PSS hardening.
- Airflow DAG `spark_etl_synthetic` completes successfully against the Spark Standalone cluster and writes output to MinIO (S3A).

**Acceptance Criteria:**
- [ ] `charts/spark-standalone/values-prod-like.yaml` exists and documents intended use
- [ ] Airflow runs with `KubernetesExecutor` without read-only filesystem failures
- [ ] DAGs are discoverable without ‚Äúrecursive loop‚Äù errors
- [ ] Airflow Variables decrypt consistently across scheduler/webserver/worker pods (shared Fernet key)
- [ ] `spark_etl_synthetic` run reaches `success` in ‚Äúprod-like‚Äù namespace
- [ ] Spark job runs with executor sizing compatible with a small local cluster (no WAITING due to 4Gi defaults)

**WS is NOT complete until Goal is achieved (all AC checked).**

---

### Context

This workstream focuses on validating and hardening Airflow execution in a ‚Äúprod-like‚Äù setup:
PSS-enabled pods, KubernetesExecutor task pods, and Spark Standalone driver connectivity to workers.

### Dependency

WS-001-10 (Example DAGs & Tests)

### Input Files

- `charts/spark-standalone/templates/airflow/configmap.yaml`
- `charts/spark-standalone/templates/airflow/*`
- `charts/spark-standalone/values-prod-like.yaml`

### Steps

1. Add shared Fernet key support (Secret + env wiring) so Variables decrypt correctly across pods
2. Ensure KubernetesExecutor worker pods use Airflow image and a shared pod template
3. Ensure DAGs are copied into a real directory (avoid ConfigMap symlink recursion)
4. Update Spark submit command in DAG to pass S3A + driver host + executor sizing overrides
5. Deploy prod-like release and run:
   - `example_bash_operator`
   - `spark_etl_synthetic`

### Completion Criteria (manual)

```bash
kubectl create ns spark-sa-prodlike || true
helm upgrade --install spark-prodlike charts/spark-standalone \
  -n spark-sa-prodlike \
  -f charts/spark-standalone/values-prod-like.yaml \
  --set ingress.enabled=false

# Trigger ETL DAG (from scheduler or webserver)
kubectl exec -n spark-sa-prodlike deploy/spark-prodlike-spark-standalone-airflow-scheduler -- \
  airflow dags trigger spark_etl_synthetic

kubectl exec -n spark-sa-prodlike deploy/spark-prodlike-spark-standalone-airflow-scheduler -- \
  airflow dags list-runs -d spark_etl_synthetic
```

