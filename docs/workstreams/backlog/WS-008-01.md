# WS-008-01: PSS Compliance Tests

## Goal
Создать 8 тестов для проверки Pod Security Standards (PSS) compliance для всех подов Spark-а на Kubernetes.

### Acceptance Criteria
1. 8 PSS тестов созданы в `tests/security/phase08/test_08_01_pss.py`
2. PSS restricted profile проверен для spark-connect, spark-worker, jupyter, hive-metastore
3. PSS baseline profile проверен для spark-connect
4. PSS privileged проверен как NOT allowed в prod preset
5. PSS labels проверены для namespace enforce/audit режимов
6. Используется `helm template + kubeconform` для быстрой проверки
7. Используется `kubectl apply --dry-run` для финальной валидации
8. Все тесты проходят

## Context

**Существующая инфраструктура:**
- `charts/spark-3.5/templates/namespace.yaml` — PSS labels (pod-security.enforce, pod-security.audit)
- `charts/spark-3.5/values.yaml` — `podSecurityStandards: true/false` toggle
- SecurityContext в pod templates (runAsUser, runAsGroup, capabilities)
- Существующие тесты в `tests/security/test_security.py` (NetworkPolicies, RBAC, SecurityContext, Compliance)

**Что нужно добавить:**
- Структуррированные PSS тесты для всех подов
- Валидация через kubeconform с PSS профильами
- Проверка PSS labels на namespace
- Отдельный файл `tests/security/phase08/test_08_01_pss.py`

## Dependency
Phase 0 (Helm Charts), Phase 1 (Critical Security)

## Input Files
- `charts/spark-3.5/templates/namespace.yaml`
- `charts/spark-3.5/templates/spark-connect.yaml`
- `charts/spark-3.5/templates/spark-standalone-worker.yaml`
- `charts/spark-3.5/templates/jupyter.yaml`
- `charts/spark-3.5/templates/hive-metastore.yaml`
- `charts/spark-3.5/values.yaml`
- `charts/spark-4.1/values.yaml`
- `tests/security/test_security.py` (reference)

## Steps

### 1. Create conftest.py for shared fixtures
- Создать `tests/security/phase08/conftest.py`
- Добавить fixtures для helm_chart_path, kubeconform_cmd
- Добавить fixtures для spark_versions (3.5, 4.1)

### 2. Create test_08_01_pss.py
- Создать `tests/security/phase08/test_08_01_pss.py`
- Implement 8 test scenarios

### 3. Implement test scenarios

**Scenario 1: PSS restricted — spark-connect pod compliant**
- `helm template spark-3.5 --set sparkConnect.enabled=true`
- Запустить kubeconform с PSS restricted profile
- Проверить отсутствие ошибок

**Scenario 2: PSS restricted — spark-worker pod compliant**
- `helm template spark-3.5 --set sparkStandalone.enabled=true`
- kubeconform с PSS restricted profile
- Проверить absence errors

**Scenario 3: PSS restricted — jupyter pod compliant**
- `helm template spark-3.5 --set jupyter.enabled=true`
- kubeconform с PSS restricted profile
- Validate compliance

**Scenario 4: PSS restricted — hive-metastore pod compliant**
- `helm template spark-3.5 --set hiveMetastore.enabled=true`
- kubeconiform с PSS restricted profile
- Проверить compliance

**Scenario 5: PSS baseline — spark-connect pod compliant**
- `helm template spark-3.5 --set sparkConnect.enabled=true,podSecurityStandards=false`
- kubeconform с PSS baseline profile
- Проверить compliance

**Scenario 6: PSS privileged — NOT allowed in prod preset**
- `helm template spark-3.5 -f presets/prod-like.yaml`
- Проверить что privileged: false во всех контейнерах
- Проверить отсутствие allowPrivilegeEscalation

**Scenario 7: PSS labels — namespace enforce=restricted**
- Проверить `templates/namespace.yaml`
- Проверить `pod-security.enforce=restricted` label
- Проверить `pod-security.warn=restricted` label

**Scenario 8: PSS labels — namespace audit=restricted**
- Проверить `templates/namespace.yaml`
- Проверить `pod-security.audit=restricted` label
- Проверить `pod-security.enforce-version=latest` label

### 4. Add __init__.py
- Создать `tests/security/phase08/__init__.py`

### 5. Run tests
- `pytest tests/security/phase08/test_08_01_pss.py -v`
- Убедиться что все 8 тестов проходят

## Code

### tests/security/phase08/conftest.py

```python
"""Shared fixtures for Phase 8 security tests"""

import pytest
from pathlib import Path

@pytest.fixture(scope="session")
def project_root() -> Path:
    """Project root directory"""
    return Path(__file__).parent.parent.parent.parent

@pytest.fixture(scope="session")
def helm_charts_dir(project_root: Path) -> Path:
    """Helm charts directory"""
    return project_root / "charts"

@pytest.fixture(scope="session")
def spark_35_chart(helm_charts_dir: Path) -> Path:
    """Spark 3.5 chart path"""
    return helm_charts_dir / "spark-3.5"

@pytest.fixture(scope="session")
def spark_41_chart(helm_charts_dir: Path) -> Path:
    """Spark 4.1 chart path"""
    return helm_charts_dir / "spark-4.1"

@pytest.fixture(scope="session")
def spark_versions():
    """Available Spark versions"""
    return ["3.5", "4.1"]

@pytest.fixture(scope="session")
def kubeconform_available():
    """Check if kubeconform is available"""
    import shutil
    return shutil.which("kubeconform") is not None

@pytest.fixture
def require_kubeconform(kubeconform_available):
    """Skip test if kubeconform not available"""
    if not kubeconform_available:
        pytest.skip("kubeconform not available")
```

### tests/security/phase08/test_08_01_pss.py

```python
"""PSS Compliance Tests for Phase 8

Tests for Pod Security Standards (PSS) compliance across all Spark pods.
"""

import pytest
import subprocess
import tempfile
from pathlib import Path
from typing import List

PSS_PROFILES = {
    "restricted": "https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/podsecuritystandards/default/restricted.yaml",
    "baseline": "https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/podsecuritystandards/default/baseline.yaml",
    "privileged": "https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/podsecuritystandards/default/privileged.yaml",
}

class TestPSSCompliance:
    """Pod Security Standards compliance tests"""

    def test_01_pss_restricted_spark_connect(self, spark_35_chart, require_kubeconform):
        """PSS restricted — spark-connect pod compliant"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            # Render template
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true"],
                capture_output=True, text=True
            )
            f.write(result.stdout)
            f.flush()

            # Validate with kubeconform
            result = subprocess.run(
                ["kubeconform", "-strict",
                 "-schema-location", PSS_PROFILES["restricted"],
                 f.name],
                capture_output=True, text=True
            )
            assert result.returncode == 0, f"PSS restricted validation failed: {result.stderr}"

    def test_02_pss_restricted_spark_worker(self, spark_35_chart, require_kubeconform):
        """PSS restricted — spark-worker pod compliant"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkStandalone.enabled=true"],
                capture_output=True, text=True
            )
            f.write(result.stdout)
            f.flush()

            result = subprocess.run(
                ["kubeconform", "-strict",
                 "-schema-location", PSS_PROFILES["restricted"],
                 f.name],
                capture_output=True, text=True
            )
            assert result.returncode == 0, f"PSS restricted validation failed: {result.stderr}"

    def test_03_pss_restricted_jupyter(self, spark_35_chart, require_kubeconform):
        """PSS restricted — jupyter pod compliant"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "jupyter.enabled=true"],
                capture_output=True, text=True
            )
            f.write(result.stdout)
            f.flush()

            result = subprocess.run(
                ["kubeconform", "-strict",
                 "-schema-location", PSS_PROFILES["restricted"],
                 f.name],
                capture_output=True, text=True
            )
            assert result.returncode == 0, f"PSS restricted validation failed: {result.stderr}"

    def test_04_pss_restricted_hive_metastore(self, spark_35_chart, require_kubeconform):
        """PSS restricted — hive-metastore pod compliant"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "hiveMetastore.enabled=true"],
                capture_output=True, text=True
            )
            f.write(result.stdout)
            f.flush()

            result = subprocess.run(
                ["kubeconform", "-strict",
                 "-schema-location", PSS_PROFILES["restricted"],
                 f.name],
                capture_output=True, text=True
            )
            assert result.returncode == 0, f"PSS restricted validation failed: {result.stderr}"

    def test_05_pss_baseline_spark_connect(self, spark_35_chart, require_kubeconform):
        """PSS baseline — spark-connect pod compliant"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true",
                 "--set", "podSecurityStandards=false"],
                capture_output=True, text=True
            )
            f.write(result.stdout)
            f.flush()

            result = subprocess.run(
                ["kubeconform", "-strict",
                 "-schema-location", PSS_PROFILES["baseline"],
                 f.name],
                capture_output=True, text=True
            )
            assert result.returncode == 0, f"PSS baseline validation failed: {result.stderr}"

    def test_06_pss_privileged_not_allowed_prod(self, spark_35_chart):
        """PSS privileged — NOT allowed in prod preset"""
        # Check prod-like preset doesn't allow privileged
        prod_preset = spark_35_chart.parent / "presets" / "prod-like-values.yaml"
        if not prod_preset.exists():
            pytest.skip("prod-like preset not found")

        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "-f", str(prod_preset)],
                capture_output=True, text=True
            )

            # Check no privileged: true
            assert "privileged: true" not in result.stdout
            # Check no allowPrivilegeEscalation: true
            assert "allowPrivilegeEscalation: true" not in result.stdout

    def test_07_pss_labels_namespace_enforce(self, spark_35_chart):
        """PSS labels — namespace enforce=restricted"""
        namespace_template = spark_35_chart / "templates" / "namespace.yaml"
        if not namespace_template.exists():
            pytest.skip("namespace.yaml not found")

        content = namespace_template.read_text()
        assert "pod-security.enforce" in content
        assert "restricted" in content

    def test_08_pss_labels_namespace_audit(self, spark_35_chart):
        """PSS labels — namespace audit=restricted"""
        namespace_template = spark_35_chart / "templates" / "namespace.yaml"
        if not namespace_template.exists():
            pytest.skip("namespace.yaml not found")

        content = namespace_template.read_text()
        assert "pod-security.audit" in content or "pod-security.warn" in content
        assert "restricted" in content
```

## Scope Estimate
- Files: 3 (conftest.py, test_08_01_pss.py, __init__.py)
- LOC: ~600 (tests)
- Scenarios: 8
- Size: MEDIUM
