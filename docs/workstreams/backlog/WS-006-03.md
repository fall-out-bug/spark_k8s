# WS-006-03: Iceberg E2E Tests

## Goal
Создать 16 Iceberg E2E тестов для проверки Apache Iceberg table functionality на NYC Taxi dataset (11GB).

### Acceptance Criteria
1. 16 Iceberg E2E тестов созданы в `tests/e2e/phase06/test_06_03_iceberg_e2e.py`
2. NYC Taxi dataset конвертируется в Iceberg формат
3. Iceberg table operations проверены (CREATE, READ, UPDATE, DELETE)
4. Time travel queries работают (snapshot-based queries)
5. Schema evolution тестируется (ADD/DROP COLUMN)
6. Partitioning & pruning проверено
7. Метрики Iceberg: scan time, file pruning, snapshot size
8. Hive Metastore интеграция работает

## Context

**Существующая инфраструктура:**
- `tests/e2e/phase06/conftest.py` — shared fixtures
- `charts/spark-3.5/templates/hive-metastore.yaml` — Hive Metastore
- Iceberg Docker images из Phase 5

**Что нужно добавить:**
- Iceberg-specific E2E тесты
- Table operations (CRUD)
- Time travel queries
- Schema evolution
- Partitioning tests
- Отдельный файл `tests/e2e/phase06/test_06_03_iceberg_e2e.py`

## Dependency
Phase 0 (Helm Charts), Phase 5 (Iceberg Final Images)

## Input Files
- `tests/e2e/phase06/conftest.py` (shared fixtures)
- `charts/spark-3.5/values.yaml`
- `charts/spark-4.1/values.yaml`

## Steps

### 1. Create test_06_03_iceberg_e2e.py
- Создать `tests/e2e/phase06/test_06_03_iceberg_e2e.py`
- Implement 16 test scenarios

### 2. Implement test scenarios

**Iceberg Scenarios (Spark versions × operations):**

**Spark 3.5.7 + Iceberg:**
1. spark-connect + Iceberg CREATE TABLE
2. spark-connect + Iceberg READ (full scan)
3. spark-connect + Iceberg UPDATE/DELETE
4. spark-connect + Iceberg TIME TRAVEL

**Spark 3.5.8 + Iceberg:**
5. spark-connect + Iceberg CREATE TABLE
6. spark-connect + Iceberg READ (partition pruning)
7. spark-connect + Iceberg MERGE (UPSERT)
8. spark-connect + Iceberg SCHEMA EVOLUTION

**Spark 4.1.0 + Iceberg:**
9. spark-connect + Iceberg CREATE TABLE
10. spark-connect + Iceberg READ (with filter)
11. spark-connect + Iceberg UPDATE (batch)
12. spark-connect + Iceberg SNAPSHOT MANAGEMENT

**Spark 4.1.1 + Iceberg:**
13. spark-connect + Iceberg CREATE TABLE
14. spark-connect + Iceberg READ (partitioned)
15. spark-connect + Iceberg DELETE (partition)
16. spark-connect + Iceberg ROLLBACK

**Iceberg Operations:**
- CREATE TABLE: создать Iceberg table с partitioning
- READ: scan with pruning
- WRITE: append data to table
- UPDATE: update existing records
- DELETE: delete records
- MERGE: upsert operation
- TIME TRAVEL: query historical snapshots
- SCHEMA EVOLUTION: add/drop columns
- PARTITIONING: test partition pruning
- ROLLBACK: rollback to previous snapshot

### 3. Iceberg Metrics Collection
- iceberg_scan_time: time to scan table
- iceberg_files_pruned: number of files skipped
- iceberg_snapshot_size: size of current snapshot
- iceberg_partitions: number of partitions

### 4. Run tests
- `pytest tests/e2e/phase06/test_06_03_iceberg_e2e.py -v`
- Убедиться что все Iceberg тесты проходят

## Code

### tests/e2e/phase06/test_06_03_iceberg_e2e.py

```python
"""Iceberg E2E Tests for Phase 6

Tests for Apache Iceberg table functionality with NYC Taxi dataset.
"""

import pytest
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
from typing import Dict, Any

SPARK_VERSIONS = ["3.5.7", "3.5.8", "4.1.0", "4.1.1"]

class TestIcebergE2E:
    """Iceberg E2E tests for table operations"""

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_01_spark_357_iceberg_create_table(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 3.5.7 Iceberg - CREATE TABLE with partitioning"""
        start_time = time.time()

        # Configure Iceberg catalog
        spark_connect_session.conf.set("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
        spark_connect_session.conf.set("spark.sql.catalog.spark_catalog.type", "hadoop")
        spark_connect_session.conf.set("spark.sql.catalog.spark_catalog.warehouse", "/tmp/iceberg/warehouse")

        # Load source data
        df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.sample(0.01)  # Use sample for faster testing

        # Create Iceberg table
        df.writeTo("spark_catalog.nyc_taxi.iceberg_table") \
            .using("iceberg") \
          .partitionedBy(col("passenger_count")) \
          .create()

        execution_time = time.time() - start_time

        # Verify table was created
        tables = spark_connect_session.sql("SHOW TABLES IN spark_catalog.nyc_taxi")
        table_exists = [row['tableName'] for row in tables.collect() if row['tableName'] == 'iceberg_table']
        assert len(table_exists) > 0, "Iceberg table should be created"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "table_created", True)

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_02_spark_357_iceberg_read_full_scan(self, spark_connect_session, metrics_collector):
        """Spark 3.5.7 Iceberg - READ full scan"""
        start_time = time.time()

        # Read from Iceberg table
        df = spark_connect_session.table("spark_catalog.nyc_taxi.iceberg_table")
        count = df.count()

        execution_time = time.time() - start_time

        assert count > 0, "Iceberg table should have data"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "row_count", count)

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_03_spark_357_iceberg_update_delete(self, spark_connect_session, metrics_collector):
        """Spark 3.5.7 Iceberg - UPDATE and DELETE operations"""
        start_time = time.time()

        # UPDATE: modify records
        spark_connect_session.sql("""
            MERGE INTO spark_catalog.nyc_taxi.iceberg_table AS target
            USING (SELECT 1 as passenger_count, 100.0 as total_amount) AS source
            ON target.passenger_count = source.passenger_count
            WHEN MATCHED THEN UPDATE SET target.total_amount = source.total_amount
        """)

        # Verify update
        updated_df = spark_connect_session.table("spark_catalog.nyc_taxi.iceberg_table") \
            .filter(col("passenger_count") == 1)
        updated_count = updated_df.count()

        # DELETE: remove records
        spark_connect_session.sql("""
            DELETE FROM spark_catalog.nyc_taxi.iceberg_table
            WHERE passenger_count = 0
        """)

        execution_time = time.time() - start_time

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "updated_count", updated_count)

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_04_spark_357_iceberg_time_travel(self, spark_connect_session, metrics_collector):
        """Spark 3.5.7 Iceberg - TIME TRAVEL (snapshot-based queries)"""
        start_time = time.time()

        # Get current snapshot
        snapshot_df = spark_connect_session.sql("""
            SELECT snapshot_id, created_at
            FROM spark_catalog.nyc_taxi.iceberg_table.snapshots
            ORDER BY created_at DESC
            LIMIT 1
        """)
        snapshots = snapshot_df.collect()

        if len(snapshots) > 0:
            snapshot_id = snapshots[0]['snapshot_id']

            # Query using snapshot ID (time travel)
            historical_df = spark_connect_session.table(f"spark_catalog.nyc_taxi.iceberg_table VERSION AS {snapshot_id}")
            historical_count = historical_df.count()

            execution_time = time.time() - start_time

            assert historical_count >= 0, "Time travel query should work"

            metrics_collector(self._testMethodName, "execution_time", execution_time)
            metrics_collector(self._testMethodName, "snapshot_id", snapshot_id)
            metrics_collector(self._testMethodName, "historical_count", historical_count)

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_05_spark_358_iceberg_create_partitioned(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 3.5.8 Iceberg - CREATE TABLE with advanced partitioning"""
        start_time = time.time()

        # Load sample data
        df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.sample(0.01)

        # Create Iceberg table with multiple partition columns
        df.writeTo("spark_catalog.nyc_taxi.iceberg_partitioned") \
            .using("iceberg") \
          .partitionedBy(col("passenger_count"), col("vendor_id")) \
          .create()

        execution_time = time.time() - start_time

        # Verify partitioning
        partitions_df = spark_connect_session.sql("SHOW PARTITIONS spark_catalog.nyc_taxi.iceberg_partitioned")
        partitions = partitions_df.collect()

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "partition_count", len(partitions))

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_06_spark_358_iceberg_read_partition_pruning(self, spark_connect_session, metrics_collector):
        """Spark 3.5.8 Iceberg - READ with partition pruning"""
        start_time = time.time()

        # Query with filter on partition column (should prune partitions)
        df = spark_connect_session.table("spark_catalog.nyc_taxi.iceberg_partitioned") \
            .filter(col("passenger_count") == 1)
        count = df.count()

        execution_time = time.time() - start_time

        assert count >= 0, "Partition pruning should work"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "filtered_count", count)

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_07_spark_358_iceberg_merge_upsert(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 3.5.8 Iceberg - MERGE (UPSERT) operation"""
        start_time = time.time()

        # Load upsert data
        upsert_df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        upsert_df = upsert_df.sample(0.001)

        # Create temp view for merge
        upsert_df.createOrReplaceTempView("upsert_source")

        # MERGE INTO (UPSERT)
        spark_connect_session.sql("""
            MERGE INTO spark_catalog.nyc_taxi.iceberg_table AS target
            USING upsert_source AS source
            ON target.passenger_count = source.passenger_count
            WHEN MATCHED THEN UPDATE SET *
            WHEN NOT MATCHED THEN INSERT *
        """)

        execution_time = time.time() - start_time

        metrics_collector(self._testMethodName, "execution_time", execution_time)

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_08_spark_358_iceberg_schema_evolution(self, spark_connect_session, metrics_collector):
        """Spark 3.5.8 Iceberg - SCHEMA EVOLUTION (ADD/DROP COLUMN)"""
        start_time = time.time()

        # Get original schema
        original_schema = spark_connect_session.table("spark_catalog.nyc_taxi.iceberg_table").schema
        original_columns = len(original_schema.fields)

        # ADD COLUMN
        spark_connect_session.sql("""
            ALTER TABLE spark_catalog.nyc_taxi.iceberg_table
            ADD COLUMN new_column string
        """)

        # Verify column was added
        new_schema = spark_connect_session.table("spark_catalog.nyc_taxi.iceberg_table").schema
        new_columns = len(new_schema.fields)

        # DROP COLUMN
        spark_connect_session.sql("""
            ALTER TABLE spark_catalog.nyc_taxi.iceberg_table
            DROP COLUMN new_column
        """)

        execution_time = time.time() - start_time

        assert new_columns == original_columns + 1, "Column should be added"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "schema_evolution", True)

    # Similar tests for Spark 4.1.0, 4.1.1...
    # test_09 through test_16

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_09_spark_410_iceberg_create_with_catalog(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 4.1.0 Iceberg - CREATE TABLE with custom catalog"""
        start_time = time.time()

        # Configure custom Iceberg catalog
        spark_connect_session.conf.set("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
        spark_connect_session.conf.set("spark.sql.catalog.local.type", "hadoop")
        spark_connect_session.conf.set("spark.sql.catalog.local.warehouse", "/tmp/iceberg/local")

        # Create table in custom catalog
        df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.sample(0.01)

        df.writeTo("local.nyc_taxi.table_410") \
            .using("iceberg") \
          .partitionedBy(col("passenger_count")) \
          .create()

        execution_time = time.time() - start_time

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "catalog", "local")

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_10_spark_410_iceberg_read_with_filter(self, spark_connect_session, metrics_collector):
        """Spark 4.1.0 Iceberg - READ with complex filter"""
        start_time = time.time()

        # Query with complex filter
        df = spark_connect_session.table("local.nyc_taxi.table_410") \
            .filter((col("passenger_count") > 0) &
                    (col("trip_distance") > 5.0) &
                    (col("total_amount") < 100.0))
        count = df.count()

        execution_time = time.time() - start_time

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "filtered_count", count)

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_11_spark_410_iceberg_update_batch(self, spark_connect_session, metrics_collector):
        """Spark 4.1.0 Iceberg - BATCH UPDATE"""
        start_time = time.time()

        # Batch update multiple rows
        spark_connect_session.sql("""
            UPDATE local.nyc_taxi.table_410
            SET total_amount = total_amount * 1.1
            WHERE passenger_count = 1
        """)

        execution_time = time.time() - start_time

        metrics_collector(self._testMethodName, "execution_time", execution_time)

    @pytest.mark.timeout(900)
    @pytest.mark.iceberg
    def test_12_spark_410_iceberg_snapshot_management(self, spark_connect_session, metrics_collector):
        """Spark 4.1.0 Iceberg - SNAPSHOT MANAGEMENT"""
        start_time = time.time()

        # List snapshots
        snapshots_df = spark_connect_session.sql("""
            SELECT * FROM local.nyc_taxi.table_410.snapshots
            ORDER BY committed_at DESC
        """)
        snapshots = snapshots_df.collect()

        # Expire old snapshots (keep only latest)
        if len(snapshots) > 1:
            oldest_snapshot_id = snapshots[-1]['snapshot_id']
            spark_connect_session.sql(f"""
                CALL local.system.expire_snapshots('nyc_taxi.table_410', TIMESTAMP '{snapshots[-1]['committed_at']}")
            """)

        execution_time = time.time() - start_time

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "snapshot_count", len(snapshots))

    # Additional tests for Spark 4.1.1...
    # test_13 through test_16
```

## Scope Estimate
- Files: 1 (test_06_03_iceberg_e2e.py)
- LOC: ~900 (tests + Iceberg-specific logic)
- Scenarios: 16
- Size: MEDIUM
