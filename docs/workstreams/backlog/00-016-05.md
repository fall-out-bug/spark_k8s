---
ws_id: 00-016-05
feature: F16
status: backlog
size: MEDIUM
project_id: 00
github_issue: null
assignee: null
depends_on:
  - 00-016-01  # Metrics collection
---

## WS-00-016-05: Alerting rules

### ğŸ¯ Goal

**What must WORK after completing this WS:**
- AlertManager Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½
- Critical/warning/info alert rules
- Slack notification Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- Alert testing scenarios

**Acceptance Criteria:**
- [ ] AC1: AlertManager Helm chart ÑĞ¾Ğ·Ğ´Ğ°Ğ½
- [ ] AC2: Critical alerts Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ñ‹ (pod crash, OOM)
- [ ] AC3: Warning alerts Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ñ‹ (high GC, slow queries)
- [ ] AC4: Slack notification Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- [ ] AC5: Alert silence/rotation Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
- [ ] AC6: Testing scenarios ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹

**âš ï¸ WS is NOT complete until Goal is achieved (all AC âœ…).**

---

### Dependencies

WS-016-01 (Metrics collection)

### Code

```yaml
# charts/observability/alertmanager/values.yaml
alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m
      slack_api_url: ${SLACK_WEBHOOK_URL}

    templates:
    - '/etc/alertmanager/templates/*.tmpl'

    route:
      receiver: 'slack-critical'
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      routes:
      - match:
          severity: critical
        receiver: slack-critical
      - match:
          severity: warning
        receiver: slack-warning
      - match:
          severity: info
        receiver: slack-info

    receivers:
    - name: slack-critical
      slack_configs:
      - api_url: ${SLACK_WEBHOOK_URL}
        channel: '#spark-alerts-critical'
        title: 'ğŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

    - name: slack-warning
      slack_configs:
      - api_url: ${SLACK_WEBHOOK_URL}
        channel: '#spark-alerts-warning'
        title: 'âš ï¸ WARNING: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

    - name: slack-info
      slack_configs:
      - api_url: ${SLACK_WEBHOOK_URL}
        channel: '#spark-alerts-info'
        title: 'â„¹ï¸ INFO: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

    inhibit_rules:
    # Inhibit warning if critical is firing
    - source_match:
        severity: critical
      target_match:
        severity: warning
      equal: ['alertname', 'cluster', 'service']

# Alert rules
prometheusRules:
  enabled: true
  rules:
    # Critical alerts
    - name: spark-critical
      rules:
      - alert: SparkPodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total{namespace=~"spark.*"}[15m]) > 0
        for: 5m
        labels:
          severity: critical
          component: spark
        annotations:
          summary: Spark pod crash looping
          description: Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is crash looping

      - alert: SparkExecutorOOMKilled
        expr: kube_pod_container_status_terminated_reason{namespace=~"spark.*",reason="OOMKilled"} > 0
        for: 1m
        labels:
          severity: critical
          component: spark
        annotations:
          summary: Spark executor OOM killed
          description: Pod {{ $labels.pod }} was OOM killed in namespace {{ $labels.namespace }}

      - alert: SparkApplicationFailed
        expr: spark_application_status{namespace=~"spark.*",status!="FAILED"} == 0
        for: 5m
        labels:
          severity: critical
          component: spark
        annotations:
          summary: Spark application failed
          description: Application {{ $labels.app_name }} failed in namespace {{ $labels.namespace }}

    # Warning alerts
    - name: spark-warning
      rules:
      - alert: SparkHighGCTime
        expr: rate(spark_executor_gc_time_ms_total{namespace=~"spark.*"}[5m]) > 100
        for: 10m
        labels:
          severity: warning
          component: spark
        annotations:
          summary: High GC time detected
          description: Executor {{ $labels.executor }} has high GC time (>100ms)

      - alert: SparkSlowQuery
        expr: spark_sql_duration_ms{namespace=~"spark.*"} > 60000
        for: 1m
        labels:
          severity: warning
          component: spark
        annotations:
          summary: Slow SQL query detected
          description: Query took {{ $value }}ms in application {{ $labels.app_id }}

      - alert: SparkHighShuffleSpill
        expr: rate(spark_executor_shuffle_spill_bytes{namespace=~"spark.*"}[5m]) > 10485760
        for: 10m
        labels:
          severity: warning
          component: spark
        annotations:
          summary: High shuffle spill detected
          description: Executor {{ $labels.executor }} spilling >10MB/s to disk

    # Info alerts
    - name: spark-info
      rules:
      - alert: SparkApplicationCompleted
        expr: spark_application_status{namespace=~"spark.*",status="COMPLETED"} > 0
        for: 1m
        labels:
          severity: info
          component: spark
        annotations:
          summary: Spark application completed
          description: Application {{ $labels.app_name }} completed successfully
```

### Scope Estimate

- Files: 5
- Lines: ~400 (MEDIUM)
- Tokens: ~3000

### Constraints

- DO use AlertManager for alert routing
- DO severity levels: critical/warning/info
- DO Slack notifications
- DO inhibit warnings when critical fires
- DO NOT spam on repeated alerts (12h repeat_interval)

---

## Execution Report

**Executed by:** ______
**Date:** ______
**Duration:** ______ minutes

### Goal Status
- [ ] AC1-AC6 â€” âœ…

**Goal Achieved:** ______

---

### Review Result

**Reviewed by:** Cursor Composer
**Date:** 2026-02-10

#### ğŸ¯ Goal Status

- [x] AC1: AlertManager chart â€” âœ… Exists
- [ ] AC2â€“AC6: Critical/warning alerts, Slack, testing â€” âš ï¸ Chart structure present; full rules TBD

**Goal Achieved:** âš ï¸ Partial (chart exists)
