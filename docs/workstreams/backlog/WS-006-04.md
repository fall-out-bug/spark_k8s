# WS-006-04: GPU+Iceberg E2E Tests

## Goal
Создать 8 GPU+Iceberg E2E тестов для проверки одновременной работы RAPIDS GPU acceleration и Apache Iceberg table operations.

### Acceptance Criteria
1. 8 GPU+Iceberg E2E тестов созданы в `tests/e2e/phase06/test_06_04_gpu_iceberg_e2e.py`
2. NYC Taxi dataset конвертируется в Iceberg формат
3. RAPIDS GPU acceleration работает с Iceberg tables
4. GPU-accelerated Iceberg operations проверены
5. Сравнение GPU+Iceberg vs CPU+Iceberg
6. Метрики combo: GPU memory + Iceberg scan time
7. Валидация результатов (checksum)

## Context

**Существующая инфраструктура:**
- `tests/e2e/phase06/test_06_02_gpu_e2e.py` — GPU E2E тесты
- `tests/e2e/phase06/test_06_03_iceberg_e2e.py` — Iceberg E2E тесты
- RAPIDS+Iceberg Docker images из Phase 5

**Что нужно добавить:**
- Combo E2E тесты (GPU + Iceberg)
- RAPIDS + Iceberg integration tests
- Performance comparison
- Отдельный файл `tests/e2e/phase06/test_06_04_gpu_iceberg_e2e.py`

## Dependency
Phase 0 (Helm Charts), Phase 5 (GPU+Iceberg Final Images)

## Input Files
- `tests/e2e/phase06/conftest.py` (shared fixtures)
- `tests/e2e/phase06/test_06_02_gpu_e2e.py` (reference)
- `tests/e2e/phase06/test_06_03_iceberg_e2e.py` (reference)
- `charts/spark-3.5/values.yaml`
- `charts/spark-4.1/values.yaml`

## Steps

### 1. Create test_06_04_gpu_iceberg_e2e.py
- Создать `tests/e2e/phase06/test_06_04_gpu_iceberg_e2e.py`
- Implement 8 test scenarios

### 2. Implement test scenarios

**GPU+Iceberg Scenarios (Spark versions):**

**Spark 3.5.7 + RAPIDS + Iceberg:**
1. GPU Iceberg CREATE TABLE
2. GPU Iceberg READ (full scan)
3. GPU Iceberg TIME TRAVEL
4. GPU Iceberg SCHEMA EVOLUTION

**Spark 4.1.0 + RAPIDS + Iceberg:**
5. GPU Iceberg CREATE TABLE
6. GPU Iceberg READ (partition pruning)
7. GPU Iceberg MERGE (UPSERT)
8. GPU Iceberg SNAPSHOT MANAGEMENT

**Operations:**
- CREATE TABLE: GPU-accelerated Iceberg table creation
- READ: GPU scan of Iceberg table
- WRITE: GPU-accelerated append to Iceberg
- MERGE: GPU UPSERT into Iceberg
- TIME TRAVEL: GPU query of historical snapshot
- PARTITION PRUNING: GPU + Iceberg partitioning
- SCHEMA EVOLUTION: GPU + schema changes
- SNAPSHOT MANAGEMENT: GPU snapshot operations

### 3. GPU+Iceberg Metrics Collection
- gpu_iceberg_scan_time: GPU scan time for Iceberg table
- gpu_memory_iceberg: GPU memory for Iceberg operations
- speedup_vs_cpu_iceberg: GPU vs CPU for Iceberg
- gpu_iceberg_throughput: rows/second

### 4. Run tests
- `pytest tests/e2e/phase06/test_06_04_gpu_iceberg_e2e.py -v`
- Убедиться что все combo тесты проходят

## Code

### tests/e2e/phase06/test_06_04_gpu_iceberg_e2e.py

```python
"""GPU+Iceberg E2E Tests for Phase 6

Tests for RAPIDS GPU acceleration with Apache Iceberg tables.
"""

import pytest
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

SPARK_VERSIONS = ["3.5.7", "4.1.0"]

class TestGPUIcebergE2E:
    """GPU+Iceberg E2E tests for combo operations"""

    @pytest.mark.timeout(1200)
    @pytest.mark.gpu
    @pytest.mark.iceberg
    def test_01_spark_357_gpu_iceberg_create_table(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 3.5.7 GPU+Iceberg - CREATE TABLE"""
        start_time = time.time()

        # Configure both RAPIDS and Iceberg
        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")
        spark_connect_session.conf.set("spark.rapids.sql.python.enabled", "true")
        spark_connect_session.conf.set("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
        spark_connect_session.conf.set("spark.sql.catalog.spark_catalog.type", "hadoop")
        spark_connect_session.conf.set("spark.sql.catalog.spark_catalog.warehouse", "/tmp/iceberg/warehouse")

        # Load data with GPU
        df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.sample(0.01)

        # Create Iceberg table (GPU-accelerated)
        df.writeTo("spark_catalog.nyc_taxi.gpu_iceberg_table") \
            .using("iceberg") \
          .partitionedBy(col("passenger_count")) \
          .create()

        execution_time = time.time() - start_time

        # Verify table creation
        tables = spark_connect_session.sql("SHOW TABLES IN spark_catalog.nyc_taxi")
        table_exists = any(row['tableName'] == 'gpu_iceberg_table' for row in tables.collect())
        assert table_exists, "GPU+Iceberg table should be created"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "acceleration", "RAPIDS+Iceberg")

    @pytest.mark.timeout(1200)
    @pytest.mark.gpu
    @pytest.mark.iceberg
    def test_02_spark_357_gpu_iceberg_read_scan(self, spark_connect_session, metrics_collector):
        """Spark 3.5.7 GPU+Iceberg - READ full scan"""
        start_time = time.time()

        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")

        # GPU-accelerated Iceberg scan
        df = spark_connect_session.table("spark_catalog.nyc_taxi.gpu_iceberg_table")
        count = df.count()

        execution_time = time.time() - start_time

        assert count > 0, "GPU+Iceberg scan should return data"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "row_count", count)
        metrics_collector(self._testMethodName, "throughput", count / execution_time)

    @pytest.mark.timeout(1200)
    @pytest.mark.gpu
    @pytest.mark.iceberg
    def test_03_spark_357_gpu_iceberg_time_travel(self, spark_connect_session, metrics_collector):
        """Spark 3.5.7 GPU+Iceberg - TIME TRAVEL"""
        start_time = time.time()

        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")

        # Get snapshot
        snapshot_df = spark_connect_session.sql("""
            SELECT snapshot_id FROM spark_catalog.nyc_taxi.gpu_iceberg_table.snapshots
            ORDER BY committed_at DESC LIMIT 1
        """)
        snapshots = snapshot_df.collect()

        if len(snapshots) > 0:
            snapshot_id = snapshots[0]['snapshot_id']

            # GPU query of historical snapshot
            historical_df = spark_connect_session.table(f"spark_catalog.nyc_taxi.gpu_iceberg_table VERSION AS {snapshot_id}")
            count = historical_df.count()

            execution_time = time.time() - start_time

            metrics_collector(self._testMethodName, "execution_time", execution_time)
            metrics_collector(self._testMethodName, "snapshot_id", snapshot_id)
            metrics_collector(self._testMethodName, "historical_count", count)

    @pytest.mark.timeout(1200)
    @pytest.mark.gpu
    @pytest.mark.iceberg
    def test_04_spark_357_gpu_iceberg_schema_evolution(self, spark_connect_session, metrics_collector):
        """Spark 3.5.7 GPU+Iceberg - SCHEMA EVOLUTION"""
        start_time = time.time()

        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")

        # ADD COLUMN with GPU
        spark_connect_session.sql("""
            ALTER TABLE spark_catalog.nyc_taxi.gpu_iceberg_table
            ADD COLUMN gpu_processed boolean
        """)

        # Verify with GPU read
        df = spark_connect_session.table("spark_catalog.nyc_taxi.gpu_iceberg_table")
        schema = df.schema

        execution_time = time.time() - start_time

        assert 'gpu_processed' in [f.name for f in schema.fields], "Column should be added"

        metrics_collector(self._testMethodName, "execution_time", execution_time)

    @pytest.mark.timeout(1200)
    @pytest.mark.gpu
    @pytest.mark.iceberg
    def test_05_spark_410_gpu_iceberg_create_partitioned(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 4.1.0 GPU+Iceberg - CREATE TABLE with advanced partitioning"""
        start_time = time.time()

        # Configure GPU+Iceberg
        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")
        spark_connect_session.conf.set("spark.rapids.sql.batchSizeBytes", "1G")
        spark_connect_session.conf.set("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
        spark_connect_session.conf.set("spark.sql.catalog.local.type", "hadoop")

        df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.sample(0.01)

        # Create with multiple partition columns
        df.writeTo("local.nyc_taxi.gpu_iceberg_410") \
            .using("iceberg") \
          .partitionedBy(col("passenger_count"), col("vendor_id")) \
          .create()

        execution_time = time.time() - start_time

        metrics_collector(self._testMethodName, "execution_time", execution_time)

    @pytest.mark.timeout(1200)
    @pytest.mark.gpu
    @pytest.mark.iceberg
    def test_06_spark_410_gpu_iceberg_read_partitioned(self, spark_connect_session, metrics_collector):
        """Spark 4.1.0 GPU+Iceberg - READ with partition pruning"""
        start_time = time.time()

        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")

        # GPU scan with partition pruning
        df = spark_connect_session.table("local.nyc_taxi.gpu_iceberg_410") \
            .filter(col("passenger_count") == 1)
        count = df.count()

        execution_time = time.time() - start_time

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "filtered_count", count)

    @pytest.mark.timeout(1200)
    @pytest.mark.gpu
    @pytest.mark.iceberg
    def test_07_spark_410_gpu_iceberg_merge_upsert(self, spark_connect_session, metrics_collector):
        """Spark 4.1.0 GPU+Iceberg - MERGE (UPSERT)"""
        start_time = time.time()

        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")

        # GPU-accelerated MERGE
        spark_connect_session.sql("""
            MERGE INTO local.nyc_taxi.gpu_iceberg_410 AS target
            USING (SELECT 1 as passenger_count, 50.0 as total_amount) AS source
            ON target.passenger_count = source.passenger_count
            WHEN MATCHED THEN UPDATE SET target.total_amount = source.total_amount
        """)

        execution_time = time.time() - start_time

        metrics_collector(self._testMethodName, "execution_time", execution_time)

    @pytest.mark.timeout(1200)
    @pytest.mark.gpu
    @pytest.mark.iceberg
    def test_08_spark_410_gpu_iceberg_snapshot_management(self, spark_connect_session, metrics_collector):
        """Spark 4.1.0 GPU+Iceberg - SNAPSHOT MANAGEMENT"""
        start_time = time.time()

        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")

        # List snapshots with GPU
        snapshots_df = spark_connect_session.sql("""
            SELECT * FROM local.nyc_taxi.gpu_iceberg_410.snapshots
            ORDER BY committed_at DESC
        """)
        snapshots = snapshots_df.collect()

        execution_time = time.time() - start_time

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "snapshot_count", len(snapshots))

    @pytest.mark.timeout(1200)
    @pytest.mark.gpu
    @pytest.mark.iceberg
    def test_gpu_vs_cpu_iceberg_comparison(self, spark_connect_session, metrics_collector):
        """Compare GPU+Iceberg vs CPU+Iceberg performance"""
        # CPU + Iceberg
        start_time = time.time()
        spark_connect_session.conf.set("spark.rapids.sql.enabled", "false")
        cpu_df = spark_connect_session.table("spark_catalog.nyc_taxi.gpu_iceberg_table")
        cpu_count = cpu_df.count()
        cpu_time = time.time() - start_time

        # GPU + Iceberg
        start_time = time.time()
        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")
        gpu_df = spark_connect_session.table("spark_catalog.nyc_taxi.gpu_iceberg_table")
        gpu_count = gpu_df.count()
        gpu_time = time.time() - start_time

        speedup = cpu_time / gpu_time if gpu_time > 0 else 0

        metrics_collector("gpu_vs_cpu_iceberg", "cpu_time", cpu_time)
        metrics_collector("gpu_vs_cpu_iceberg", "gpu_time", gpu_time)
        metrics_collector("gpu_vs_cpu_iceberg", "speedup_factor", speedup)

        # Results should match
        assert cpu_count == gpu_count, "GPU and CPU should return same results"
```

## Scope Estimate
- Files: 1 (test_06_04_gpu_iceberg_e2e.py)
- LOC: ~500 (tests + combo logic)
- Scenarios: 8
- Size: MEDIUM
