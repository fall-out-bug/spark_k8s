# WS-009-03: CI/CD Integration

## Goal
Создать GitHub Actions workflows для автоматического запуска тестов с параллельным исполнением и агрегацией результатов.

### Acceptance Criteria
1. GitHub Actions workflow создан в `.github/workflows/test-suite.yml`
2. Parallel execution настроен (matrix strategy)
3. Result aggregation настроена (upload artifacts)
4. Scheduled runs реализованы (nightly, weekly)
5. PR comment с результатами тестов
6. Status checks интегрированы
7. Retry механизм для flaky tests
8. Test report доступен как artifact

## Context

**Существующая инфраструктура:**
- `tests/parallel/executor.py` — ParallelExecutor (будет создан в WS-009-01)
- `tests/parallel/aggregator.py` — ResultAggregator (будет расширен в WS-009-02)

**Что нужно добавить:**
- GitHub Actions workflows
- Matrix strategy для параллельного запуска
- Artifact upload для отчетов
- Scheduled triggers
- PR comments
- Status checks
- Retry mechanism

## Dependency
WS-009-02 (Result Aggregation)

## Input Files
- `tests/parallel/executor.py` (reference)
- `tests/parallel/aggregator.py` (reference)
- `charts/spark-3.5/values.yaml` (for test configuration)
- `charts/spark-4.1/values.yaml` (for test configuration)

## Steps

### 1. Create GitHub Actions workflow
- Создать `.github/workflows/test-suite.yml`
- Matrix strategy для разных тестовых сценариев
- Parallel execution с reusable workflows

### 2. Configure job dependencies
- Setup job (install dependencies, setup k8s)
- Test jobs (parallel execution)
- Aggregate job (collect results)
- Report job (generate and upload reports)

### 3. Implement scheduled runs
- Nightly smoke tests
- Weekly full matrix
- Monthly load tests

### 4. Implement PR integration
- Comment on PR with test results
- Status checks for commit
- Artifact links in comments

### 5. Implement retry mechanism
- Retry failed tests automatically
- Mark flaky tests
- Configurable retry count

### 6. Configure artifact retention
- Keep reports for 30 days
- Keep test history for trend analysis
- Cleanup old artifacts

## Code

### .github/workflows/test-suite.yml

```yaml
name: Spark K8s Test Suite

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Nightly smoke tests (2 AM UTC)
    - cron: '0 2 * * *'
    # Weekly full matrix (Sunday 3 AM UTC)
    - cron: '0 3 * * 0'
    # Monthly load tests (1st of month, 4 AM UTC)
    - cron: '0 4 1 * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        type: choice
        options:
          - smoke
          - e2e
          - load
          - all
      spark_version:
        description: 'Spark version'
        required: false
        type: choice
        options:
          - '3.5.7'
          - '3.5.8'
          - '4.1.0'
          - '4.1.1'
          - 'all'

env:
  HELM_VERSION: 'v3.12.0'
  K8S_VERSION: 'v1.27.3'
  PYTHON_VERSION: '3.10'

jobs:
  setup:
    name: Setup Test Environment
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      should-run-load: ${{ steps.check-trigger.outputs.should-run-load }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set test matrix
        id: set-matrix
        run: |
          if [ "${{ github.event_name }}" == "schedule" ]; then
            # Determine matrix based on schedule
            HOUR=$(date +%H -u)
            if [ "$HOUR" == "02" ]; then
              # Nightly smoke tests
              echo 'matrix={"include":[{"spark_version":"3.5.7","scenario":"baseline"},{"spark_version":"3.5.8","scenario":"baseline"},{"spark_version":"4.1.0","scenario":"baseline"},{"spark_version":"4.1.1","scenario":"baseline"}]}' >> $GITHUB_OUTPUT
            elif [ "$HOUR" == "03" ]; then
              # Weekly full matrix
              python scripts/generate-test-matrix.py > matrix.json
              echo "matrix=$(cat matrix.json)" >> $GITHUB_OUTPUT
            elif [ "$HOUR" == "04" ]; then
              # Monthly load tests
              echo 'matrix={"include":[{"spark_version":"3.5.7","scenario":"load"},{"spark_version":"4.1.0","scenario":"load"}]}' >> $GITHUB_OUTPUT
            fi
          elif [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            # User-specified tests
            python scripts/generate-test-matrix.py --type ${{ inputs.test_type }} --version ${{ inputs.spark_version }} > matrix.json
            echo "matrix=$(cat matrix.json)" >> $GITHUB_OUTPUT
          else
            # PR/Push: quick smoke tests
            echo 'matrix={"include":[{"spark_version":"3.5.7","scenario":"baseline"},{"spark_version":"4.1.0","scenario":"baseline"}]}' >> $GITHUB_OUTPUT
          fi

      - name: Check if load tests should run
        id: check-trigger
        run: |
          if [ "${{ github.event_name }}" == "schedule" ]; then
            DAY=$(date +%d -u)
            if [ "$DAY" == "01" ]; then
              echo "should-run-load=true" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: ${{ env.HELM_VERSION }}

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: ${{ env.K8S_VERSION }}

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PTHON_VERSION }}

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements-test.txt

      - name: Create kind cluster
        run: |
          curl -Lo ./kind https://kind.sigs.k8s.io/dl/${{ env.K8S_VERSION }}/kind-linux-amd64
          chmod +x ./kind
          sudo mv ./kind /usr/local/bin/kind
          kind create cluster --config=.github/kind-config.yaml

      - name: Verify cluster
        run: |
          kubectl cluster-info
          kubectl get nodes

  test:
    name: Test (${{ matrix.spark_version }} - ${{ matrix.scenario }})
    runs-on: ubuntu-latest
    needs: setup
    strategy:
      matrix: ${{ fromJson(needs.setup.outputs.matrix) }}
      fail-fast: false
      max-parallel: 4
    outputs:
      result-file: ${{ steps.run-tests.outputs.result-file }}
      status: ${{ steps.run-tests.outputs.status }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PREFIX }}

      - name: Restore Python cache
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements-test.txt

      - name: Download test data
        run: |
          wget -O /tmp/nyc-taxi.parquet https://example.com/test-data/nyc-taxi-11gb.parquet

      - name: Run tests
        id: run-tests
        env:
          SPARK_VERSION: ${{ matrix.spark_version }}
          TEST_SCENARIO: ${{ matrix.scenario }}
          KUBECONFIG: /root/.kube/config
        run: |
          mkdir -p test-results

          # Run tests with parallel executor
          python -m tests.parallel.executor \
            --spark-version $SPARK_VERSION \
            --scenario $TEST_SCENARIO \
            --workers 2 \
            --output test-results/${{ matrix.spark_version }}-${{ matrix.scenario }}.json \
            --junit test-results/${{ matrix.spark_version }}-${{ matrix.scenario }}.xml

          echo "result-file=test-results/${{ matrix.spark_version }}-${{ matrix.scenario }}.json" >> $GITHUB_OUTPUT

          # Check exit code
          if [ $? -eq 0 ]; then
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "status=failed" >> $GITHUB_OUTPUT
          fi

      - name: Retry failed tests
        if: steps.run-tests.outputs.status == 'failed'
        env:
          SPARK_VERSION: ${{ matrix.spark_version }}
          TEST_SCENARIO: ${{ matrix.scenario }}
        run: |
          # Retry once for flaky tests
          python -m tests.parallel.executor \
            --spark-version $SPARK_VERSION \
            --scenario $TEST_SCENARIO \
            --workers 2 \
            --retry-only \
            --output test-results/${{ matrix.spark_version }}-${{ matrix.scenario }}-retry.json \
            --junit test-results/${{ matrix.spark_version }}-${{ matrix.scenario }}-retry.xml

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.spark_version }}-${{ matrix.scenario }}
          path: |
            test-results/*.json
            test-results/*.xml
          retention-days: 30

      - name: Upload coverage
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: coverage-${{ matrix.spark_version }}-${{ matrix.scenario }}
          path: .coverage
          retention-days: 30

      - name: Generate PR comment
        if: github.event_name == 'pull_request'
        run: |
          python scripts/generate-pr-comment.py \
            --results ${{ steps.run-tests.outputs.result-file }} \
            --spark-version ${{ matrix.spark_version }} \
            --scenario ${{ matrix.scenario }} \
            --output pr-comment-${{ matrix.spark_version }}-${{ matrix.scenario }}.md

      - name: Upload PR comment
        uses: actions/upload-artifact@v3
        if: github.event_name == 'pull_request'
        with:
          name: pr-comment-${{ matrix.spark_version }}-${{ matrix.scenario }}
          path: pr-comment-*.md
          retention-days: 7

  aggregate:
    name: Aggregate Results
    runs-on: ubuntu-latest
    needs: [setup, test]
    if: always()
    outputs:
      summary: ${{ steps.aggregate.outputs.summary }}
      html-report: ${{ steps.aggregate.outputs.html-report }}
      trends: ${{ steps.aggregate.outputs.trends }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PREFIX }}

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements-test.txt

      - name: Download all test results
        uses: actions/download-artifact@v3
        with:
          path: test-results

      - name: Aggregate results
        id: aggregate
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_REF: ${{ github.ref }}
          GITHUB_SHA: ${{ github.sha }}
        run: |
          # Run aggregator
          python -m tests.parallel.aggregator \
            --input test-results \
            --output test-results/aggregated \
            --format json,html,junit \
            --history test-history.db \
            --branch ${GITHUB_REF#refs/heads/} \
            --commit $GITHUB_SHA \
            --trigger ${{ github.event_name }}

          # Generate summary
          echo "summary=$(cat test-results/aggregated/summary.json)" >> $GITHUB_OUTPUT
          echo "html-report=test-results/aggregated/report.html" >> $GITHUB_OUTPUT

          # Analyze trends
          python -m tests.parallel.trends \
            --db test-history.db \
            --branch ${GITHUB_REF#refs/heads/} \
            --output test-results/aggregated/trends.json

          echo "trends=$(cat test-results/aggregated/trends.json)" >> $GITHUB_OUTPUT

      - name: Upload aggregated results
        uses: actions/upload-artifact@v3
        with:
          name: aggregated-results
          path: test-results/aggregated
          retention-days: 90

      - name: Generate status badge
        run: |
          python scripts/generate-badge.py \
            --results test-results/aggregated/summary.json \
            --output test-results/aggregated/badge.svg

      - name: Upload badge
        uses: actions/upload-artifact@v3
        with:
          name: test-status-badge
          path: test-results/aggregated/badge.svg
          retention-days: 90

      - name: Update GitHub status
        if: github.event_name == 'push'
        run: |
          python scripts/update-github-status.py \
            --results test-results/aggregated/summary.json \
            --commit ${{ github.sha }}

      - name: Send alerts
        if: failure() || steps.aggregate.outputs.trends != ''
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          EMAIL_SMTP: ${{ secrets.EMAIL_SMTP }}
          EMAIL_USER: ${{ secrets.EMAIL_USER }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
        run: |
          python scripts/send-alerts.py \
            --results test-results/aggregated/summary.json \
            --trends test-results/aggregated/trends.json

  report:
    name: Generate Report
    runs-on: ubuntu-latest
    needs: aggregate
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download aggregated results
        uses: actions/download-artifact@v3
        with:
          name: aggregated-results
          path: test-results/aggregated

      - name: Post PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const summary = JSON.parse(fs.readFileSync('test-results/aggregated/summary.json', 'utf8'));
            const trends = JSON.parse(fs.readFileSync('test-results/aggregated/trends.json', 'utf8'));

            let comment = '## Test Results\\n\\n';
            comment += `**Total:** ${summary.total} | `;
            comment += `**Passed:** ${summary.passed} \\u2713 | `;
            comment += `**Failed:** ${summary.failed} \\u2717 | `;
            comment += `**Duration:** ${summary.duration.toFixed(1)}s\\n\\n`;

            if (summary.failed > 0) {
              comment += '### Failed Tests\\n\\n';
              for (const test of summary.failed_tests) {
                comment += `- ${test.name}: ${test.error}\\n`;
              }
            }

            if (trends.regressions.length > 0) {
              comment += '\\n### Regressions Detected\\n\\n';
              for (const trend of trends.regressions) {
                comment += `- ${trend.metric}: ${trend.change}%\\n`;
              }
            }

            comment += `\\n[Full Report](${summary.html_report_url})`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Deploy report to GitHub Pages
        if: github.ref == 'refs/heads/main'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: test-results/aggregated
```

### scripts/generate-test-matrix.py

```python
#!/usr/bin/env python
"""Generate test matrix for GitHub Actions"""

import argparse
import json

SPARK_VERSIONS = ['3.5.7', '3.5.8', '4.1.0', '4.1.1']

SCENARIOS = {
    'smoke': ['baseline', 'connect', 'gpu', 'iceberg'],
    'e2e': ['core-e2e', 'gpu-e2e', 'iceberg-e2e', 'standalone-e2e'],
    'load': ['baseline-load', 'gpu-load', 'iceberg-load', 'comparison-load'],
    'security': ['pss', 'scc', 'network-policies', 'rbac']
}

def generate_matrix(test_type='smoke', spark_version='all'):
    """Generate test matrix"""
    matrix = {'include': []}

    versions = SPARK_VERSIONS if spark_version == 'all' else [spark_version]
    scenarios = SCENARIOS.get(test_type, SCENARIOS['smoke'])

    for version in versions:
        for scenario in scenarios:
            matrix['include'].append({
                'spark_version': version,
                'scenario': scenario
            })

    return matrix

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--type', default='smoke', choices=['smoke', 'e2e', 'load', 'all'])
    parser.add_argument('--version', default='all')
    args = parser.parse_args()

    matrix = generate_matrix(args.type, args.version)
    print(json.dumps(matrix))
```

### scripts/generate-pr-comment.py

```python
#!/usr/bin/env python
"""Generate PR comment from test results"""

import argparse
import json
from pathlib import Path

def generate_comment(results_path, spark_version, scenario, output_path):
    """Generate PR comment"""
    with open(results_path) as f:
        results = json.load(f)

    summary = results.get('suite', {})
    tests = results.get('tests', {})

    comment = f"""## Test Results: {spark_version} - {scenario}

**Total:** {summary.get('tests', 0)} |
**Passed:** {summary.get('passed', 0)} \\u2713 |
**Failed:** {summary.get('failed', 0)} \\u2717 |
**Errors:** {summary.get('errors', 0)} ! |
**Duration:** {summary.get('duration', 0):.1f}s

"""

    if summary.get('failed', 0) > 0:
        comment += "### Failed Tests\\n\\n"
        for name, test in tests.items():
            if test.get('status') == 'failed':
                comment += f"- **{name}**\\n"
                if test.get('error_message'):
                    comment += f"  ```\\n  {test['error_message'][:200]}\\n  ```\\n"

    with open(output_path, 'w') as f:
        f.write(comment)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--results', required=True)
    parser.add_argument('--spark-version', required=True)
    parser.add_argument('--scenario', required=True)
    parser.add_argument('--output', required=True)
    args = parser.parse_args()

    generate_comment(args.results, args.spark_version, args.scenario, args.output)
```

### scripts/generate-badge.py

```python
#!/usr/bin/env python
"""Generate test status badge"""

import argparse
import json
from pathlib import Path

def generate_badge(results_path, output_path):
    """Generate SVG badge"""
    with open(results_path) as f:
        results = json.load(f)

    summary = results.get('suite', {})
    total = summary.get('tests', 0)
    passed = summary.get('passed', 0)
    pass_rate = (passed / total * 100) if total > 0 else 0

    if pass_rate >= 95:
        color = '#28a745'
        status = 'passing'
    elif pass_rate >= 80:
        color = '#ffc107'
        status = 'partial'
    else:
        color = '#dc3545'
        status = 'failing'

    svg = f"""<svg xmlns="http://www.w3.org/2000/svg" width="120" height="20">
  <linearGradient id="b" x2="0" y2="100%">
    <stop offset="0" stop-color="#bbb" stop-opacity=".1"/>
    <stop offset="1" stop-opacity=".1"/>
  </linearGradient>
  <mask id="a">
    <rect width="120" height="20" rx="3" fill="#fff"/>
  </mask>
  <g mask="url(#a)">
    <path fill="#555" d="M0 0h60v20H0z"/>
    <path fill="{color}" d="M60 0h60v20H60z"/>
    <path fill="url(#b)" d="M0 0h120v20H0z"/>
  </g>
  <g fill="#fff" text-anchor="middle" font-family="DejaVu Sans,Verdana,Geneva,sans-serif" font-size="11">
    <text x="30" y="15" fill="#010101" fill-opacity=".3">tests</text>
    <text x="30" y="14">tests</text>
    <text x="90" y="15" fill="#010101" fill-opacity=".3">{status}</text>
    <text x="90" y="14">{status}</text>
  </g>
</svg>"""

    with open(output_path, 'w') as f:
        f.write(svg)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--results', required=True)
    parser.add_argument('--output', required=True)
    args = parser.parse_args()

    generate_badge(args.results, args.output)
```

### scripts/update-github-status.py

```python
#!/usr/bin/env python
"""Update GitHub commit status"""

import argparse
import json
import os
from urllib.request import Request, urlopen

def update_status(results_path, commit, repo=None, token=None):
    """Update GitHub status"""
    with open(results_path) as f:
        results = json.load(f)

    summary = results.get('suite', {})
    passed = summary.get('passed', 0)
    total = summary.get('tests', 0)
    failed = summary.get('failed', 0)

    if failed == 0:
        state = 'success'
        description = f'{passed}/{total} tests passed'
    elif failed <= total * 0.1:
        state = 'failure'
        description = f'{passed}/{total} tests passed (some failures)'
    else:
        state = 'failure'
        description = f'{passed}/{total} tests passed (many failures)'

    if not token or not repo:
        print(f"Status: {state} - {description}")
        return

    url = f"https://api.github.com/repos/{repo}/statuses/{commit}"
    payload = json.dumps({
        'state': state,
        'description': description,
        'context': 'ci/tests'
    }).encode('utf-8')

    req = Request(url, data=payload, headers={
        'Content-Type': 'application/json',
        'Authorization': f'token {token}'
    })

    with urlopen(req) as response:
        print(f"Status updated: {state}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--results', required=True)
    parser.add_argument('--commit', required=True)
    parser.add_argument('--repo', default=os.environ.get('GITHUB_REPOSITORY'))
    parser.add_argument('--token', default=os.environ.get('GITHUB_TOKEN'))
    args = parser.parse_args()

    update_status(args.results, args.commit, args.repo, args.token)
```

### scripts/send-alerts.py

```python
#!/usr/bin/env python
"""Send alerts for test results"""

import argparse
import json
import os
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from urllib.parse import quote
from urllib.request import Request, urlopen

def send_slack_alert(webhook, message):
    """Send Slack alert"""
    if not webhook:
        return

    import json
    payload = json.dumps({'text': message}).encode('utf-8')
    req = Request(webhook, data=payload, headers={'Content-Type': 'application/json'})

    with urlopen(req) as response:
        print("Slack alert sent")

def send_email_alert(smtp_host, smtp_user, smtp_password, from_email, to_emails, subject, body):
    """Send email alert"""
    if not smtp_host or not to_emails:
        return

    msg = MIMEMultipart()
    msg['Subject'] = subject
    msg['From'] = from_email
    msg['To'] = ', '.join(to_emails)
    msg.attach(MIMEText(body, 'plain'))

    with smtplib.SMTP(smtp_host, 587) as server:
        server.starttls()
        if smtp_user and smtp_password:
            server.login(smtp_user, smtp_password)
        server.send_message(msg)

    print("Email alert sent")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--results', required=True)
    parser.add_argument('--trends', required=True)
    args = parser.parse_args()

    with open(args.results) as f:
        results = json.load(f)

    with open(args.trends) as f:
        trends = json.load(f)

    summary = results.get('suite', {})
    regressions = trends.get('regressions', [])

    if summary.get('failed', 0) > 0 or regressions:
        message = f"Test Alert: {summary['passed']}/{summary['tests']} passed"

        if regressions:
            message += f"\\n\\nRegressions detected:"
            for r in regressions:
                message += f"\\n- {r['metric']}: {r['change']}%"

        send_slack_alert(
            os.environ.get('SLACK_WEBHOOK'),
            message
        )

        send_email_alert(
            os.environ.get('EMAIL_SMTP'),
            os.environ.get('EMAIL_USER'),
            os.environ.get('EMAIL_PASSWORD'),
            'tests@spark-k8s.io',
            os.environ.get('EMAIL_TO', '').split(','),
            f"Test Alert: {summary['passed']}/{summary['tests']} passed",
            message
        )
```

## Scope Estimate
- Files: 6 (workflow + 5 scripts)
- LOC: ~700 (workflow YAML + scripts)
- Size: MEDIUM
