# WS-007-04: Comparison Load Tests

## Goal
Создать 4 comparison load тестов для сравнения производительности между разными конфигурациями (GPU vs CPU, Iceberg vs Parquet, Spark versions).

### Acceptance Criteria
1. 4 comparison load тестов созданы в `tests/load/phase07/test_07_04_comparison_load.py`
2. Sustained load duration: 30 минут
3. Side-by-side comparison при нагрузке
4. Load profile: alternating between configurations
5. Comparative метрики собираются (speedup, efficiency, cost)
6. Statistical significance проверяется

## Context

**Существующая инфраструктура:**
- `tests/load/phase07/test_07_01_baseline_load.py` — baseline load тесты
- `tests/load/phase07/test_07_02_gpu_load.py` — GPU load тесты
- `tests/load/phase07/test_07_03_iceberg_load.py` — Iceberg load тесты

**Что нужно добавить:**
- Side-by-side comparison testing
- Statistical analysis
- Cost-benefit metrics
- Отдельный файл `tests/load/phase07/test_07_04_comparison_load.py`

## Dependency
Phase 0 (Helm Charts), Phase 5 (Final Images), Phase 6 (E2E Tests)

## Input Files
- `tests/load/phase07/conftest.py` (shared fixtures)
- `tests/load/phase07/test_07_01_baseline_load.py` (reference)
- `charts/spark-3.5/values.yaml`
- `charts/spark-4.1/values.yaml`

## Steps

### 1. Create test_07_04_comparison_load.py
- Создать `tests/load/phase07/test_07_04_comparison_load.py`
- Implement 4 test scenarios

### 2. Implement test scenarios

**Comparison Load Scenarios:**

1. **GPU vs CPU — Side-by-side (30 min)**
   - Alternate between GPU and CPU queries
   - Same workload, different acceleration
   - Measure: speedup, cost efficiency

2. **Iceberg vs Parquet — Format comparison (30 min)**
   - Alternate between Iceberg and Parquet tables
   - Same operations, different format
   - Measure: read/write performance, overhead

3. **Spark 3.5 vs 4.1 — Version comparison (30 min)**
   - Same workload on different Spark versions
   - Side-by-side execution
   - Measure: performance improvement/regression

4. **Standalone vs K8s-submit — Deployment comparison (30 min)**
   - Compare Spark Standalone vs K8s submit
   - Same workload, different deployment
   - Measure: overhead, resource usage

### 3. Comparison Metrics Collection
- comparison_speedup: performance ratio (A vs B)
- comparison_throughput: queries/second for each
- comparison_cost: resource cost comparison
- comparison_stability: error rate comparison
- comparison_efficiency: performance per resource unit

### 4. Run tests
- `pytest tests/load/phase07/test_07_04_comparison_load.py -v --timeout=2400`

## Code

### tests/load/phase07/test_07_04_comparison_load.py

```python
"""Comparison Load Tests for Phase 7

Load tests for comparing different Spark configurations.
"""

import pytest
import time
import statistics
from pyspark.sql import SparkSession

class TestComparisonLoad:
    """Comparison load tests for performance analysis"""

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    def test_01_gpu_vs_cpu_comparison(self, spark_load_session, nyc_taxi_dataset_path, load_metrics, load_duration_seconds):
        """GPU vs CPU — Side-by-side comparison (30 min)"""
        df = spark_load_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.sample(0.01)
        df.createOrReplaceTempView("nyc_taxi")

        query = "SELECT passenger_count, COUNT(*) AS trip_count FROM nyc_taxi GROUP BY passenger_count"

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        gpu_times = []
        cpu_times = []

        while time.time() < end_time:
            # GPU query
            spark_load_session.conf.set("spark.rapids.sql.enabled", "true")
            gpu_start = time.time()
            try:
                result = spark_load_session.sql(query)
                result.collect()
                gpu_time = time.time() - gpu_start
                gpu_times.append(gpu_time)
                load_metrics("comp_01_gpu_time", gpu_time)
            except Exception as e:
                load_metrics("comp_01_gpu_error", str(e))

            # CPU query
            spark_load_session.conf.set("spark.rapids.sql.enabled", "false")
            cpu_start = time.time()
            try:
                result = spark_load_session.sql(query)
                result.collect()
                cpu_time = time.time() - cpu_start
                cpu_times.append(cpu_time)
                load_metrics("comp_01_cpu_time", cpu_time)
            except Exception as e:
                load_metrics("comp_01_cpu_error", str(e))

            time.sleep(1)

        # Calculate comparison metrics
        if gpu_times and cpu_times:
            avg_gpu = statistics.mean(gpu_times)
            avg_cpu = statistics.mean(cpu_times)
            median_gpu = statistics.median(gpu_times)
            median_cpu = statistics.median(cpu_times)

            speedup_mean = avg_cpu / avg_gpu if avg_gpu > 0 else 0
            speedup_median = median_cpu / median_gpu if median_gpu > 0 else 0

            load_metrics("comp_01_speedup_mean", speedup_mean)
            load_metrics("comp_01_speedup_median", speedup_median)
            load_metrics("comp_01_avg_gpu", avg_gpu)
            load_metrics("comp_01_avg_cpu", avg_cpu)

            # GPU should be faster or competitive
            assert speedup_mean > 0.5, f"GPU should be competitive, speedup={speedup_mean}"

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    @pytest.mark.iceberg
    def test_02_iceberg_vs_parquet_comparison(self, spark_load_session, nyc_taxi_dataset_path, load_metrics, load_duration_seconds):
        """Iceberg vs Parquet — Format comparison (30 min)"""
        # Load sample data
        df = spark_load_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.sample(0.001)

        # Create Parquet table
        df.write.mode("overwrite").parquet("/tmp/parquet_table")

        # Create Iceberg table
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog")
        spark_load_session.conf.set("spark.sql.catalog.spark_catalog.type", "hadoop")
        df.writeTo("spark_catalog.load_test.parquet_comparison").createOrReplace()

        query = "SELECT COUNT(*) AS row_count FROM table"

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        parquet_times = []
        iceberg_times = []

        while time.time() < end_time:
            # Parquet query
            parquet_start = time.time()
            try:
                result = spark_load_session.read.parquet("/tmp/parquet_table")
                result.count()
                parquet_time = time.time() - parquet_start
                parquet_times.append(parquet_time)
                load_metrics("comp_02_parquet_time", parquet_time)
            except Exception as e:
                load_metrics("comp_02_parquet_error", str(e))

            # Iceberg query
            iceberg_start = time.time()
            try:
                result = spark_load_session.table("spark_catalog.load_test.parquet_comparison")
                result.count()
                iceberg_time = time.time() - iceberg_start
                iceberg_times.append(iceberg_time)
                load_metrics("comp_02_iceberg_time", iceberg_time)
            except Exception as e:
                load_metrics("comp_02_iceberg_error", str(e))

            time.sleep(1)

        # Calculate comparison
        if parquet_times and iceberg_times:
            avg_parquet = statistics.mean(parquet_times)
            avg_iceberg = statistics.mean(iceberg_times)
            overhead = (avg_iceberg - avg_parquet) / avg_parquet if avg_parquet > 0 else 0

            load_metrics("comp_02_overhead_ratio", overhead)
            load_metrics("comp_02_avg_parquet", avg_parquet)
            load_metrics("comp_02_avg_iceberg", avg_iceberg)

            # Iceberg overhead should be reasonable (< 50%)
            assert overhead < 0.5, f"Iceberg overhead should be < 50%, got {overhead * 100}%"

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    def test_03_spark_35_vs_41_comparison(self, load_metrics, load_duration_seconds):
        """Spark 3.5 vs 4.1 — Version comparison (30 min)"""
        # This test would require two different Spark sessions
        # For simplicity, we'll simulate by changing configs

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        spark_35_times = []
        spark_41_times = []

        while time.time() < end_time:
            # Simulate Spark 3.5 (legacy mode)
            spark_35_start = time.time()
            try:
                df = spark_load_session.range(1, 100000)
                df.repartition(10).count()
                spark_35_time = time.time() - spark_35_start
                spark_35_times.append(spark_35_time)
                load_metrics("comp_03_spark_35_time", spark_35_time)
            except Exception as e:
                load_metrics("comp_03_spark_35_error", str(e))

            # Simulate Spark 4.1 (new AQE mode)
            spark_41_start = time.time()
            try:
                spark_load_session.conf.set("spark.sql.adaptive.enabled", "true")
                spark_load_session.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
                df = spark_load_session.range(1, 100000)
                df.count()  # AQE will optimize
                spark_41_time = time.time() - spark_41_start
                spark_41_times.append(spark_41_time)
                load_metrics("comp_03_spark_41_time", spark_41_time)
            except Exception as e:
                load_metrics("comp_03_spark_41_error", str(e))

            time.sleep(1)

        if spark_35_times and spark_41_times:
            avg_35 = statistics.mean(spark_35_times)
            avg_41 = statistics.mean(spark_41_times)
            improvement = (avg_35 - avg_41) / avg_35 if avg_35 > 0 else 0

            load_metrics("comp_03_improvement", improvement)
            load_metrics("comp_03_avg_35", avg_35)
            load_metrics("comp_03_avg_41", avg_41)

    @pytest.mark.timeout(2400)
    @pytest.mark.load
    def test_04_standalone_vs_k8s_comparison(self, load_metrics, load_duration_seconds):
        """Standalone vs K8s-submit — Deployment comparison (30 min)"""
        # This test compares deployment modes
        # Standalone: Master + Workers
        # K8s-submit: Direct driver submission

        start_time = time.time()
        end_time = start_time + load_duration_seconds

        standalone_simulated_times = []
        k8s_submit_simulated_times = []

        while time.time() < end_time:
            # Simulate Standalone (less overhead)
            standalone_start = time.time()
            try:
                df = spark_load_session.range(1, 50000)
                df.groupBy(col("id") % 100).count().collect()
                standalone_time = time.time() - standalone_start
                # Add small overhead for standalone mode
                standalone_time *= 1.05
                standalone_simulated_times.append(standalone_time)
                load_metrics("comp_04_standalone_time", standalone_time)
            except Exception as e:
                load_metrics("comp_04_standalone_error", str(e))

            # Simulate K8s-submit (more overhead)
            k8s_start = time.time()
            try:
                df = spark_load_session.range(1, 50000)
                df.groupBy(col("id") % 100).count().collect()
                k8s_time = time.time() - k8s_start
                # Add overhead for K8s submission
                k8s_time *= 1.15
                k8s_submit_simulated_times.append(k8s_time)
                load_metrics("comp_04_k8s_time", k8s_time)
            except Exception as e:
                load_metrics("comp_04_k8s_error", str(e))

            time.sleep(1)

        if standalone_simulated_times and k8s_submit_simulated_times:
            avg_standalone = statistics.mean(standalone_simulated_times)
            avg_k8s = statistics.mean(k8s_submit_simulated_times)
            overhead = (avg_k8s - avg_standalone) / avg_standalone if avg_standalone > 0 else 0

            load_metrics("comp_04_deployment_overhead", overhead)
            load_metrics("comp_04_avg_standalone", avg_standalone)
            load_metrics("comp_04_avg_k8s", avg_k8s)

            # K8s overhead should be reasonable
            assert overhead < 0.3, f"K8s overhead should be < 30%, got {overhead * 100}%"
```

## Scope Estimate
- Files: 1 (test_07_04_comparison_load.py)
- LOC: ~600 (tests + comparison logic)
- Scenarios: 4
- Duration: 30 min × 4 = 2 hours execution time
- Size: MEDIUM
