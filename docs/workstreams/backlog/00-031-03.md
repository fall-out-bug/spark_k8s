# WS-031-03: Airflow DAG - 5-Stage Pipeline

## Summary
Airflow DAG with 5+ stages, branching for parallel CatBoost training per borough.

## Scope
- Create Airflow DAG with 5 stages
- Branching for parallel training (Manhattan, Brooklyn, Queens, Bronx, Staten Island)
- Integration with Spark Standalone via spark-submit
- Model versioning in MinIO

## Acceptance Criteria
- [ ] DAG visible in Airflow UI
- [ ] All 5 stages execute successfully
- [ ] Branching works (parallel training)
- [ ] Models saved to MinIO with versioning
- [ ] Alerts on failure

## DAG Structure

```
                    ┌─────────────────┐
                    │  1. ingest_data │
                    │  (check MinIO)  │
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │ 2. feature_prep │
                    │  (Spark ETL)    │
                    └────────┬────────┘
                             │
          ┌──────────────────┼──────────────────┬──────────────────┐
          │                  │                  │                  │
   ┌──────▼──────┐    ┌──────▼──────┐    ┌──────▼──────┐    ┌──────▼──────┐
   │ 3a. train_  │    │ 3b. train_  │    │ 3c. train_  │    │ 3d. train_  │
   │ manhattan   │    │ brooklyn    │    │ queens      │    │ bronx_si    │
   │ (CatBoost)  │    │ (CatBoost)  │    │ (CatBoost)  │    │ (CatBoost)  │
   └──────┬──────┘    └──────┬──────┘    └──────┬──────┘    └──────┬──────┘
          │                  │                  │                  │
          └──────────────────┼──────────────────┴──────────────────┘
                             │
                    ┌────────▼────────┐
                    │   4. validate   │
                    │   (MAPE < 15%)  │
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │ 5. predict_save │
                    │  (7-day forecast│
                    │   + model save) │
                    └─────────────────┘
```

## Technical Details

### DAG File
```python
# dags/nyc_taxi_ml_pipeline.py

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta

default_args = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'start_date': datetime(2026, 2, 20),
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'nyc_taxi_ml_pipeline',
    default_args=default_args,
    schedule_interval='0 6 * * *',  # Daily at 6am
    catchup=False,
) as dag:

    # Task 1: Check data availability
    check_data = PythonOperator(
        task_id='check_data_availability',
        python_callable=check_minio_data,
    )

    # Task 2: Feature preparation
    feature_prep = BashOperator(
        task_id='feature_preparation',
        bash_command='spark-submit --master spark://scenario2-spark-35-standalone-master:7077 /dags/spark_jobs/taxi_feature_engineering.py',
    )

    # Task 3: Parallel training (TaskGroup)
    with TaskGroup('train_models') as train_group:
        for borough in ['manhattan', 'brooklyn', 'queens', 'bronx_si']:
            PythonOperator(
                task_id=f'train_{borough}',
                python_callable=train_catboost,
                op_kwargs={'borough': borough},
            )

    # Task 4: Validation
    validate = PythonOperator(
        task_id='validate_models',
        python_callable=validate_all_models,
    )

    # Task 5: Prediction & Save
    predict = PythonOperator(
        task_id='predict_and_save',
        python_callable=generate_7day_forecast,
    )

    check_data >> feature_prep >> train_group >> validate >> predict
```

### Airflow Configuration
- Use existing `spark-35-airflow-sa` namespace
- Spark connection: `spark://scenario2-spark-35-standalone-master:7077`
- MinIO connection: `http://minio.spark-infra.svc.cluster.local:9000`

## Dependencies
- WS-031-01 (data available)
- WS-031-02 (feature engineering script)
- Airflow deployed with SparkSubmitOperator support

## Estimated Complexity
High - DAG orchestration + branching

## Files to Create/Modify
- `dags/nyc_taxi_ml_pipeline.py` (new)
- `dags/spark_jobs/` (directory)
