# WS-031-06: Prediction Pipeline - 7-Day Forecast

## Summary
Generate and store 7-day revenue and trip count forecasts per zone.

## Scope
- Load trained models from MinIO
- Generate features for next 7 days
- Run inference per borough
- Store predictions in MinIO

## Acceptance Criteria
- [ ] Models loaded from MinIO
- [ ] 7-day forecast generated for all zones
- [ ] Predictions stored in `s3a://nyc-taxi/predictions/`
- [ ] Forecast API/data available for consumption

## Technical Details

### Prediction Schema
```python
prediction_schema = {
    "forecast_date": "date",           # Date of prediction
    "zone_id": "int64",                # TLC zone ID
    "borough": "string",               # Borough name
    "predicted_revenue": "float64",    # Predicted daily revenue
    "predicted_trips": "float64",      # Predicted daily trip count
    "revenue_lower": "float64",        # Prediction interval lower
    "revenue_upper": "float64",        # Prediction interval upper
    "trips_lower": "float64",
    "trips_upper": "float64",
    "model_version": "string",         # Model version used
    "forecast_generated": "timestamp", # When forecast was made
}
```

### Inference Script
```python
# dags/spark_jobs/generate_forecast.py

from pyspark.sql import SparkSession
import pickle
from datetime import datetime, timedelta
import pandas as pd

def generate_7day_forecast(spark, model_version: str):
    """Generate 7-day forecast using trained models."""

    # Load models
    models_path = f"s3a://ml-models/taxi-predictor/{model_version}/"

    # Load feature data (last 30 days for historical aggregates)
    features = spark.read.format("iceberg").load("nyc_taxi.features")
    features_latest = features.filter(features.date >= date_sub(current_date(), 30))

    # Generate future dates
    future_dates = [(datetime.now() + timedelta(days=i)).date() for i in range(1, 8)]

    predictions = []
    for borough in ['manhattan', 'brooklyn', 'queens', 'bronx_si']:
        # Load borough model
        model_binary = spark.read.format("binaryFile").load(f"{models_path}{borough}.pkl").first().content
        models = pickle.loads(model_binary)

        # Get borough data
        borough_data = features_latest.filter(features_latest.borough == borough)

        # For each future date, generate features and predict
        for forecast_date in future_dates:
            # Create features for this date (using historical aggregates)
            future_features = create_future_features(borough_data, forecast_date)

            # Predict
            pred_revenue = models['revenue'].predict(future_features)
            pred_trips = models['trips'].predict(future_features)

            predictions.append({
                'forecast_date': forecast_date,
                'borough': borough,
                'predicted_revenue': pred_revenue.sum(),
                'predicted_trips': pred_trips.sum(),
                'model_version': model_version,
                'forecast_generated': datetime.now(),
            })

    # Save predictions
    pred_df = spark.createDataFrame(predictions)
    pred_df.write.mode('overwrite').parquet(f"s3a://nyc-taxi/predictions/{model_version}/")

    return predictions
```

### Output Format
```
s3a://nyc-taxi/predictions/
├── v20260220/
│   ├── part-00000.parquet
│   └── _SUCCESS
├── latest/  # symlink or copy
└── history/
    └── v20260219/
```

### API Endpoint (Optional)
```python
# FastAPI for serving predictions
from fastapi import FastAPI
app = FastAPI()

@app.get("/predictions/{borough}")
async def get_predictions(borough: str, days: int = 7):
    df = spark.read.parquet("s3a://nyc-taxi/predictions/latest/")
    return df.filter(df.borough == borough).limit(days).toPandas().to_dict()
```

## Dependencies
- WS-031-04 (trained models)
- Feature data available
- Spark Standalone running

## Estimated Complexity
Medium - Inference pipeline + storage

## Files to Create/Modify
- `dags/spark_jobs/generate_forecast.py` (new)
- `dags/spark_jobs/feature_utils.py` (new)
