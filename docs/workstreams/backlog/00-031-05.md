# WS-031-05: Jupyter Notebook - Interactive Analysis

## Summary
Jupyter notebook for interactive ML development with Spark Connect and Pandas API.

## Scope
- Connect to Spark via Spark Connect (Standalone backend)
- Use Pandas API on Spark for data exploration
- Train/test models interactively
- Visualize predictions

## Acceptance Criteria
- [ ] Notebook connects to Spark Connect successfully
- [ ] Pandas API on Spark works with NYC Taxi data
- [ ] Interactive model training works
- [ ] Visualizations render correctly
- [ ] Notebook runs end-to-end

## Technical Details

### Notebook Structure
```markdown
# NYC Taxi ML Pipeline - Interactive Analysis

## 1. Setup & Connection
- Connect to Spark Connect
- Configure MinIO access
- Load zone lookup

## 2. Data Exploration
- Load raw TLC data
- Basic statistics
- Distribution analysis

## 3. Feature Engineering (Interactive)
- Temporal features
- Geospatial features
- Historical aggregates

## 4. Model Training
- Train/test split
- CatBoost training
- Hyperparameter tuning

## 5. Evaluation
- MAPE by borough
- Feature importance
- Residual analysis

## 6. Prediction
- Generate 7-day forecast
- Save to MinIO
- Visualization
```

### Connection Code
```python
# Section 1: Setup
from pyspark.sql import SparkSession
import pyspark.pandas as ps

# Connect to Spark Connect (Standalone backend)
spark = SparkSession.builder \
    .remote("sc://scenario1-spark-35-connect:15002") \
    .appName("nyc-taxi-exploration") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio.spark-infra.svc.cluster.local:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
    .getOrCreate()

# Use Pandas API on Spark
ps.set_option("compute.default_index_type", "distributed")

# Load data
df = ps.read_parquet("s3a://nyc-taxi/raw/*.parquet")
df.head()
```

### Visualization Example
```python
# Section 6: Prediction Visualization
import plotly.express as px

# Aggregate predictions by date
predictions = ps.read_parquet("s3a://nyc-taxi/predictions/7day_forecast/")
daily_pred = predictions.groupby(['date', 'borough']).agg({
    'predicted_revenue': 'sum',
    'predicted_trips': 'sum'
}).reset_index()

# Convert to pandas for plotting
daily_pred_pd = daily_pred.to_pandas()

# Plot
fig = px.line(daily_pred_pd, x='date', y='predicted_revenue', color='borough',
              title='7-Day Revenue Forecast by Borough')
fig.show()
```

### Notebook Location
```
examples/nyc_taxi_ml_pipeline.ipynb
```

## Dependencies
- WS-031-01 (data in MinIO)
- Spark Connect running (scenario1)
- Jupyter pod with required libraries

## Estimated Complexity
Medium - Notebook development + visualization

## Files to Create/Modify
- `examples/nyc_taxi_ml_pipeline.ipynb` (new)
- `examples/requirements.txt` (update if needed)
