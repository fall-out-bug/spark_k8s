---
ws_id: 00-008-06
feature: F08
status: backlog
size: SMALL
project_id: 00
github_issue: null
assignee: null
depends_on:
  - 00-008-01  # Jupyter GPU/Iceberg scenarios
---

## WS-00-008-06: Dataset generation utilities

### üéØ Goal

**What must WORK after completing this WS:**
- Utility script for generating test datasets
- NYC Taxi sample dataset (~100MB Parquet)
- Support for different sizes (small, medium, large)

**Acceptance Criteria:**
- [ ] AC1: scripts/tests/data/generate-dataset.sh created
- [ ] AC2: Generates NYC Taxi sample in Parquet format
- [ ] AC3: Supports --size flag (small/medium/large)
- [ ] AC4: Dataset saved to scripts/tests/data/nyc-taxi-sample.parquet
- [ ] AC5: Integration with smoke scenarios (DATASET_PATH variable)

**‚ö†Ô∏è WS is NOT complete until Goal is achieved (all AC ‚úÖ).**

---

### Context

Current smoke tests use minimal datasets (pi.py). For realistic testing, we need a proper dataset that exercises Spark's data processing capabilities.

### Dependencies

- WS-008-01: Jupyter GPU/Iceberg scenarios (needs dataset for Iceberg testing)

### Input Files

- None (creating new utility)

### Steps

1. **Create generate-dataset.sh script**

   - Download NYC Taxi data sample or generate synthetic data
   - Convert to Parquet format
   - Support size variations

2. **Create data directory**

   ```bash
   mkdir -p scripts/tests/data
   ```

3. **Add DATASET_PATH variable to scenarios**

### Code

```bash
#!/bin/bash
# scripts/tests/data/generate-dataset.sh

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DATA_DIR="${SCRIPT_DIR}/data"
DATASET_NAME="nyc-taxi-sample"
DATASET_PATH="${DATA_DIR}/${DATASET_NAME}.parquet"

SIZE="${1:-small}"
ROWS_SMALL=1000
ROWS_MEDIUM=10000
ROWS_LARGE=100000

generate_dataset() {
    local rows="$1"

    mkdir -p "$DATA_DIR"

    log_info "Generating NYC Taxi sample dataset (~$rows rows)"

    # Use Python to generate synthetic taxi data
    python3 << EOF
import pandas as pd
import numpy as np

# Generate synthetic taxi trip data
np.random.seed(42)
n = $rows

data = {
    'VendorID': np.random.choice([1, 2], n),
    'tpep_pickup_datetime': pd.date_range('2023-01-01', periods=n, freq='1min'),
    'tpep_dropoff_datetime': pd.date_range('2023-01-01', periods=n, freq='1min') + pd.Timedelta(minutes=np.random.randint(5, 30, n)),
    'passenger_count': np.random.randint(1, 6, n),
    'trip_distance': np.random.uniform(0.5, 20, n),
    'RatecodeID': np.random.choice([1, 2, 3, 4, 5], n),
    'store_and_fwd_flag': np.random.choice(['N', 'Y'], n),
    'PULocationID': np.random.randint(1, 266, n),
    'DOLocationID': np.random.randint(1, 266, n),
    'payment_type': np.random.choice([1, 2, 3, 4, 5, 6], n),
    'fare_amount': np.random.uniform(5, 100, n),
    'extra': np.random.uniform(0, 10, n),
    'mta_tax': np.random.uniform(0, 1, n),
    'tip_amount': np.random.uniform(0, 20, n),
    'tolls_amount': np.random.uniform(0, 15, n),
    'improvement_surcharge': 0.3,
    'total_amount': np.random.uniform(5, 150, n)
}

df = pd.DataFrame(data)
df.to_parquet('$DATASET_PATH', index=False)
print(f"Generated {len(df)} rows to $DATASET_PATH")
EOF

    local size_bytes
    size_bytes=$(du -b "$DATASET_PATH" | cut -f1)
    local size_mb
    size_mb=$((size_bytes / 1024 / 1024))

    log_success "Dataset generated: $DATASET_PATH (~${size_mb}MB)"
}

case "$SIZE" in
    small)
        generate_dataset "$ROWS_SMALL"
        ;;
    medium)
        generate_dataset "$ROWS_MEDIUM"
        ;;
    large)
        generate_dataset "$ROWS_LARGE"
        ;;
    *)
        log_error "Invalid size: $SIZE (use: small, medium, large)"
        exit 1
        ;;
esac
```

### Scope Estimate

- Files: 1
- Lines: ~400 (SMALL)
- Tokens: ~3000

### Completion Criteria

```bash
# Generate dataset
bash scripts/tests/data/generate-dataset.sh small

# Verify file exists
ls -lh scripts/tests/data/nyc-taxi-sample.parquet

# Test in scenario
export DATASET_PATH="$(pwd)/scripts/tests/data/nyc-taxi-sample.parquet"
```

### Constraints

- DO NOT download from external URLs (generate synthetically)
- DO NOT commit large files to git (add to .gitignore)
- DO NOT use real PII data (synthetic only)

---

## Execution Report

**Executed by:** ______
**Date:** ______
**Duration:** ______ minutes

### Goal Status
- [ ] AC1-AC5 ‚Äî ‚úÖ

**Goal Achieved:** ______

### Files Changed
| File | Action | LOC |
|------|--------|-----|
|      |        |     |

### Statistics
- **Files Changed:** ______
- **Lines Added:** ______
- **Lines Removed:** ______
- **Test Coverage:** ______ %
- **Tests Passed:** ______
- **Tests Failed:** ______

### Deviations from Plan
- ______

### Commit
______
