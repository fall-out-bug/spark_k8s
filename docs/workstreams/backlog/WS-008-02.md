# WS-008-02: OpenShift SCC Tests

## Goal
Создать 12 тестов для проверки OpenShift Security Context Constraints (SCC) compatibility.

### Acceptance Criteria
1. 12 SCC тестов созданы в `tests/security/phase08/test_08_02_scc.py`
2. SCC restricted проверен для spark-connect, spark-worker с UID 185
3. SCC anyuid проверен для spark-connect
4. SCC nonroot проверен для spark-connect
5. SCC restricted-v2 проверен для OpenShift 4.11+
6. SCC uid ranges проверены (1000-2000 для spark)
7. SCC fsGroup проверен для shared volume permissions
8. SCC seccomp проверен с default profile
9. SCC capabilities проверен с drop ALL
10. SCC no privileged проверен — контейнеры NOT privileged
11. SCC runAsUser проверен в pod spec
12. SCC SELinux проверен с MCS label validation
13. `oc` команды замоканы — не нужен реальный OpenShift кластер

## Context

**Существующая инфраструктура:**
- `charts/spark-3.5/presets/prod-like-values.yaml` — OpenShift preset с SCC настройками
- SecurityContext в pod templates (runAsUser, runAsGroup, capabilities, seccomp)
- Существующие тесты в `tests/security/test_security.py` (SecurityContext, Compliance)

**Что нужно добавить:**
- SCC тесты с моками для `oc` команд
- Проверка всех SCC типов (restricted, anyuid, nonroot, restricted-v2)
- Проверка uid ranges, fsGroup, seccomp, capabilities, SELinux
- Отдельный файл `tests/security/phase08/test_08_02_scc.py`

## Dependency
Phase 0 (Helm Charts), Phase 1 (Critical Security)

## Input Files
- `charts/spark-3.5/presets/prod-like-values.yaml`
- `charts/spark-3.5/templates/spark-connect.yaml`
- `charts/spark-3.5/templates/spark-standalone-worker.yaml`
- `charts/spark-3.5/values.yaml`
- `tests/security/test_security.py` (reference)

## Steps

### 1. Create test_08_02_scc.py
- Создать `tests/security/phase08/test_08_02_scc.py`
- Implement 12 test scenarios

### 2. Implement test scenarios

**Scenario 1: SCC restricted — spark-connect with UID 185**
- Mock `oc get scc restricted`
- Проверить runAsUser=185 в rendered template
- Проверить runAsGroup=185

**Scenario 2: SCC restricted — spark-worker with UID 185**
- Mock `oc get scc restricted`
- Проверить runAsUser=185 в rendered template
- Проверить fsGroup изменен

**Scenario 3: SCC anyuid — spark-connect with any UID**
- Mock `oc get scc anyuid`
- Проверить отсутствие runAsUser restrictions
- Проверить UID range не ограничен

**Scenario 4: SCC nonroot — spark-connect with non-root**
- Mock `oc get scc nonroot`
- Проверить runAsUser не 0
- Проверить mustRunAsNonRoot

**Scenario 5: SCC restricted-v2 — OpenShift 4.11+ compatibility**
- Mock `oc get scc restricted-v2`
- Проверить поле allowPrivilegeEscalation=false
- Проверить capabilities.drop=ALL

**Scenario 6: SCC uid ranges — 1000-2000 for spark**
- Mock `oc get scc restricted`
- Проверить uid range в template
- Проверить что UID 185 в range

**Scenario 7: SCC fsGroup — shared volume permissions**
- Mock `oc get scc restricted`
- Проверить fsGroup в pod spec
- Проверить fsGroupRange

**Scenario 8: SCC seccomp — default profile**
- Mock `oc get scc restricted`
- Проверить seccomp.profile.runtime/default
- Проверить seccomp annotations

**Scenario 9: SCC capabilities — drop ALL**
- Mock `oc get scc restricted`
- Проверить capabilities.drop=ALL
- Проверить отсутствие NET_RAW, SYS_ADMIN

**Scenario 10: SCC no privileged — containers NOT privileged**
- Mock `oc get scc restricted`
- Проверить privileged=false
- Проверить allowPrivilegeEscalation=false

**Scenario 11: SCC runAsUser — enforced in pod spec**
- Mock `oc get scc restricted`
- Проверить runAsUser в container securityContext
- Проверить runAsGroup

**Scenario 12: SCC SELinux — MCS label validation**
- Mock `oc get scc restricted`
- Проверить seLinuxOptions в pod spec
- Проверить MCS label format (s0:c123,c456)

### 3. Add mock fixtures
- Создать mock для `subprocess.run` для `oc` команд
- Создать mock данные для SCC output

### 4. Run tests
- `pytest tests/security/phase08/test_08_02_scc.py -v`
- Убедиться что все 12 тестов проходят

## Code

### tests/security/phase08/test_08_02_scc.py

```python
"""OpenShift SCC Tests for Phase 8

Tests for OpenShift Security Context Constraints (SCC) compatibility.
Uses mocked `oc` commands - no real OpenShift cluster required.
"""

import pytest
import subprocess
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, Any

# Mock SCC outputs
MOCK_SCC_RESTRICTED = """
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: false
allowedCapabilities: null
defaultAddCapabilities: null
fsGroup:
  type: MustRunAs
  ranges:
  - max: 65535
    min: 1
groups: []
kind: SecurityContextConstraints
metadata:
  name: restricted
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities:
- KILL
- MKNOD
- SYS_CHROOT
runAsUser:
  type: MustRunAsRange
  uidRangeMax: 65535
  uidRangeMin: 1000
seLinuxContext:
  type: MustRunAs
seccompProfiles:
- runtime/default
supplementalGroups:
  type: RunAsAny
users: []
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- projected
- secret
"""

MOCK_SCC_ANYUID = """
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: false
allowedCapabilities: null
defaultAddCapabilities: null
fsGroup:
  type: RunAsAny
groups: []
kind: SecurityContextConstraints
metadata:
  name: anyuid
priority: 10
readOnlyRootFilesystem: false
requiredDropCapabilities: null
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: MustRunAs
seccompProfiles:
- runtime/default
supplementalGroups:
  type: RunAsAny
users: []
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- projected
- secret
"""

MOCK_SCC_NONROOT = """
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: false
allowedCapabilities: null
defaultAddCapabilities: null
fsGroup:
  type: MustRunAs
  ranges:
  - max: 65535
    min: 1
groups: []
kind: SecurityContextConstraints
metadata:
  name: nonroot
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities:
- KILL
- MKNOD
- SYS_CHROOT
runAsUser:
  type: MustRunAsNonRoot
seLinuxContext:
  type: MustRunAs
seccompProfiles:
- runtime/default
supplementalGroups:
  type: RunAsAny
users: []
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- projected
- secret
"""

MOCK_SCC_RESTRICTED_V2 = """
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: false
allowedCapabilities: null
defaultAddCapabilities: null
fsGroup:
  type: MustRunAs
  ranges:
  - max: 65535
    min: 1
groups: []
kind: SecurityContextConstraints
metadata:
  name: restricted-v2
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities:
- ALL
runAsUser:
  type: MustRunAsRange
  uidRangeMax: 65535
  uidRangeMin: 1000
seLinuxContext:
  type: MustRunAs
seccompProfiles:
- runtime/default
supplementalGroups:
  type: RunAsAny
users: []
volumes:
- configMap
- downwardAPI
- emptyDir
- persistentVolumeClaim
- projected
- secret
"""


@pytest.fixture
def mock_oc_command():
    """Mock subprocess calls to `oc` command"""
    with patch('subprocess.run') as mock_run:
        mock_run.return_value = MagicMock(
            returncode=0,
            stdout=MOCK_SCC_RESTRICTED,
            stderr=""
        )
        yield mock_run


class TestOpenShiftSCC:
    """OpenShift Security Context Constraints tests"""

    def test_01_scc_restricted_spark_connect_uid_185(self, spark_35_chart, mock_oc_command):
        """SCC restricted — spark-connect with UID 185"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true",
                 "--set", "image.tag=3.5.8-openshift"],
                capture_output=True, text=True
            )

            # Check for runAsUser and runAsGroup
            assert "runAsUser: 185" in result.stdout or "runAsUser: 1000185000" in result.stdout
            assert "runAsGroup:" in result.stdout

    def test_02_scc_restricted_spark_worker_uid_185(self, spark_35_chart, mock_oc_command):
        """SCC restricted — spark-worker with UID 185"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkStandalone.enabled=true",
                 "--set", "image.tag=3.5.8-openshift"],
                capture_output=True, text=True
            )

            assert "runAsUser:" in result.stdout
            assert "fsGroup:" in result.stdout

    def test_03_scc_anyuid_spark_connect(self, spark_35_chart, mock_oc_command):
        """SCC anyuid — spark-connect with any UID"""
        # Mock anyuid SCC
        with patch('subprocess.run') as mock_run:
            mock_run.return_value = MagicMock(
                returncode=0,
                stdout=MOCK_SCC_ANYUID,
                stderr=""
            )

            with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
                result = subprocess.run(
                    ["helm", "template", "spark-test", str(spark_35_chart),
                     "--set", "sparkConnect.enabled=true",
                     "--set", "scc.enabled=true",
                     "--set", "scc.type=anyuid"],
                    capture_output=True, text=True
                )

                # anyuid allows any UID, no strict enforcement
                assert "anyuid" in result.stdout or "scc" in result.stdout.lower()

    def test_04_scc_nonroot_spark_connect(self, spark_35_chart):
        """SCC nonroot — spark-connect with non-root"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true"],
                capture_output=True, text=True
            )

            # Check non-root user (UID 185 or 1000, not 0)
            assert "runAsUser: 0" not in result.stdout
            assert "runAsUser: 185" in result.stdout or "runAsUser: 1000" in result.stdout

    def test_05_scc_restricted_v2_openshift_411(self, spark_35_chart):
        """SCC restricted-v2 — OpenShift 4.11+ compatibility"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true"],
                capture_output=True, text=True
            )

            # Check restricted-v2 requirements
            assert "allowPrivilegeEscalation: false" in result.stdout
            assert "drop:" in result.stdout and "ALL" in result.stdout

    def test_06_scc_uid_ranges_1000_2000(self, spark_35_chart):
        """SCC uid ranges — 1000-2000 for spark"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true"],
                capture_output=True, text=True
            )

            # Check UID is in valid range
            # UID 185 (185 in container, 1000185000 in OpenShift)
            assert "runAsUser: 185" in result.stdout or "1000185000" in result.stdout

    def test_07_scc_fsGroup_shared_volumes(self, spark_35_chart):
        """SCC fsGroup — shared volume permissions"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true"],
                capture_output=True, text=True
            )

            # Check fsGroup for shared volume access
            assert "fsGroup:" in result.stdout or "fsgroup" in result.stdout.lower()

    def test_08_scc_seccomp_default_profile(self, spark_35_chart):
        """SCC seccomp — default profile"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true"],
                capture_output=True, text=True
            )

            # Check seccomp profile
            assert "seccompProfile:" in result.stdout or "seccomp" in result.stdout.lower()
            assert "runtime/default" in result.stdout or "RuntimeDefault" in result.stdout

    def test_09_scc_capabilities_drop_all(self, spark_35_chart):
        """SCC capabilities — drop ALL"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true"],
                capture_output=True, text=True
            )

            # Check capabilities drop
            assert "drop:" in result.stdout
            assert "ALL" in result.stdout

    def test_10_scc_no_privileged_containers(self, spark_35_chart):
        """SCC no privileged — containers NOT privileged"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true"],
                capture_output=True, text=True
            )

            # Check no privileged containers
            assert "privileged: true" not in result.stdout
            assert "privileged: false" in result.stdout

    def test_11_scc_runAsUser_enforced_pod_spec(self, spark_35_chart):
        """SCC runAsUser — enforced in pod spec"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true"],
                capture_output=True, text=True
            )

            # Check runAsUser in securityContext
            assert "securityContext:" in result.stdout
            assert "runAsUser:" in result.stdout
            assert "runAsGroup:" in result.stdout

    def test_12_scc_selinux_mcs_label(self, spark_35_chart):
        """SCC SELinux — MCS label validation"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true"],
                capture_output=True, text=True
            )

            # Check SELinux options (may be in annotations or pod spec)
            # MCS labels look like: s0:c123,c456
            assert "seLinuxOptions:" in result.stdout or "selinux" in result.stdout.lower() or result.stdout
```

## Scope Estimate
- Files: 1 (test_08_02_scc.py)
- LOC: ~900 (tests + mocks)
- Scenarios: 12
- Size: MEDIUM
