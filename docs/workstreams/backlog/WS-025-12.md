---
ws_id: 025-12
feature: F25
status: backlog
size: LARGE
project_id: 00
github_issue: null
assignee: null
depends_on: ["025-03", "025-04"]
---

## WS-025-12: Spark Job Tracing + Profiling Dashboards and Recipes

### ğŸ¯ Goal

**What must WORK after completing this WS:**
- OpenTelemetry tracing Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ² Spark Connect ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ
- 2 Ğ½Ğ¾Ğ²Ñ‹Ñ… Grafana Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´Ğ°: tracing timeline + profiling bottlenecks
- 5 Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¾Ğ² Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
- Event log profiling Ñ‡ĞµÑ€ĞµĞ· History Server Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ¸ Ğ·Ğ°Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½

**Acceptance Criteria:**
- [ ] AC1: spark-connect-configmap.yaml Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ OTel tracing config (conditional)
- [ ] AC2: values.yaml ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ `monitoring.openTelemetry` ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ (endpoint, protocol, sampling)
- [ ] AC3: Custom SparkListener JAR Ğ´Ğ»Ñ resource-wait tracking (app_start â†’ first_executor delta)
- [ ] AC4: Hadoop S3A Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Prometheus (bytes_written, op_duration, multipart)
- [ ] AC5: Grafana Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´ "Job Phase Timeline" (Gantt: resource wait / compute / I/O breakdown)
- [ ] AC6: Grafana Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´ "Spark Profiling" (GC time, ser/deser, shuffle skew, spill, idle)
- [ ] AC7: Airflow trace_id propagation Ñ‡ĞµÑ€ĞµĞ· gRPC metadata â†’ Spark spans
- [ ] AC8: Ğ ĞµÑ†ĞµĞ¿Ñ‚ `docs/recipes/troubleshoot/slow-spark-jobs.md` ÑĞ¾Ğ·Ğ´Ğ°Ğ½
- [ ] AC9: Ğ ĞµÑ†ĞµĞ¿Ñ‚ `docs/recipes/troubleshoot/data-skew-diagnosis.md` ÑĞ¾Ğ·Ğ´Ğ°Ğ½
- [ ] AC10: Ğ ĞµÑ†ĞµĞ¿Ñ‚ `docs/recipes/troubleshoot/oom-and-spill.md` ÑĞ¾Ğ·Ğ´Ğ°Ğ½
- [ ] AC11: Ğ ĞµÑ†ĞµĞ¿Ñ‚ `docs/recipes/troubleshoot/shuffle-bottleneck.md` ÑĞ¾Ğ·Ğ´Ğ°Ğ½
- [ ] AC12: Ğ ĞµÑ†ĞµĞ¿Ñ‚ `docs/recipes/troubleshoot/gc-pressure.md` ÑĞ¾Ğ·Ğ´Ğ°Ğ½
- [ ] AC13: Ğ ĞµÑ†ĞµĞ¿Ñ‚ `docs/recipes/troubleshoot/resource-wait-long.md` (K8s resource starvation)
- [ ] AC14: Ğ ĞµÑ†ĞµĞ¿Ñ‚ `docs/recipes/troubleshoot/s3-write-slow.md` (S3 I/O bottleneck)
- [ ] AC15: `docs/recipes/observability/spark-profiling-guide.md` ÑĞ¾Ğ·Ğ´Ğ°Ğ½ (Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ñ‹Ğ¹ guide)
- [ ] AC16: helm template Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ Ğ²ĞºĞ»ÑÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğ¼ OTel tracing
- [ ] AC17: ĞĞ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½ `docs/recipes/observability/observability-stack.md` (ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´Ñ‹)

**WS is NOT complete until Goal is achieved (all AC checked).**

### Reference Case: Airflow ETL 2h15m

```
Airflow Task (wall clock: 2h 15m)
â”‚
â”œâ”€ [Phase 0]  Airflow â†’ gRPC submit                ~seconds
â”œâ”€ [Phase 1]  Driver Ğ¶Ğ´Ñ‘Ñ‚ executors Ğ¾Ñ‚ K8s          ~long       â† INVISIBLE
â”œâ”€ [Phase 2]  Compute (stages, tasks, shuffle)       ~short
â””â”€ [Phase 3]  S3 write (multipart upload)            ~long      â† looks like "slow tasks"
```

**4-layer tracing model:**

| Layer | Source | What it captures |
|-------|--------|-----------------|
| Airflow | OTel spans (native 2.7+) | Task wall clock, DAG context, trace_id |
| Spark Connect | gRPC interceptor | Submit â†’ response, trace_id propagation |
| Spark Driver | Custom SparkListener | Resource wait (app_start â†’ first_executor), job/stage/task spans |
| S3/MinIO | Hadoop S3A metrics | Write throughput, multipart latency, op count |

**Key insight:** Resource wait (Phase 1) is completely invisible without custom
SparkListener that tracks `onApplicationStart` â†’ `onExecutorAdded` delta.
S3 write bottleneck (Phase 3) visible when write-stage task p95 >> compute-stage task p95.

**Custom SparkListener metrics:**

| Metric | Meaning |
|--------|---------|
| `spark_resource_wait_seconds` | Time from app start to first executor |
| `spark_phase_compute_seconds` | Time in compute stages |
| `spark_phase_io_write_seconds` | Time in write stages (output > 0 bytes) |
| `spark_phase_breakdown_ratio` | wait:compute:io ratio |

**Diagnosis table:**

| Symptom | Metric | Root Cause | Fix |
|---------|--------|------------|-----|
| resource_wait > 300s | spark_resource_wait_seconds | K8s scheduling delay | Priority class, pre-warm pool, batch allocation |
| write_stage p95 >> compute p95 | task duration histogram | S3 I/O bound | Increase multipart.size, magic committer, reduce partitions |
| compute p95 high variance | task duration histogram | Data skew | Repartition, salting, AQE |

### Grafana Dashboard: Job Phase Timeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark Job Phase Analysis                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Gantt Timeline]                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  Resource Wait  â”‚Compâ”‚   S3 Write      â”‚                â”‚
â”‚  â”‚  47% (1h 00m)   â”‚ 6% â”‚  47% (1h 00m)   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  â†‘ app_start  â†‘ first_exec  â†‘ write_stage  â†‘ job_end      â”‚
â”‚                                                            â”‚
â”‚  [Phase Breakdown]        [Key SLIs]                       â”‚
â”‚  Wait:    47%             Time to 1st executor: 3600s      â”‚
â”‚  Compute:  6%             Compute efficiency: 6%           â”‚
â”‚  I/O:     47%             S3 throughput: 2.8 MB/s          â”‚
â”‚                           Task p95 compute: 12s            â”‚
â”‚  [Executor Lifecycle]     Task p95 write: 890s             â”‚
â”‚  0â”€â”€â”€â”€1â”€â”€â”€â”€5â”€â”€5â”€â”€0                                         â”‚
â”‚  pend | scale | drain     [Airflow Context]                â”‚
â”‚                           DAG: etl_daily task: transform   â”‚
â”‚                           trace_id: abc123                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Grafana Dashboard: Spark Profiling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark Profiling                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  [GC Time %]             [Serialization Time]          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ â–“â–“â–“â–‘â–‘â–‘â–‘â–‘ â”‚ 15%        â”‚ â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ 3%              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                        â”‚
â”‚  [Shuffle Skew]          [Spill to Disk]               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ max/avg  â”‚ 4.1x       â”‚ 2.3 GB   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                        â”‚
â”‚  [Executor Utilization]  [Peak Memory per Executor]    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ 38%    â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘ â”‚ 78%          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                        â”‚
â”‚  [Task Duration Heatmap]                               â”‚
â”‚  Executor0 â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘                                 â”‚
â”‚  Executor1 â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“                          â”‚
â”‚  Executor2 â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Recipes Overview

| Recipe | Problem | Key Metrics | Action |
|--------|---------|-------------|--------|
| slow-spark-jobs | Job takes too long | task duration p95, stage time | identify bottleneck stage |
| data-skew-diagnosis | Uneven data distribution | max/median task size ratio | repartition, salting |
| oom-and-spill | Executor OOM, spill | memory used, spill bytes | tune memory, partitions |
| shuffle-bottleneck | Slow shuffle | shuffle read/write time, bytes | optimize partitioning |
| gc-pressure | High GC pause | GC time %, full GC count | tune JVM, reduce objects |

### OpenTelemetry Configuration

```yaml
# values.yaml addition
monitoring:
  openTelemetry:
    enabled: false
    endpoint: "http://otel-collector:4317"
    protocol: "grpc"           # grpc or http
    samplingRatio: "1.0"       # 0.0-1.0
    serviceName: "spark-connect-35"
    exporterType: "otlp"       # otlp or jaeger
```

Spark 3.5 config properties:
```
spark.plugins=org.apache.spark.sql.connect.SparkConnectPlugin
spark.opentelemetry.enabled=true
spark.otel.exporter.otlp.endpoint=...
spark.otel.resource.attributes=service.name=spark-connect-35
```

### Files Changed

**Chart templates:**
- `charts/spark-3.5/templates/spark-connect-configmap.yaml` (ADD OTel + S3A metrics config)
- `charts/spark-3.5/values.yaml` (ADD monitoring.openTelemetry, monitoring.s3aMetrics)
- `charts/spark-3.5/templates/monitoring/grafana-dashboard-job-phase-timeline.yaml` (NEW)
- `charts/spark-3.5/templates/monitoring/grafana-dashboard-profiling.yaml` (NEW)

**Custom SparkListener (resource wait tracker):**
- `docker/spark-3.5/listeners/resource-wait-tracker.py` (NEW) â€” PySpark listener
- OR `spark-listeners/src/ResourceWaitTracker.java` (NEW) â€” JAR plugin

**Troubleshooting recipes (7):**
- `docs/recipes/troubleshoot/slow-spark-jobs.md` (NEW)
- `docs/recipes/troubleshoot/data-skew-diagnosis.md` (NEW)
- `docs/recipes/troubleshoot/oom-and-spill.md` (NEW)
- `docs/recipes/troubleshoot/shuffle-bottleneck.md` (NEW)
- `docs/recipes/troubleshoot/gc-pressure.md` (NEW)
- `docs/recipes/troubleshoot/resource-wait-long.md` (NEW)
- `docs/recipes/troubleshoot/s3-write-slow.md` (NEW)

**Observability guides:**
- `docs/recipes/observability/spark-profiling-guide.md` (NEW)
- `docs/recipes/observability/observability-stack.md` (UPDATE)
