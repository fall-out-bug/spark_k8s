# WS-008-06: Container Security Tests

## Goal
Создать 8 тестов для проверки Container Security для всех подов Spark.

### Acceptance Criteria
1. 8 container security тестов созданы в `tests/security/phase08/test_08_06_container_security.py`
2. Non-root user проверен — runAsUser=185
3. Non-root group проверен — runAsGroup=185
4. Read-only root проверен — readOnlyRootFilesystem option
5. No privilege escalation проверен — allowPrivilegeEscalation=false
6. Drop capabilities проверен — drop ALL
7. No privileged проверен — privileged=false
8. Seccomp profile проверен — runtime/default
9. Resource limits проверен — memory/CPU enforced

## Context

**Существующая инфраструктура:**
- `charts/spark-3.5/templates/spark-connect.yaml` — SecurityContext в pod spec
- `charts/spark-3.5/values.yaml` — securityContext configuration
- Существующие тесты в `tests/security/test_security.py` (SecurityContext: PSS restricted, non-root user, readonly root)

**Что нужно добавить:**
- Структурированные container security тесты
- Проверка всех securityContext полей
- Проверка container-level и pod-level securityContext
- Отдельный файл `tests/security/phase08/test_08_06_container_security.py`

## Dependency
Phase 0 (Helm Charts), Phase 1 (Critical Security)

## Input Files
- `charts/spark-3.5/templates/spark-connect.yaml`
- `charts/spark-3.5/templates/spark-standalone-worker.yaml`
- `charts/spark-3.5/templates/jupyter.yaml`
- `charts/spark-3.5/templates/hive-metastore.yaml`
- `charts/spark-3.5/values.yaml`
- `tests/security/test_security.py` (reference)

## Steps

### 1. Create test_08_06_container_security.py
- Создать `tests/security/phase08/test_08_06_container_security.py`
- Implement 8 test scenarios

### 2. Implement test scenarios

**Scenario 1: Non-root user — runAsUser=185**
- Render template для spark-connect
- Проверить pod-level securityContext.runAsUser
- Проверить container-level securityContext.runAsUser
- Проверить что UID не 0 (root)

**Scenario 2: Non-root group — runAsGroup=185**
- Render template для spark-connect
- Проверить pod-level securityContext.runAsGroup
- Проверить container-level securityContext.runAsGroup
- Проверить fsGroup если используется

**Scenario 3: Read-only root — readOnlyRootFilesystem option**
- Render template для spark-connect
- Проверить securityContext.readOnlyRootFilesystem
- Проверить что ephemeral volumes используются для /tmp, /logs

**Scenario 4: No privilege escalation — allowPrivilegeEscalation=false**
- Render template для spark-connect
- Проверить securityContext.allowPrivilegeEscalation=false
- Проверить для всех контейнеров

**Scenario 5: Drop capabilities — drop ALL**
- Render template для spark-connect
- Проверить securityContext.capabilities.drop
- Проверить что ALL в списке drop
- Проверить отсутствие NET_RAW, SYS_ADMIN

**Scenario 6: No privileged — privileged=false**
- Render template для spark-connect
- Проверить securityContext.privileged=false
- Проверить для всех контейнеров
- Проверить отсутствие privileged:true

**Scenario 7: Seccomp profile — runtime/default**
- Render template для spark-connect
- Проверить securityContext.seccompProfile.type
- Проверить что type=RuntimeDefault или runtime/default
- Проверить annotations для seccomp

**Scenario 8: Resource limits — memory/CPU enforced**
- Render template для spark-connect
- Проверить resources.requests.memory
- Проверить resources.requests.cpu
- Проверить resources.limits.memory
- Проверить resources.limits.cpu

### 3. Run tests
- `pytest tests/security/phase08/test_08_06_container_security.py -v`
- Убедиться что все 8 тестов проходят

## Code

### tests/security/phase08/test_08_06_container_security.py

```python
"""Container Security Tests for Phase 8

Tests for container security settings across all Spark pods.
"""

import pytest
import subprocess
import tempfile
import yaml
from pathlib import Path
from typing import Dict, List, Any


class TestContainerSecurity:
    """Container Security tests"""

    @pytest.fixture
    def rendered_template(self, spark_35_chart):
        """Render helm template with security enabled"""
        with tempfile.NamedTemporaryFile(mode="w+", suffix=".yaml") as f:
            result = subprocess.run(
                ["helm", "template", "spark-test", str(spark_35_chart),
                 "--set", "sparkConnect.enabled=true",
                 "--set", "podSecurityStandards=true"],
                capture_output=True, text=True
            )
            return result.stdout

    @pytest.fixture
    def pods(self, rendered_template):
        """Parse Pod/Deployment/StatefulSet resources"""
        documents = list(yaml.safe_load_all(rendered_template))
        pod_specs = []

        for doc in documents:
            if not doc:
                continue
            kind = doc.get('kind')
            if kind in ['Pod', 'Deployment', 'StatefulSet', 'DaemonSet', 'Job']:
                if kind == 'Pod':
                    pod_spec = doc.get('spec', {})
                else:
                    pod_spec = doc.get('spec', {}).get('template', {}).get('spec', {})

                pod_specs.append({
                    'kind': kind,
                    'metadata': doc.get('metadata', {}),
                    'spec': pod_spec
                })

        return pod_specs

    def test_01_non_root_user_runasuser_185(self, pods):
        """Non-root user — runAsUser=185"""
        found_spark_pod = False
        found_non_root = False

        for pod in pods:
            metadata = pod.get('metadata', {})
            name = metadata.get('name', '').lower()

            # Focus on Spark pods
            if 'spark' not in name:
                continue

            found_spark_pod = True
            spec = pod.get('spec', {})

            # Check pod-level securityContext
            pod_security = spec.get('securityContext', {})
            run_as_user = pod_security.get('runAsUser')

            # Check container-level securityContext
            containers = spec.get('containers', [])
            for container in containers:
                container_security = container.get('securityContext', {})
                container_run_as_user = container_security.get('runAsUser')

                # Either pod-level or container-level should be set
                effective_user = container_run_as_user or run_as_user

                if effective_user:
                    assert effective_user != 0, \
                        f"Pod {name} runs as root (UID 0)"
                    found_non_root = True
                    # Check for Spark user (typically 185 or 1000)
                    assert effective_user in [185, 1000, 1000185000], \
                        f"Pod {name} has unexpected runAsUser: {effective_user}"

        assert found_spark_pod, "No Spark pods found"
        assert found_non_root, "No non-root user configuration found"

    def test_02_non_root_group_runasgroup_185(self, pods):
        """Non-root group — runAsGroup=185"""
        found_spark_pod = False
        found_group = False

        for pod in pods:
            metadata = pod.get('metadata', {})
            name = metadata.get('name', '').lower()

            if 'spark' not in name:
                continue

            found_spark_pod = True
            spec = pod.get('spec', {})

            # Check pod-level securityContext
            pod_security = spec.get('securityContext', {})
            run_as_group = pod_security.get('runAsGroup')
            fs_group = pod_security.get('fsGroup')

            # Check container-level securityContext
            containers = spec.get('containers', [])
            for container in containers:
                container_security = container.get('securityContext', {})
                container_run_as_group = container_security.get('runAsGroup')

                # Either pod-level or container-level should be set
                effective_group = container_run_as_group or run_as_group

                if effective_group:
                    assert effective_group != 0, \
                        f"Pod {name} runs as root group (GID 0)"
                    found_group = True

                # Check fsGroup as well
                if fs_group:
                    assert fs_group != 0, \
                        f"Pod {name} has fsGroup set to root (GID 0)"
                    found_group = True

        assert found_spark_pod, "No Spark pods found"
        # Note: runAsGroup might not always be set, fsGroup is often used instead

    def test_03_readonly_root_filesystem(self, pods):
        """Read-only root — readOnlyRootFilesystem option"""
        found_spark_pod = False
        found_readonly = False

        for pod in pods:
            metadata = pod.get('metadata', {})
            name = metadata.get('name', '').lower()

            if 'spark' not in name:
                continue

            found_spark_pod = True
            spec = pod.get('spec', {})

            # Check pod-level securityContext
            pod_security = spec.get('securityContext', {})
            readonly_root = pod_security.get('readOnlyRootFilesystem')

            # Check container-level securityContext
            containers = spec.get('containers', [])
            for container in containers:
                container_security = container.get('securityContext', {})
                container_readonly = container_security.get('readOnlyRootFilesystem')

                # Either pod-level or container-level
                effective_readonly = container_readonly if container_readonly is not None else readonly_root

                if effective_readonly:
                    found_readonly = True

                # If readonly, check for emptyDir volumes for writable paths
                if effective_readonly:
                    volumes = spec.get('volumes', [])
                    has_emptydir = any(v.get('emptyDir') for v in volumes)
                    # This is a soft check - readonly root should have volumes for /tmp, etc.

        assert found_spark_pod, "No Spark pods found"
        # Note: readOnlyRootFilesystem might not always be enabled

    def test_04_no_privilege_escalation(self, pods):
        """No privilege escalation — allowPrivilegeEscalation=false"""
        found_spark_pod = False
        all_containers_secure = True

        for pod in pods:
            metadata = pod.get('metadata', {})
            name = metadata.get('name', '').lower()

            if 'spark' not in name:
                continue

            found_spark_pod = True
            spec = pod.get('spec', {})

            # Check all containers
            containers = spec.get('containers', [])
            init_containers = spec.get('initContainers', [])
            all_containers = containers + init_containers

            for container in all_containers:
                container_security = container.get('securityContext', {})
                allow_escalation = container_security.get('allowPrivilegeEscalation')

                # Should be explicitly set to false
                if allow_escalation is not None:
                    assert allow_escalation is False, \
                        f"Pod {name} container {container.get('name')} has allowPrivilegeEscalation=true"

        assert found_spark_pod, "No Spark pods found"

    def test_05_drop_capabilities_all(self, pods):
        """Drop capabilities — drop ALL"""
        found_spark_pod = False
        found_drop_all = False

        dangerous_capabilities = ['NET_RAW', 'SYS_ADMIN', 'SYS_MODULE', 'SYS_PTRACE']

        for pod in pods:
            metadata = pod.get('metadata', {})
            name = metadata.get('name', '').lower()

            if 'spark' not in name:
                continue

            found_spark_pod = True
            spec = pod.get('spec', {})

            # Check all containers
            containers = spec.get('containers', [])
            init_containers = spec.get('initContainers', [])
            all_containers = containers + init_containers

            for container in all_containers:
                container_security = container.get('securityContext', {})
                capabilities = container_security.get('capabilities', {})
                drop = capabilities.get('drop', [])
                add = capabilities.get('add', [])

                # Check for ALL in drop
                if 'ALL' in drop or 'all' in drop:
                    found_drop_all = True

                # Check no dangerous capabilities are added
                for cap in dangerous_capabilities:
                    assert cap not in add, \
                        f"Pod {name} container {container.get('name')} has dangerous capability: {cap}"

        assert found_spark_pod, "No Spark pods found"

    def test_06_no_privileged_containers(self, pods):
        """No privileged — privileged=false"""
        found_spark_pod = False
        all_containers_unprivileged = True

        for pod in pods:
            metadata = pod.get('metadata', {})
            name = metadata.get('name', '').lower()

            if 'spark' not in name:
                continue

            found_spark_pod = True
            spec = pod.get('spec', {})

            # Check all containers
            containers = spec.get('containers', [])
            init_containers = spec.get('initContainers', [])
            all_containers = containers + init_containers

            for container in all_containers:
                container_security = container.get('securityContext', {})
                privileged = container_security.get('privileged')

                # Must be explicitly false or unset (defaults to false)
                assert privileged is not True, \
                    f"Pod {name} container {container.get('name')} is privileged"

        assert found_spark_pod, "No Spark pods found"

    def test_07_seccomp_profile_runtime_default(self, pods):
        """Seccomp profile — runtime/default"""
        found_spark_pod = False
        found_seccomp = False

        for pod in pods:
            metadata = pod.get('metadata', {})
            name = metadata.get('name', '').lower()

            if 'spark' not in name:
                continue

            found_spark_pod = True
            spec = pod.get('spec', {})

            # Check annotations for seccomp
            annotations = metadata.get('annotations', {})
            seccomp_annotation = annotations.get('seccomp.security.alpha.kubernetes.io/pod')

            # Check pod-level securityContext
            pod_security = spec.get('securityContext', {})
            seccomp_profile = pod_security.get('seccompProfile', {})

            # Check container-level securityContext
            containers = spec.get('containers', [])
            for container in containers:
                container_security = container.get('securityContext', {})
                container_seccomp = container_security.get('seccompProfile', {})

                # Check for seccomp configuration
                if seccomp_profile or container_seccomp or seccomp_annotation:
                    found_seccomp = True

                    # Verify it's using runtime/default
                    profile_type = seccomp_profile.get('type') or container_seccomp.get('type')
                    if profile_type:
                        assert profile_type in ['RuntimeDefault', 'runtime/default', 'Runtime/default'], \
                            f"Pod {name} has unexpected seccomp profile type: {profile_type}"

        assert found_spark_pod, "No Spark pods found"

    def test_08_resource_limits_memory_cpu(self, pods):
        """Resource limits — memory/CPU enforced"""
        found_spark_pod = False
        found_limits = False

        for pod in pods:
            metadata = pod.get('metadata', {})
            name = metadata.get('name', '').lower()

            if 'spark' not in name:
                continue

            found_spark_pod = True
            spec = pod.get('spec', {})

            # Check all containers
            containers = spec.get('containers', [])

            for container in containers:
                resources = container.get('resources', {})
                requests = resources.get('requests', {})
                limits = resources.get('limits', {})

                # Check for resource configuration
                has_memory = 'memory' in requests or 'memory' in limits
                has_cpu = 'cpu' in requests or 'cpu' in limits

                if has_memory or has_cpu:
                    found_limits = True

                # If limits are set, they should be reasonable
                if limits:
                    memory_limit = limits.get('memory')
                    if memory_limit:
                        # Check format (e.g., 2Gi, 512Mi)
                        assert 'Gi' in memory_limit or 'Mi' in memory_limit or 'G' in memory_limit or 'M' in memory_limit, \
                            f"Pod {name} container {container.get('name')} has invalid memory limit format: {memory_limit}"

        assert found_spark_pod, "No Spark pods found"
        # Note: Resources might not be set in all cases (depends on preset)
```

## Scope Estimate
- Files: 1 (test_08_06_container_security.py)
- LOC: ~700 (tests)
- Scenarios: 8
- Size: MEDIUM
