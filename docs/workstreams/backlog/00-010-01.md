---
ws_id: 00-010-01
feature: F10
status: backlog
size: MEDIUM
project_id: 00
github_issue: null
assignee: null
depends_on:
  - 00-009-01  # JDK 17 base layer
---

## WS-00-010-01: Spark core layers (4) + tests

### ðŸŽ¯ Goal

**What must WORK after completing this WS:**
- 4 Spark core intermediate images (3.5.7, 3.5.8, 4.1.0, 4.1.1)
- Each extends from JDK 17 base layer
- Spark binary downloaded and cached
- Unit tests validate Spark installation

**Acceptance Criteria:**
- [ ] AC1: 4 Dockerfiles created (spark-3.5.7-core, spark-3.5.8-core, spark-4.1.0-core, spark-4.1.1-core)
- [ ] AC2: Each extends spark-k8s-jdk-17:latest
- [ ] AC3: Spark binaries downloaded from Apache mirrors
- [ ] AC4: tests/docker-intermediate/test-spark-core.sh passes for all versions
- [ ] AC5: spark-shell and spark-submit available
- [ ] AC6: Layer caching works (rebuild is fast)

**âš ï¸ WS is NOT complete until Goal is achieved (all AC âœ…).**

---

### Context

Spark runtime images need Spark binaries. This WS creates intermediate layers with Spark installed, which can be reused by final runtime images.

### Dependencies

- WS-009-01: JDK 17 base layer (extends from this)
- F09: Phase 3 (base layers complete)

### Input Files

- docker/docker-base/jdk-17/Dockerfile

### Steps

1. **Create Spark core directory structure**

2. **Create Dockerfiles for each version**

3. **Download and cache Spark binaries**

4. **Create unit test script**

5. **Add to Makefile**

### Code

```dockerfile
# docker/docker-intermediate/spark-3.5.7-core/Dockerfile
ARG BASE_IMAGE=spark-k8s-jdk-17:latest
FROM ${BASE_IMAGE}

LABEL maintainer="spark-k8s"
LABEL description="Spark K8s - Spark 3.5.7 Core Intermediate Layer"
LABEL version="1.0.0"
LABEL spark.version="3.5.7"

# Spark download URLs
ARG SPARK_VERSION=3.5.7
ARG SPARK_HADOOP_VERSION=3
ARG SPARK_MIRROR=https://downloads.apache.org/spark

# Download and install Spark
RUN wget -q ${SPARK_MIRROR}/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}.tgz && \
    tar xzf spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION} /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${SPARK_HADOOP_VERSION}.tgz

# Set Spark environment
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:${SPARK_HOME}/bin:${SPARK_HOME}/sbin
ENV PYTHONPATH=${SPARK_HOME}/python:${PYTHONPATH}

# Verify Spark installation
RUN ${SPARK_HOME}/bin/spark-shell --version && \
    ${SPARK_HOME}/bin/spark-submit --version

# Working directory
WORKDIR /opt/spark

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD ${SPARK_HOME}/bin/spark-shell --version || exit 1

# Default shell
SHELL ["/bin/bash", "-c"]
```

```bash
#!/bin/bash
# docker/docker-intermediate/test-spark-core.sh

set -e

SPARK_VERSION="${1:-3.5.7}"
IMAGE="spark-k8s-spark-${SPARK_VERSION}-core:latest"

echo "Testing Spark ${SPARK_VERSION} core layer: $IMAGE"

# Test 1: Image exists
echo "Test 1: Check image exists"
docker images "$IMAGE" --format "{{.Repository}}:{{.Tag}}" | grep -q "$IMAGE" || {
    echo "FAIL: Image not found"
    exit 1
}
echo "PASS: Image exists"

# Test 2: Spark installation
echo "Test 2: Verify Spark installation"
docker run --rm "$IMAGE" bash -c '${SPARK_HOME}/bin/spark-shell --version' | grep -q "version ${SPARK_VERSION}" || {
    echo "FAIL: Spark ${SPARK_VERSION} not found"
    exit 1
}
echo "PASS: Spark ${SPARK_VERSION} installed"

# Test 3: Spark commands available
echo "Test 3: Check Spark commands"
docker run --rm "$IMAGE" which spark-shell >/dev/null || {
    echo "FAIL: spark-shell not found"
    exit 1
}
docker run --rm "$IMAGE" which spark-submit >/dev/null || {
    echo "FAIL: spark-submit not found"
    exit 1
}
docker run --rm "$IMAGE" which pyspark >/dev/null || {
    echo "FAIL: pyspark not found"
    exit 1
}
echo "PASS: Spark commands available"

# Test 4: Python integration
echo "Test 4: Check Python integration"
docker run --rm "$IMAGE" python -c "import sys; sys.path.append('/opt/spark/python'); import pyspark; print(pyspark.__version__)" | grep -q "${SPARK_VERSION}" || {
    echo "FAIL: PySpark not accessible"
    exit 1
}
echo "PASS: PySpark accessible"

# Test 5: Simple Spark job
echo "Test 5: Run simple Spark job"
docker run --rm "$IMAGE" spark-submit --version | grep -q "${SPARK_VERSION}" || {
    echo "FAIL: spark-submit not working"
    exit 1
}
echo "PASS: spark-submit works"

echo ""
echo "All tests passed! âœ…"
```

### Expected Outcome

- 4 Dockerfiles in `docker/docker-intermediate/spark-{version}-core/`
- Test script in `docker/docker-intermediate/`
- Images: `spark-k8s-spark-3.5.7-core:latest`, etc.
- Layer caching enabled for fast rebuilds

### Scope Estimate

- Files: 5 (4 Dockerfiles + 1 test script)
- Lines: ~800 (MEDIUM)
- Tokens: ~6000

### Completion Criteria

```bash
# Build all Spark core layers
make build-spark-core-all

# Test specific version
bash docker/docker-intermediate/test-spark-core.sh 3.5.7

# Test all versions
for v in 3.5.7 3.5.8 4.1.0 4.1.1; do
    bash docker/docker-intermediate/test-spark-core.sh $v
done
```

### Constraints

- DO NOT exceed 1GB per image (Spark binary ~350MB + base)
- DO use official Apache mirrors
- DO require WS-009-01 completion
- DO NOT include custom configurations (add in derived images)

---

## Execution Report

**Executed by:** ______
**Date:** ______
**Duration:** ______ minutes

### Goal Status
- [ ] AC1-AC6 â€” âœ…

**Goal Achieved:** ______

### Files Changed
| File | Action | LOC |
|------|--------|-----|
|      |        |     |

### Statistics
- **Files Changed:** ______
- **Lines Added:** ______
- **Lines Removed:** ______
- **Test Coverage:** ______ %
- **Tests Passed:** ______
- **Tests Failed:** ______

### Deviations from Plan
- ______

### Commit
______
