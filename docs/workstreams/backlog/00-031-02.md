# WS-031-02: Feature Engineering - Spark Pipeline

## Summary
Spark pipeline for feature generation from raw TLC data.

## Scope
- Read raw parquet from MinIO
- Generate temporal, geospatial, and historical features
- Write to Iceberg table `nyc_taxi.features`

## Acceptance Criteria
- [ ] Iceberg table `nyc_taxi.features` created
- [ ] All 3 feature categories generated
- [ ] Feature quality validated (no nulls in required fields)
- [ ] Query performance acceptable (< 10s for aggregation)

## Feature Categories

### 1. Temporal Features
```python
temporal_features = [
    "hour_of_day",        # 0-23
    "day_of_week",        # 0-6
    "is_weekend",         # bool
    "is_holiday",         # bool (US holidays)
    "month",              # 1-12
    "quarter",            # 1-4
    "is_rush_hour",       # 7-9am, 5-7pm
    "week_of_year",       # 1-52
]
```

### 2. Geospatial Features
```python
geo_features = [
    "pickup_zone_id",     # TLC zone
    "dropoff_zone_id",    # TLC zone
    "pickup_borough",     # Manhattan, Brooklyn, Queens, Bronx, Staten Island
    "trip_distance",      # miles
    "avg_speed",          # mph
    "trip_duration",      # minutes
    "is_airport",         # JFK/LGA/EWR pickup
]
```

### 3. Historical Aggregates (7-day target)
```python
historical_features = [
    "avg_trips_zone_hour_l30d",      # avg trips per zone per hour (last 30d)
    "avg_revenue_zone_day_l30d",     # avg revenue per zone per day (last 30d)
    "trips_same_day_last_week",       # trips same day last week
    "trips_same_day_last_year",       # YoY comparison
    "revenue_same_day_last_week",
    "revenue_ma_7d",                  # 7-day moving average
    "trips_ma_7d",                    # 7-day moving average
]
```

## Target Variables (Multi-target)
```python
targets = [
    "total_amount_7d",    # Sum of revenue for next 7 days by zone
    "trip_count_7d",      # Count of trips for next 7 days by zone
]
```

## Technical Details

### Spark Session Configuration
```python
spark = SparkSession.builder \
    .appName("nyc-taxi-feature-engineering") \
    .master("spark://scenario2-spark-35-standalone-master:7077") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio.spark-infra.svc.cluster.local:9000") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.spark_catalog.type", "hadoop") \
    .config("spark.sql.catalog.spark_catalog.warehouse", "s3a://nyc-taxi/warehouse") \
    .getOrCreate()
```

### Pipeline Stages
1. Read raw parquet
2. Parse timestamps, extract temporal features
3. Join with zone lookup table for borough
4. Calculate historical aggregates (window functions)
5. Create 7-day target variables (look-ahead)
6. Write to Iceberg

### Script Location
```
dags/spark_jobs/taxi_feature_engineering.py
```

## Dependencies
- WS-031-01 (data in MinIO)
- Iceberg configured in Spark
- Zone lookup table

## Estimated Complexity
High - Complex window functions + Iceberg integration

## Files to Create/Modify
- `dags/spark_jobs/taxi_feature_engineering.py` (new)
- `dags/spark_jobs/zone_lookup.csv` (new)
