---
ws_id: 00-017-01
feature: F17
status: backlog
size: MEDIUM
project_id: 00
github_issue: null
assignee: null
depends_on:
  - 00-006-01  # Helm charts
  - 00-011-01  # Spark 3.5 images
  - 00-011-02  # Spark 4.1 images
---

## WS-00-017-01: Spark Connect Go client library

### üéØ Goal

**What must WORK after completing this WS:**
- Spark Connect Go client library
- gRPC integration with Spark Connect server
- SQL execution
- DataFrame operations
- Session management

**Acceptance Criteria:**
- [ ] AC1: Go client library —Å–æ–∑–¥–∞–Ω
- [ ] AC2: gRPC —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Spark Connect —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] AC3: SQL query execution —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] AC4: DataFrame collect —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] AC5: Session management —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] AC6: Error handling —Ä–∞–±–æ—Ç–∞–µ—Ç

**‚ö†Ô∏è WS is NOT complete until Goal is achieved (all AC ‚úÖ).**

---

### Dependencies

Phase 0 (F06), Phase 5 (F11)

### Code

```go
// tests/go/client/connect.go
// NOTE: This code template aligns with Spark Connect gRPC API.
// Reference: https://spark.apache.org/docs/latest/api/connect/
// When official Apache Spark Connect Go client is available, use it instead.
//
// Spark Connect Protocol:
// - Use ExecutePlanRequest with Plan protobuf messages
// - Response contains OutputBatches with Arrow data
// - Session management via CreateSessionRequest/CloseSessionRequest

package spark

import (
	"context"
	"crypto/tls"
	"fmt"
	"time"

	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials"
)

// NOTE: Import proto definitions from spark/connect/proto when available
// For now, using placeholder structures matching Spark Connect API

// Client represents a Spark Connect client
type Client struct {
	conn      *grpc.ClientConn
	clientID  string
	sessionID string
	timeout   time.Duration
}

// ClientOption configures a Spark Connect client
type ClientOption func(*Client)

// WithTimeout sets request timeout
func WithTimeout(timeout time.Duration) ClientOption {
	return func(c *Client) {
		c.timeout = timeout
	}
}

// WithUserID sets the user ID for the session
func WithUserID(userID string) ClientOption {
	return func(c *Client) {
		c.clientID = userID
	}
}

// NewClient creates a new Spark Connect client
func NewClient(ctx context.Context, endpoint string, opts ...ClientOption) (*Client, error) {
	client := &Client{
		clientID: "go-client",
		timeout:  30 * time.Second,
	}

	// Apply options
	for _, opt := range opts {
		opt(client)
	}

	// Create gRPC connection
	creds := credentials.NewTLS(&tls.Config{
		InsecureSkipVerify: true, // For development
	})

	conn, err := grpc.DialContext(ctx, endpoint,
		grpc.WithTransportCredentials(creds),
		grpc.WithBlock(),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to connect to Spark Connect: %w", err)
	}

	client.conn = conn

	return client, nil
}

// Close closes client connection
func (c *Client) Close() error {
	if c.conn != nil {
		return c.conn.Close()
	}
	return nil
}

// Session represents a Spark session
type Session struct {
	client    *Client
	sessionID string
	config    map[string]string
}

// CreateSession creates a new Spark session
func (c *Client) CreateSession(ctx context.Context) (*Session, error) {
	// TODO: Send CreateSessionRequest via gRPC
	// Request contains UserContext with userId, userName, appName
	sessionID := fmt.Sprintf("go-session-%d", time.Now().UnixNano())

	return &Session{
		client:    c,
		sessionID: sessionID,
		config:    make(map[string]string),
	}, nil
}

// ID returns the session ID
func (s *Session) ID() string {
	return s.sessionID
}

// Close closes the session
func (s *Session) Close(ctx context.Context) error {
	// TODO: Send CloseSessionRequest via gRPC
	return nil
}

// DataFrame represents a Spark DataFrame
type DataFrame struct {
	session  *Session
	plan     string // JSON representation of Plan protobuf
}

// SQL executes a SQL query and returns a DataFrame
func (s *Session) SQL(ctx context.Context, query string) (*DataFrame, error) {
	// TODO: Wrap query in Plan with SQL relation
	plan := fmt.Sprintf(`{"sql": {"query": "%s"}}`, query)

	return &DataFrame{
		session: s,
		plan:     plan,
	}, nil
}

// Collect collects all rows from the DataFrame
func (df *DataFrame) Collect(ctx context.Context) ([]Row, error) {
	// TODO: Send ExecutePlanRequest and parse OutputBatches with Arrow data
	// Response contains batches with row data in Arrow format
	return make([]Row, 0), nil
}

// Show prints the first 20 rows
func (df *DataFrame) Show(ctx context.Context) error {
	// TODO: Collect and display rows
	fmt.Printf("DataFrame: %s\n", df.plan)
	return nil
}

// Count returns the number of rows
func (df *DataFrame) Count(ctx context.Context) (int64, error) {
	// TODO: Execute plan and parse row count
	return 0, nil
}

// Row represents a single row from Spark Connect response
type Row struct {
	values map[string]interface{} // Arrow-derived column values
}

// String returns a string representation of the row
func (r *Row) String() string {
	return fmt.Sprint(r.values)
}

// GetInt gets an int value at column name
func (r *Row) GetInt(col string) (int32, error) {
	val, ok := r.values[col]
	if !ok {
		return 0, fmt.Errorf("column %s not found", col)
	}
	switch v := val.(type) {
	case int32:
		return v, nil
	case int:
		return int32(v), nil
	case float64:
		return int32(v), nil
	default:
		return 0, fmt.Errorf("column %s is not an int", col)
	}
}

// GetString gets a string value at column name
func (r *Row) GetString(col string) (string, error) {
	val, ok := r.values[col]
	if !ok {
		return "", fmt.Errorf("column %s not found", col)
	}
	if s, ok := val.(string); ok {
		return s, nil
	}
	return fmt.Sprintf("%v", val), nil
}

// GetFloat gets a float value at column name
func (r *Row) GetFloat(col string) (float64, error) {
	val, ok := r.values[col]
	if !ok {
		return 0.0, fmt.Errorf("column %s not found", col)
	}
	switch v := val.(type) {
	case float64:
		return v, nil
	case int:
		return float64(v), nil
	case int32:
		return float64(v), nil
	default:
		return 0.0, fmt.Errorf("column %s is not a float", col)
	}
}
```

### Scope Estimate

- Files: 8
- Lines: ~800 (MEDIUM)
- Tokens: ~6000

### Constraints

- DO use official Spark Connect gRPC protocol
- DO support Spark 3.5+ and 4.1+
- DO use TLS for production connections
- DO handle connection errors gracefully
- DO NOT support Spark < 3.5 (no Connect)

---

## Execution Report

**Executed by:** ______
**Date:** ______
**Duration:** ______ minutes

### Goal Status
- [ ] AC1-AC6 ‚Äî ‚úÖ

**Goal Achieved:** ______
