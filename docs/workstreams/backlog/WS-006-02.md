# WS-006-02: GPU E2E Tests

## Goal
Создать 16 GPU E2E тестов для проверки RAPIDS GPU acceleration на NYC Taxi dataset (11GB).

### Acceptance Criteria
1. 16 GPU E2E тестов созданы в `tests/e2e/phase06/test_06_02_gpu_e2e.py`
2. NYC Taxi dataset используется для GPU тестов
3. RAPIDS acceleration проверен (cuDF, cuML)
4. GPU queries (GPU-accelerated) vs CPU queries сравнение
5. Метрики GPU: GPU memory, GPU utilization, speedup factor
6. Тесты работают с spark-connect + GPU enabled
7. Валидация результатов GPU vs CPU (checksum)
8. Timeout mechanisms для GPU операций

## Context

**Существующая инфраструктура:**
- `tests/e2e/phase06/test_06_01_core_e2e.py` — core E2E тесты
- `charts/spark-3.5/values.yaml` — GPU configuration
- RAPIDS Docker images из Phase 5

**Что нужно добавить:**
- GPU-specific E2E тесты
- RAPIDS integration tests (cuDF for DataFrame, cuML for ML)
- GPU vs CPU comparison
- GPU metrics collection
- Отдельный файл `tests/e2e/phase06/test_06_02_gpu_e2e.py`

## Dependency
Phase 0 (Helm Charts), Phase 5 (GPU Final Images)

## Input Files
- `tests/e2e/phase06/conftest.py` (shared fixtures)
- `tests/e2e/phase06/test_06_01_core_e2e.py` (reference)
- `charts/spark-3.5/values.yaml`
- `charts/spark-4.1/values.yaml`

## Steps

### 1. Create test_06_02_gpu_e2e.py
- Создать `tests/e2e/phase06/test_06_02_gpu_e2e.py`
- Implement 16 test scenarios

### 2. Implement test scenarios

**GPU Scenarios (Spark versions × GPU variants):**

**Spark 3.5.7 + RAPIDS:**
1. spark-connect + GPU baseline (Q1-Q4)
2. spark-connect + GPU optimized (Q1-Q4)

**Spark 3.5.8 + RAPIDS:**
3. spark-connect + GPU baseline (Q1-Q4)
4. spark-connect + GPU optimized (Q1-Q4)

**Spark 4.1.0 + RAPIDS:**
5. spark-connect + GPU baseline (Q1-Q4)
6. spark-connect + GPU optimized (Q1-Q4)

**Spark 4.1.1 + RAPIDS:**
7. spark-connect + GPU baseline (Q1-Q4)
8. spark-connect + GPU optimized (Q1-Q4)

**GPU Queries:**
- Q1: cuDF accelerated COUNT(*)
- Q2: cuDF accelerated GROUP BY
- Q3: cuDF accelerated JOIN
- Q4: cuML accelerated ML operation (linear regression)

**Total: 8 scenarios × 2 variants = 16 tests**

### 3. GPU Metrics Collection
- gpu_memory_used: peak GPU memory
- gpu_utilization: GPU utilization percentage
- speedup_factor: GPU time vs CPU time
- gpu_spark_time: GPU-accelerated execution time

### 4. Run tests
- `pytest tests/e2e/phase06/test_06_02_gpu_e2e.py -v`
- Убедиться что все GPU тесты проходят

## Code

### tests/e2e/phase06/test_06_02_gpu_e2e.py

```python
"""GPU E2E Tests for Phase 6

Tests for RAPIDS GPU acceleration with NYC Taxi dataset.
"""

import pytest
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from typing import Dict, Any

SPARK_VERSIONS = ["3.5.7", "3.5.8", "4.1.0", "4.1.1"]
GPU_VARIANTS = ["baseline", "optimized"]

class TestGPUE2E:
    """GPU E2E tests for RAPIDS acceleration"""

    @pytest.mark.timeout(900)
    @pytest.mark.gpu
    def test_01_spark_357_gpu_baseline_q1_cudf_count(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 3.5.7 GPU baseline - Q1: cuDF COUNT(*)"""
        start_time = time.time()

        # Enable RAPIDS GPU acceleration
        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")
        spark_connect_session.conf.set("spark.rapids.sql.python.enabled", "true")

        # Load data with GPU
        df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df.createOrReplaceTempView("nyc_taxi")

        # Execute GPU-accelerated query
        result = spark_connect_session.sql("SELECT COUNT(*) AS row_count FROM nyc_taxi")
        count = result.collect()[0]['row_count']

        execution_time = time.time() - start_time

        # Validate
        assert count > 0, "Row count should be greater than 0"
        assert count > 1000000, "Full dataset should have more than 1M rows"

        # Collect GPU metrics
        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "row_count", count)
        metrics_collector(self._testMethodName, "acceleration", "RAPIDS")

        # Verify GPU was used (check execution plan)
        plan = spark_connect_session.sql("SELECT COUNT(*) FROM nyc_taxi").explain(True)
        plan_str = str(plan)
        # GpuColumnarToRow or similar indicates GPU usage
        assert "Gpu" in plan_str or "gpu" in plan_str.lower(), \
            "Query should use GPU acceleration"

    @pytest.mark.timeout(900)
    @pytest.mark.gpu
    def test_02_spark_357_gpu_baseline_q2_cudf_groupby(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 3.5.7 GPU baseline - Q2: cuDF GROUP BY"""
        start_time = time.time()

        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")

        df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df.createOrReplaceTempView("nyc_taxi")

        result = spark_connect_session.sql("""
            SELECT passenger_count, COUNT(*) AS trip_count, AVG(total_amount) AS avg_fare
            FROM nyc_taxi
            GROUP BY passenger_count
            ORDER BY passenger_count
        """)
        rows = result.collect()

        execution_time = time.time() - start_time

        assert len(rows) > 0, "GPU GROUP BY should return results"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "result_count", len(rows))
        metrics_collector(self._testMethodName, "acceleration", "RAPIDS")

    @pytest.mark.timeout(900)
    @pytest.mark.gpu
    def test_03_spark_357_gpu_baseline_q3_cudf_join(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 3.5.7 GPU baseline - Q3: cuDF JOIN"""
        start_time = time.time()

        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")

        df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df.createOrReplaceTempView("nyc_taxi")

        result = spark_connect_session.sql("""
            SELECT t1.passenger_count, COUNT(*) AS trip_count
            FROM nyc_taxi t1
            INNER JOIN nyc_taxi t2 ON t1.passenger_count = t2.passenger_count
            WHERE t1.trip_distance > 5 AND t2.trip_distance > 5
            GROUP BY t1.passenger_count
            ORDER BY t1.passenger_count
        """)
        rows = result.collect()

        execution_time = time.time() - start_time

        assert len(rows) > 0, "GPU JOIN should return results"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "result_count", len(rows))
        metrics_collector(self._testMethodName, "acceleration", "RAPIDS")

    @pytest.mark.timeout(900)
    @pytest.mark.gpu
    def test_04_spark_357_gpu_baseline_q4_cuml_lr(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 3.5.7 GPU baseline - Q4: cuML Linear Regression"""
        start_time = time.time()

        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")
        spark_connect_session.conf.set("spark.rapids.ml.enabled", "true")

        from pyspark.ml.regression import LinearRegression
        from pyspark.ml.feature import VectorAssembler

        # Load sample data for ML (using smaller dataset for training)
        df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df = df.select("passenger_count", "trip_distance", "total_amount").na.drop()
        df = df.sample(0.01)  # Use 1% for training to reduce time

        # Prepare features
        assembler = VectorAssembler(
            inputCols=["passenger_count", "trip_distance"],
            outputCol="features"
        )
        df = assembler.transform(df)

        # Train Linear Regression on GPU
        lr = LinearRegression(featuresCol="features", labelCol="total_amount")
        model = lr.fit(df)

        execution_time = time.time() - start_time

        # Validate
        assert model is not None, "Model should be trained"
        assert model.coefficients is not None, "Model should have coefficients"

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "acceleration", "RAPIDS cuML")

    # Similar tests for other combinations...
    # test_05 through test_16 for all version/variant combinations

    @pytest.mark.timeout(900)
    @pytest.mark.gpu
    def test_05_spark_357_gpu_optimized_q1_cudf_count(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Spark 3.5.7 GPU optimized - Q1: cuDF COUNT (optimized settings)"""
        start_time = time.time()

        # Optimized RAPIDS settings
        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")
        spark_connect_session.conf.set("spark.rapids.sql.batchSizeBytes", "1G")
        spark_connect_session.conf.set("spark.rapids.sql.reader.batchSizeRows", "1000000")
        spark_connect_session.conf.set("spark.rapids.sql.variableFloatAgg.enabled", "true")
        spark_connect_session.conf.set("spark.rapids.sql.castFloatToInt.enabled", "true")

        df = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df.createOrReplaceTempView("nyc_taxi")

        result = spark_connect_session.sql("SELECT COUNT(*) AS row_count FROM nyc_taxi")
        count = result.collect()[0]['row_count']

        execution_time = time.time() - start_time

        assert count > 0

        metrics_collector(self._testMethodName, "execution_time", execution_time)
        metrics_collector(self._testMethodName, "row_count", count)
        metrics_collector(self._testMethodName, "acceleration", "RAPIDS Optimized")

    # Additional tests for Spark 3.5.8, 4.1.0, 4.1.1...
    # Following same pattern as tests 01-05 above

    @pytest.mark.timeout(900)
    @pytest.mark.gpu
    def test_gpu_vs_cpu_speedup(self, spark_connect_session, nyc_taxi_dataset_path, metrics_collector):
        """Compare GPU vs CPU execution speed for same query"""
        # CPU execution
        start_time = time.time()
        spark_connect_session.conf.set("spark.rapids.sql.enabled", "false")
        df_cpu = spark_connect_session.read.parquet(str(nyc_taxi_dataset_path))
        df_cpu.createOrReplaceTempView("nyc_taxi")
        result_cpu = spark_connect_session.sql("""
            SELECT passenger_count, COUNT(*) AS trip_count
            FROM nyc_taxi
            GROUP BY passenger_count
        """)
        result_cpu.collect()
        cpu_time = time.time() - start_time

        # GPU execution
        start_time = time.time()
        spark_connect_session.conf.set("spark.rapids.sql.enabled", "true")
        result_gpu = spark_connect_session.sql("""
            SELECT passenger_count, COUNT(*) AS trip_count
            FROM nyc_taxi
            GROUP BY passenger_count
        """)
        result_gpu.collect()
        gpu_time = time.time() - start_time

        speedup = cpu_time / gpu_time if gpu_time > 0 else 0

        metrics_collector("gpu_vs_cpu", "cpu_time", cpu_time)
        metrics_collector("gpu_vs_cpu", "gpu_time", gpu_time)
        metrics_collector("gpu_vs_cpu", "speedup_factor", speedup)

        # GPU should be faster (or at least not much slower)
        assert speedup > 0.5, f"GPU should be competitive, speedup={speedup}"
```

## Scope Estimate
- Files: 1 (test_06_02_gpu_e2e.py)
- LOC: ~900 (tests + GPU-specific logic)
- Scenarios: 16
- Size: MEDIUM
