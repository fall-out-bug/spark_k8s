---
ws_id: 00-013-04
feature: F13
status: backlog
size: MEDIUM
project_id: 00
github_issue: null
assignee: null
depends_on:
  - 00-006-01  # Helm charts
  - 00-012-01  # Core E2E
---

## WS-00-013-04: Comparison load (4 scenarios)

### üéØ Goal

**What must WORK after completing this WS:**
- 4 comparison scenarios (3.5.8 vs 4.1.1)
- Performance regression detection
- Version comparison report

**Acceptance Criteria:**
- [ ] AC1: 4 comparison scenarios created
- [ ] AC2: Same queries run on both versions
- [ ] AC3: Performance comparison report generated
- [ ] AC4: No significant regressions (> 20% slowdown)
- [ ] AC5: Both versions stable under load

**‚ö†Ô∏è WS is NOT complete until Goal is achieved (all AC ‚úÖ).**

---

### Scenarios

| Scenario | Version A | Version B | Query Type | Metric |
|----------|-----------|-----------|------------|--------|
| 1 | 3.5.8 | 4.1.1 | SELECT + aggregation | Throughput |
| 2 | 3.5.8 | 4.1.1 | JOIN + filter | Latency p95 |
| 3 | 3.5.8 | 4.1.1 | Window function | Memory usage |
| 4 | 3.5.8 | 4.1.1 | Mixed workload | Error rate |

### Dependencies

- WS-006-01 (Helm charts)
- WS-012-01 (Core E2E)

### Code

```python
# tests/load/comparison/test_version_comparison.py
import pytest

@pytest.mark.timeout(4800)  # 80 minutes (2 x 40 min)
def test_version_comparison_select_aggregation(
    spark_358_client,
    spark_411_client
):
    """
    Compare Spark 3.5.8 vs 4.1.1 performance
    Same query, 30 minutes each version
    """
    query = """
        SELECT
            passenger_count,
            SUM(fare_amount) as total_fare,
            AVG(trip_distance) as avg_distance,
            COUNT(*) as trip_count
        FROM nyc_taxi
        GROUP BY passenger_count
    """

    # Test 3.5.8
    metrics_358 = run_sustained_load(
        spark_358_client,
        query,
        duration_sec=1800,
        interval_sec=1
    )

    # Test 4.1.1
    metrics_411 = run_sustained_load(
        spark_411_client,
        query,
        duration_sec=1800,
        interval_sec=1
    )

    # Comparison assertions
    regression = (metrics_358["throughput"] - metrics_411["throughput"]) / metrics_358["throughput"]
    assert regression < 0.20, f"Significant regression: {regression:.1%} slowdown"

    # Generate report
    report = {
        "query": "select_aggregation",
        "spark_358": metrics_358,
        "spark_411": metrics_411,
        "throughput_diff_pct": (metrics_411["throughput"] - metrics_358["throughput"]) / metrics_358["throughput"] * 100,
        "latency_p95_diff_pct": (metrics_411["latency_p95"] - metrics_358["latency_p95"]) / metrics_358["latency_p95"] * 100
    }

    return report

def run_sustained_load(client, query, duration_sec, interval_sec):
    """Helper for sustained load execution"""
    start_time = datetime.now()
    end_time = start_time + timedelta(seconds=duration_sec)

    latencies = []
    queries_total = 0
    queries_success = 0

    while datetime.now() < end_time:
        try:
            query_start = datetime.now()
            client.sql(query).collect()
            query_end = datetime.now()

            latencies.append((query_end - query_start).total_seconds() * 1000)
            queries_success += 1
        except Exception:
            pass
        finally:
            queries_total += 1
            time.sleep(interval_sec)

    latencies.sort()
    return {
        "throughput": queries_success / duration_sec,
        "latency_p50": latencies[len(latencies) // 2],
        "latency_p95": latencies[int(len(latencies) * 0.95)],
        "latency_p99": latencies[int(len(latencies) * 0.99)],
        "error_rate": (queries_total - queries_success) / queries_total
    }
```

### Scope Estimate

- Files: 5
- Lines: ~600 (MEDIUM)
- Tokens: ~4500

### Constraints

- DO use identical queries and data
- DO use same resource configuration
- DO generate comparison report
- DO flag regressions > 20%

---

## Execution Report

**Executed by:** ______
**Date:** ______
**Duration:** ______ minutes

### Goal Status
- [ ] AC1-AC5 ‚Äî ‚úÖ

**Goal Achieved:** ______
