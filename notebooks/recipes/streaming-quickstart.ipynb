{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Streaming Quickstart: Kafka → Iceberg\n",
    "\n",
    "This notebook demonstrates a real-time pipeline: Kafka source → transformation → Iceberg sink.\n",
    "\n",
    "**Note:** Structured Streaming requires Spark in local or cluster mode (not Spark Connect). Run this notebook with `local[*]` master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Start Kafka first: `docker compose -f examples/streaming/docker-compose.yaml up -d`\n",
    "\n",
    "Then create the Spark session with Kafka and Iceberg packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType\n",
    "\n",
    "KAFKA_BOOTSTRAP = os.environ.get(\"KAFKA_BOOTSTRAP_SERVERS\", \"localhost:9092\")\n",
    "CHECKPOINT = os.environ.get(\"CHECKPOINT_LOCATION\", \"/tmp/streaming-checkpoint\")\n",
    "\n",
    "# Spark 4.1: use spark-sql-kafka-0-10_2.13:4.1.0; Spark 3.5: use _2.12:3.5.0\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"streaming-quickstart\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.1.0\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://warehouse/iceberg\")\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", CHECKPOINT)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Spark {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define schema and read from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"event_ts\", LongType(), True),\n",
    "])\n",
    "\n",
    "stream_df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribe\", \"events\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    "    .withColumn(\"processing_time\", current_timestamp())\n",
    ")\n",
    "\n",
    "stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run streaming query (console sink for demo)\n",
    "\n",
    "For production, use Iceberg sink. See `examples/streaming/kafka_to_iceberg.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    stream_df.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT)\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Run for 60 seconds\n",
    "query.awaitTermination(60)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full pipeline (spark-submit)\n",
    "\n",
    "For Kafka → Iceberg with exactly-once, run:\n",
    "\n",
    "```bash\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.1.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.5.0 examples/streaming/kafka_to_iceberg.py\n",
    "```\n",
    "\n",
    "See `examples/streaming/README.md` for full setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
