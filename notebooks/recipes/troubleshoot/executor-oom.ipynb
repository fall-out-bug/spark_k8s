{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executor OOM Troubleshooting\n",
    "\n",
    "**Цель:** Диагностика и исправление OutOfMemoryError у Spark executors\n",
    "\n",
    "**Symptoms:**\n",
    "- `OOMKilled` в pod status\n",
    "- `java.lang.OutOfMemoryError` в логах\n",
    "- Job завершается с ошибкой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Импортируйте библиотеки и подключитесь к Spark Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import kubernetes\n",
    "import json\n",
    "\n",
    "# Настройка\n",
    "SPARK_CONNECT_URL = os.environ.get('SPARK_CONNECT_URL', 'sc://spark-connect:15002')\n",
    "NAMESPACE = os.environ.get('NAMESPACE', 'spark')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .remote(SPARK_CONNECT_URL) \\n",
    "    .getOrCreate()\n",
    "\n",
    "# Kubernetes API\n",
    "k8s = kubernetes.config.load_kube_config()\n",
    "v1 = kubernetes.client.CoreV1Api(api_client=k8s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check for OOMKilled Pods\n",
    "\n",
    "Проверить есть ли executor pods с OOMKilled статусом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получить все executor pods\n",
    "pods = v1.list_namespaced_pod(NAMESPACE, label_selector='spark-role=executor')\n",
    "\n",
    "oom_pods = []\n",
    "for pod in pods.items:\n",
    "    for container_status in pod.status.container_statuses:\n",
    "        if container_status.terminated and container_status.terminated_reason == 'OOMKilled':\n",
    "            oom_pods.append(pod.metadata.name)\n",
    "\n",
    "print(f\"Found {len(oom_pods)} OOMKilled pods:\")\n",
    "for pod_name in oom_pods:\n",
    "    print(f\"  - {pod_name}\")\n",
    "\n",
    "if len(oom_pods) == 0:\n",
    "    print(\"✅ No OOMKilled pods found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analyze Memory Usage\n",
    "\n",
    "Изучить memory consumption executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получить метрики памяти через Spark UI\n",
    "from pyspark.sparkcontext import SparkContext\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Executor memory stats\n",
    "executor_memory = sc._jsc.sc().getExecutorMemoryStatus().collectAsMap()\n",
    "\n",
    "print(\"Executor Memory Usage:\")\n",
    "for executor_id, stats in executor_memory.items():\n",
    "    used_mb = stats.memoryUsed() / 1024 / 1024\n",
    "    total_mb = stats.totalOnHeap() / 1024 / 1024\n",
    "    usage_pct = (used_mb / total_mb * 100) if total_mb > 0 else 0\n",
    "    print(f\"  {executor_id}: {used_mb:.1f}MB / {total_mb:.1f}MB ({usage_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check Current Configuration\n",
    "\n",
    "Изучить текущую memory конфигурацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получить текущую конфигурацию\n",
    "conf = spark._jconf.getAll()\n",
    "\n",
    "memory_settings = {k: v for k, v in conf.items() if 'memory' in k.lower()}\n",
    "\n",
    "print(\"Current Memory Settings:\")\n",
    "for key, value in sorted(memory_settings.items()):\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Recommendations\n",
    "\n",
    "**Диагностика завершена. Вот рекомендации:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\"\n",
    "## Рекомендации:\n",
    "\n",
    "### 1. Увеличьте executor.memory\n",
    "```bash\n",
    "helm upgrade spark-connect charts/spark-4.1 -n spark \\\\\n",
    "  --set connect.executor.memory=4g \\\\\n",
    "  --set connect.executor.memoryLimit=5g\n",
    "```  \n",
    "\n",
    "### 2. Уменьшите memoryOverhead\n",
    "```bash\n",
    "helm upgrade spark-connect charts/spark-4.1 -n spark \\\\\n",
    "  --set connect.executor.memoryOverhead=512m\n",
    "```  \n",
    "\n",
    "### 3. Уменьшите shuffle partition size\n",
    "```python\n",
    "df = spark.repartition(10)  # меньше partitions\n",
    "```  \n",
    "\n",
    "### 4. Включите off-heap memory\n",
    "```bash\n",
    "helm upgrade spark-connect charts/spark-4.1 -n spark \\\\\n",
    "  --set connect.sparkConf.spark\\.memory.offHeap.enabled=true \\\\\n",
    "  --set connect.sparkConf.spark\\.memory.offHeap.size=1g\n",
    "```  \n",
    "\n",
    "### 5. Используйте adaptive execution\n",
    "```python\n",
    "spark.conf.set('spark.sql.adaptive.enabled', 'true')\n",
    "spark.conf.set('spark.sql.adaptive.coalescePartitions.enabled', 'true')\n",
    "```  \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"✅ Diagnostics complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
