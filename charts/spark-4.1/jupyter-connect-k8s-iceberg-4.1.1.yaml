# Preset: Jupyter + Spark Connect + Iceberg (smoke test - local mode)
# Source: Spark 4.1.0 local mode with Iceberg libs
# Components: Connect + Jupyter (Hive Metastore disabled for smoke test)

global:
  s3:
    enabled: false
    endpoint: "http://minio:9000"
    accessKey: "minioadmin"
    secretKey: "minioadmin"
    pathStyleAccess: true
    sslEnabled: false

spark-base:
  minio:
    enabled: false
  postgresql:
    enabled: false

rbac:
  create: true
  serviceAccountName: "spark-41"

core:
  enabled: false
  hiveMetastore:
    enabled: false

connect:
  replicas: 1
  backendMode: local
  image:
    repository: spark-custom
    tag: "4.1.0"
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: "0"
      memory: "512Mi"
    limits:
      cpu: "500m"
      memory: "2Gi"
  sparkConf:
    # Iceberg with Hadoop catalog (no Hive Metastore needed)
    "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions"
    "spark.sql.catalog.spark_catalog": "org.apache.iceberg.spark.SparkSessionCatalog"
    "spark.sql.catalog.spark_catalog.type": "hadoop"
    "spark.sql.catalog.local": "org.apache.iceberg.spark.SparkCatalog"
    "spark.sql.catalog.local.type": "hadoop"
    "spark.sql.catalog.local.warehouse": "/tmp/warehouse/iceberg"
    "spark.sql.iceberg.vectorization.enabled": "true"
  driver:
    host: ""
    port: 7078
  eventLog:
    enabled: false

jupyter:
  enabled: true
  image:
    repository: jupyter-spark
    tag: "4.1.0"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8888
  env:
    SPARK_CONNECT_URL: "sc://spark-connect:15002"
  resources:
    requests:
      cpu: "0"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "1Gi"
  persistence:
    enabled: false

hiveMetastore:
  enabled: false
  initSchema: false

historyServer:
  enabled: false

ingress:
  enabled: false

security:
  podSecurityStandards: false

sparkOperator:
  enabled: false

celeborn:
  enabled: false

monitoring:
  serviceMonitor:
    enabled: false
