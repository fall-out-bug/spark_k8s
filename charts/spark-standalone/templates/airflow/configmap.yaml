{{- if .Values.airflow.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "spark-standalone.fullname" . }}-airflow-config
  labels:
    app: airflow
    {{- include "spark-standalone.labels" . | nindent 4 }}
data:
  AIRFLOW__CORE__EXECUTOR: "KubernetesExecutor"
  AIRFLOW__CORE__LOAD_EXAMPLES: "False"
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "False"
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
  AIRFLOW__KUBERNETES__NAMESPACE: {{ default .Release.Namespace .Values.airflow.kubernetesExecutor.namespace | quote }}
  AIRFLOW__KUBERNETES__WORKER_CONTAINER_REPOSITORY: {{ default .Values.sparkMaster.image.repository .Values.airflow.kubernetesExecutor.workerImage.repository | quote }}
  AIRFLOW__KUBERNETES__WORKER_CONTAINER_TAG: {{ default .Values.sparkMaster.image.tag .Values.airflow.kubernetesExecutor.workerImage.tag | quote }}
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: {{ printf "postgresql+psycopg2://%s:%s@%s:%d/%s" .Values.airflow.postgresql.username .Values.airflow.postgresql.password .Values.airflow.postgresql.host (int .Values.airflow.postgresql.port) .Values.airflow.postgresql.database | quote }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "spark-standalone.fullname" . }}-airflow-dags
  labels:
    app: airflow
    {{- include "spark-standalone.labels" . | nindent 4 }}
data:
  example_bash_operator.py: |
    from datetime import datetime
    from airflow import DAG
    from airflow.operators.bash import BashOperator

    with DAG(
        dag_id="example_bash_operator",
        start_date=datetime(2024, 1, 1),
        schedule=None,
        catchup=False,
        tags=["test"],
    ) as dag:
        BashOperator(
            task_id="echo",
            bash_command="echo hello-from-airflow",
        )

  spark_etl_synthetic.py: |
    """
    Synthetic Spark ETL DAG (Spark Standalone + MinIO).
    """
    from datetime import datetime

    from airflow import DAG
    from airflow.models import Variable
    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
    import textwrap

    SPARK_IMAGE = Variable.get("spark_image", default_var="spark-custom:3.5.7")
    SPARK_NAMESPACE = Variable.get("spark_namespace", default_var="default")
    SPARK_MASTER = Variable.get("spark_standalone_master", default_var="spark://spark-sa-spark-standalone-master:7077")

    S3_ENDPOINT = Variable.get("s3_endpoint", default_var="http://minio:9000")
    S3_ACCESS_KEY = Variable.get("s3_access_key", default_var="minioadmin")
    S3_SECRET_KEY = Variable.get("s3_secret_key", default_var="minioadmin")

    ETL_SCRIPT = textwrap.dedent(r'''
        from pyspark.sql import SparkSession
        from pyspark.sql.functions import rand, expr

        spark = SparkSession.builder.appName("SyntheticETL").getOrCreate()

        df = (
            spark.range(0, 100000)
            .withColumn("value", rand())
            .withColumn("category", expr("id % 10"))
        )

        result = df.groupBy("category").agg({"value": "avg", "id": "count"})
        result.write.mode("overwrite").parquet("s3a://processed-data/synthetic/")
        spark.stop()
    ''')

    with DAG(
        dag_id="spark_etl_synthetic",
        start_date=datetime(2024, 1, 1),
        schedule=None,
        catchup=False,
        tags=["spark", "etl", "synthetic"],
    ) as dag:
        KubernetesPodOperator(
            task_id="spark_etl_synthetic",
            name="spark-etl-synthetic",
            namespace=SPARK_NAMESPACE,
            image=SPARK_IMAGE,
            cmds=["bash", "-lc"],
            arguments=[
                "cat > /tmp/etl.py <<'PY'\n"
                + ETL_SCRIPT
                + "\nPY\n"
                + f"spark-submit --master {SPARK_MASTER} /tmp/etl.py",
            ],
            env_vars={
                "S3_ENDPOINT": S3_ENDPOINT,
                "AWS_ACCESS_KEY_ID": S3_ACCESS_KEY,
                "AWS_SECRET_ACCESS_KEY": S3_SECRET_KEY,
            },
            get_logs=True,
            is_delete_operator_pod=True,
        )

  mlflow_training_synthetic.py: |
    """
    Synthetic ML training DAG with MLflow logging.
    """
    from datetime import datetime

    from airflow import DAG
    from airflow.models import Variable
    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator

    SPARK_IMAGE = Variable.get("spark_image", default_var="spark-custom:3.5.7")
    SPARK_NAMESPACE = Variable.get("spark_namespace", default_var="default")
    SPARK_MASTER = Variable.get("spark_standalone_master", default_var="spark://spark-sa-spark-standalone-master:7077")
    MLFLOW_TRACKING_URI = Variable.get("mlflow_tracking_uri", default_var="http://spark-sa-spark-standalone-mlflow:5000")

    S3_ENDPOINT = Variable.get("s3_endpoint", default_var="http://minio:9000")
    S3_ACCESS_KEY = Variable.get("s3_access_key", default_var="minioadmin")
    S3_SECRET_KEY = Variable.get("s3_secret_key", default_var="minioadmin")

    TRAIN_SCRIPT = textwrap.dedent(r'''
        from pyspark.sql import SparkSession
        from pyspark.ml.feature import VectorAssembler
        from pyspark.ml.regression import LinearRegression
        from pyspark.ml.evaluation import RegressionEvaluator
        import mlflow
        import mlflow.spark

        spark = SparkSession.builder.appName("MLflowTraining").getOrCreate()

        df = spark.range(0, 10000).selectExpr(
            "id",
            "rand() as feature1",
            "rand() as feature2",
            "id * 0.5 + rand() * 10 as label",
        )

        assembler = VectorAssembler(inputCols=["feature1", "feature2"], outputCol="features")
        data = assembler.transform(df)
        train, test = data.randomSplit([0.8, 0.2], seed=42)

        mlflow.set_tracking_uri("MLFLOW_TRACKING_URI")
        mlflow.set_experiment("spark-standalone-training")

        with mlflow.start_run(run_name="linear-regression"):
            lr = LinearRegression(featuresCol="features", labelCol="label")
            model = lr.fit(train)
            predictions = model.transform(test)
            evaluator = RegressionEvaluator(labelCol="label", predictionCol="prediction")
            rmse = evaluator.evaluate(predictions, {evaluator.metricName: "rmse"})
            r2 = evaluator.evaluate(predictions, {evaluator.metricName: "r2"})
            mlflow.log_metric("rmse", rmse)
            mlflow.log_metric("r2", r2)
            mlflow.spark.log_model(model, "model")

        spark.stop()
    ''').replace("MLFLOW_TRACKING_URI", MLFLOW_TRACKING_URI)

    with DAG(
        dag_id="mlflow_training_synthetic",
        start_date=datetime(2024, 1, 1),
        schedule=None,
        catchup=False,
        tags=["spark", "mlflow", "synthetic"],
    ) as dag:
        KubernetesPodOperator(
            task_id="mlflow_training_synthetic",
            name="mlflow-training-synthetic",
            namespace=SPARK_NAMESPACE,
            image=SPARK_IMAGE,
            cmds=["bash", "-lc"],
            arguments=[
                "cat > /tmp/train.py <<'PY'\n"
                + TRAIN_SCRIPT
                + "\nPY\n"
                + f"spark-submit --master {SPARK_MASTER} /tmp/train.py",
            ],
            env_vars={
                "S3_ENDPOINT": S3_ENDPOINT,
                "AWS_ACCESS_KEY_ID": S3_ACCESS_KEY,
                "AWS_SECRET_ACCESS_KEY": S3_SECRET_KEY,
            },
            get_logs=True,
            is_delete_operator_pod=True,
        )
{{- end }}
