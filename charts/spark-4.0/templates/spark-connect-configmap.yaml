{{- if .Values.connect.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "spark-4.0.fullname" . }}-connect-config
  labels:
    {{- include "spark-4.0.labels" . | nindent 4 }}
data:
  spark-properties.conf.template: |-
    {{- if eq .Values.connect.backendMode "standalone" }}
    # Spark Master - Standalone backend mode
    spark.master=spark://{{ .Values.connect.standalone.masterService }}:{{ .Values.connect.standalone.masterPort }}
    spark.driver.bindAddress=0.0.0.0
    spark.driver.host={{ default (printf "%s-connect" (include "spark-4.0.fullname" .)) .Values.connect.driver.host }}
    spark.driver.port={{ .Values.connect.driver.port }}
    spark.driver.blockManager.port={{ .Values.connect.driver.blockManagerPort }}
    {{- else if eq .Values.connect.backendMode "local" }}
    # Spark Master - Local mode (for smoke tests)
    spark.master=local[*]
    {{- else }}
    # Spark Master - Kubernetes executors mode
    spark.master=k8s://https://kubernetes.default.svc.cluster.local:443

    # K8s executor config
    spark.kubernetes.namespace={{ .Release.Namespace }}
    spark.kubernetes.executor.podTemplateFile=/tmp/spark-conf/executor-pod-template.yaml
    spark.kubernetes.executor.request.cores={{ .Values.connect.executor.cores }}
    spark.kubernetes.executor.limit.cores={{ .Values.connect.executor.coresLimit }}
    spark.kubernetes.executor.request.memory={{ .Values.connect.executor.memory }}
    spark.kubernetes.container.image={{ .Values.connect.image.repository }}:{{ .Values.connect.image.tag }}
    spark.driver.bindAddress=0.0.0.0
    spark.driver.host={{ default (printf "%s-connect" (include "spark-4.0.fullname" .)) .Values.connect.driver.host }}
    spark.driver.port={{ .Values.connect.driver.port }}
    spark.driver.blockManager.port={{ .Values.connect.driver.blockManagerPort }}

    # Dynamic allocation
    spark.dynamicAllocation.enabled={{ .Values.connect.dynamicAllocation.enabled }}
    spark.dynamicAllocation.shuffleTracking.enabled=true
    spark.dynamicAllocation.minExecutors={{ .Values.connect.dynamicAllocation.minExecutors }}
    spark.dynamicAllocation.maxExecutors={{ .Values.connect.dynamicAllocation.maxExecutors }}

    # AQE and executor observability
    spark.kubernetes.executor.podNamePrefix={{ .Release.Name }}-exec
    {{- end }}

    # AQE (Adaptive Query Execution) - enabled by default for +20-40% perf
    spark.sql.adaptive.enabled=true
    spark.sql.adaptive.coalescePartitions.enabled=true
    spark.sql.adaptive.skewJoin.enabled=true

    spark.connect.grpc.binding.address=0.0.0.0
    spark.connect.grpc.binding.port=15002

    # Event log (for History Server)
    {{- if .Values.connect.eventLog.enabled }}
    spark.eventLog.enabled=true
    spark.eventLog.dir={{ .Values.connect.eventLog.dir }}
    {{- else }}
    spark.eventLog.enabled=false
    {{- end }}

    {{- if .Values.connect.sparkConf }}
    # Extra Spark conf (user overrides)
    {{- range $key, $value := .Values.connect.sparkConf }}
    {{ $key }}={{ $value }}
    {{- end }}
    {{- end }}

    {{- if .Values.celeborn.enabled }}
    # Celeborn shuffle
    spark.shuffle.manager=org.apache.spark.shuffle.celeborn.RssShuffleManager
    spark.celeborn.master.endpoints={{ .Values.celeborn.masterEndpoints }}
    spark.celeborn.push.replicate.enabled=true
    spark.celeborn.client.spark.shuffle.writer=hash
    spark.celeborn.client.spark.fetch.throwsFetchFailure=true
    {{- end }}

    {{- if .Values.global.s3.enabled }}
    # S3 config
    spark.hadoop.fs.s3a.endpoint={{ .Values.global.s3.endpoint }}
    spark.hadoop.fs.s3a.access.key=${AWS_ACCESS_KEY_ID}
    spark.hadoop.fs.s3a.secret.key=${AWS_SECRET_ACCESS_KEY}
    spark.hadoop.fs.s3a.path.style.access={{ .Values.global.s3.pathStyleAccess }}
    {{- end }}

    {{- if and .Values.features .Values.features.iceberg .Values.features.iceberg.enabled }}
    # Iceberg configuration
    spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
    spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
    spark.sql.catalog.spark_catalog.type={{ .Values.features.iceberg.catalogType | default "hadoop" }}
    spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.iceberg.type={{ .Values.features.iceberg.catalogType | default "hadoop" }}
    spark.sql.catalog.iceberg.warehouse={{ .Values.features.iceberg.warehouse | default "s3a://warehouse/iceberg" }}
    spark.sql.catalog.iceberg.io-impl={{ .Values.features.iceberg.ioImpl | default "org.apache.iceberg.hadoop.HadoopFileIO" }}
    {{- if eq .Values.features.iceberg.catalogType "hive" }}
    spark.sql.catalog.iceberg.uri={{ .Values.hiveMetastore.service.host | default "hive-metastore" }}:9083
    {{- end }}
    {{- if eq .Values.features.iceberg.catalogType "rest" }}
    spark.sql.catalog.iceberg.uri={{ .Values.features.iceberg.restCatalogUri | required "restCatalogUri required when catalogType is 'rest'" }}
    spark.sql.catalog.iceberg.token={{ .Values.features.iceberg.restCatalogToken | default "" }}
    {{- end }}
    spark.sql.iceberg.vectorization.enabled=true
    spark.sql.iceberg.v2.enabled=true
    spark.sql.iceberg.delete-planning.enabled=true
    spark.sql.iceberg.deletes.enabled=true
    {{- end }}

    {{- if and .Values.features .Values.features.openLineage .Values.features.openLineage.enabled }}
    # OpenLineage configuration
    spark.extraListeners=io.openlineage.spark.OpenLineageSparkListener
    spark.openlineage.host={{ .Values.features.openLineage.endpoint | default "http://marquez:5000" }}
    spark.openlineage.namespace={{ .Values.features.openLineage.namespace | default "spark-k8s" }}
    {{- if .Values.features.openLineage.jobPrefix }}
    spark.openlineage.jobPrefix={{ .Values.features.openLineage.jobPrefix }}
    {{- end }}
    {{- if .Values.features.openLineage.facets }}
    spark.openlineage.facets.spark.version.enabled={{ .Values.features.openLineage.facets.sparkVersion | default true }}
    spark.openlineage.facets.source.code.enabled={{ .Values.features.openLineage.facets.sourceCode | default false }}
    {{- end }}
    {{- if eq .Values.features.openLineage.transport.type "kafka" }}
    spark.openlineage.transport.type=kafka
    spark.openlineage.transport.topic={{ .Values.features.openLineage.transport.kafka.topic }}
    spark.openlineage.transport.brokers={{ .Values.features.openLineage.transport.kafka.brokers }}
    {{- else if eq .Values.features.openLineage.transport.type "file" }}
    spark.openlineage.transport.type=file
    spark.openlineage.transport.path={{ .Values.features.openLineage.transport.file.path }}
    {{- end }}
    {{- end }}
{{- end }}
