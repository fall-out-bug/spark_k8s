{{- if .Values.connect.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "spark-3.5.fullname" . }}-connect
  labels:
    app: spark-connect
    app.kubernetes.io/component: connect
    {{- include "spark-3.5.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.connect.replicas }}
  selector:
    matchLabels:
      app: spark-connect
      app.kubernetes.io/component: connect
  template:
    metadata:
      labels:
        app: spark-connect
        app.kubernetes.io/component: connect
        {{- include "spark-3.5.selectorLabels" . | nindent 8 }}
      {{- if .Values.connect.metrics.enabled }}
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: {{ .Values.connect.metrics.port | quote }}
        prometheus.io/path: "/metrics/prometheus"
      {{- end }}
    spec:
      serviceAccountName: {{ include "spark-3.5.serviceAccountName" . }}
      {{- if .Values.security.podSecurityStandards }}
      securityContext:
        {{- include "spark-base.podSecurityContext" . | nindent 8 }}
      {{- end }}
      {{- with .Values.connect.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.connect.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      volumes:
        - name: spark-connect-config
          configMap:
            name: {{ include "spark-3.5.fullname" . }}-connect-config
        - name: executor-pod-template
          configMap:
            name: {{ include "spark-3.5.fullname" . }}-executor-pod-template
        - name: spark-connect-writable
          emptyDir: {}
        - name: ivy-cache
          emptyDir: {}
      initContainers:
        - name: render-spark-config
          image: alpine:3.19
          command:
            - /bin/sh
            - -c
          args:
            - >-
              apk add --no-cache gettext &&
              envsubst < /opt/spark/conf-k8s-templates/spark-properties.conf.template > /tmp/spark-conf/spark-defaults.conf &&
              envsubst < /opt/spark/conf-k8s-templates/executor-pod-template.yaml.template > /tmp/spark-conf/executor-pod-template.yaml
          env:
            {{- if .Values.global.s3.enabled }}
            {{- if .Values.global.s3.existingSecret }}
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.global.s3.existingSecret }}
                  key: access-key
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.global.s3.existingSecret }}
                  key: secret-key
            {{- else }}
            - name: AWS_ACCESS_KEY_ID
              value: {{ .Values.global.s3.accessKey | quote }}
            - name: AWS_SECRET_ACCESS_KEY
              value: {{ .Values.global.s3.secretKey | quote }}
            {{- end }}
            {{- end }}
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: IVY_DIR
              value: "/tmp/spark-ivy-cache"

          {{- if .Values.security.podSecurityStandards }}
          securityContext:
            {{- include "spark-base.initContainerSecurityContext" . | nindent 12 }}
          {{- end }}
          volumeMounts:
            - name: spark-connect-config
              mountPath: /opt/spark/conf-k8s-templates/spark-properties.conf.template
              subPath: spark-properties.conf.template
            - name: executor-pod-template
              mountPath: /opt/spark/conf-k8s-templates/executor-pod-template.yaml.template
              subPath: executor-pod-template.yaml.template
            - name: spark-connect-writable
              mountPath: /tmp/spark-conf
      containers:
        - name: spark-connect
          image: "{{ .Values.connect.image.repository }}:{{ .Values.connect.image.tag }}"
          imagePullPolicy: {{ .Values.connect.image.pullPolicy }}
          env:
            - name: SPARK_MODE
              value: "connect"
            - name: SPARK_CONNECT_URL
              value: "sc://0.0.0.0:15002"
            - name: SPARK_CONF_DIR
              value: "/tmp/spark-conf"
            - name: HADOOP_USER_NAME
              value: "spark"
            - name: HADOOP_SECURITY_AUTHENTICATION
              value: "simple"
            - name: SPARK_SUBMIT_OPTS
              value: "-Dhadoop.security.authentication=simple"
            {{- range $key, $val := .Values.connect.sparkConf }}
            - name: SPARK_{{ $key | replace "." "_" | upper }}
              value: {{ $val | quote }}
            {{- end }}
            {{- if and .Values.global.s3.enabled .Values.global.s3.existingSecret }}
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.global.s3.existingSecret }}
                  key: access-key
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.global.s3.existingSecret }}
                  key: secret-key
            {{- end }}
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: IVY_DIR
              value: "/tmp/spark-ivy-cache"

          {{- if .Values.security.podSecurityStandards }}
          securityContext:
            {{- include "spark-base.containerSecurityContext" . | nindent 12 }}
          {{- end }}
          command:
            - "/bin/sh"
            - "-c"
          args:
            - >-
              {{- if eq .Values.connect.backendMode "local" }}
              /opt/spark/bin/spark-submit --properties-file /tmp/spark-conf/spark-defaults.conf --master local[*] --class org.apache.spark.sql.connect.service.SparkConnectServer
              {{- else if eq .Values.connect.backendMode "k8s" }}
              /opt/spark/bin/spark-submit --properties-file /tmp/spark-conf/spark-defaults.conf --master k8s://https://kubernetes.default.svc:443 --deploy-mode client --conf spark.driver.host=$(POD_IP) --class org.apache.spark.sql.connect.service.SparkConnectServer
              {{- else if eq .Values.connect.backendMode "standalone" }}
              /opt/spark/bin/spark-submit --properties-file /tmp/spark-conf/spark-defaults.conf --master spark://{{ include "spark-3.5.fullname" . }}-standalone-master:7077 --class org.apache.spark.sql.connect.service.SparkConnectServer
              {{- end }}
          ports:
            - name: grpc
              containerPort: 15002
              protocol: TCP
            {{- if or (eq .Values.connect.backendMode "k8s") (eq .Values.connect.backendMode "standalone") }}
            - name: driver
              containerPort: 7078
              protocol: TCP
            - name: blockmanager
              containerPort: 7079
              protocol: TCP
            {{- end }}
            {{- if .Values.connect.metrics.enabled }}
            - name: metrics
              containerPort: {{ .Values.connect.metrics.port }}
              protocol: TCP
            {{- end }}
          readinessProbe:
            tcpSocket:
              port: 15002
            initialDelaySeconds: 180
            periodSeconds: 10
            failureThreshold: 10
          livenessProbe:
            tcpSocket:
              port: 15002
            initialDelaySeconds: 180
            periodSeconds: 30
            failureThreshold: 10
          volumeMounts:
            - name: spark-connect-config
              mountPath: /opt/spark/conf-k8s-templates/spark-properties.conf.template
              subPath: spark-properties.conf.template
            - name: executor-pod-template
              mountPath: /opt/spark/conf-k8s-templates/executor-pod-template.yaml.template
              subPath: executor-pod-template.yaml.template
            - name: spark-connect-writable
              mountPath: /tmp/spark-conf
            - name: ivy-cache
              mountPath: /tmp/spark-ivy-cache
          resources:
            {{- if and .Values.connect.executor.gpu .Values.connect.executor.gpu.enabled }}
            limits:
              cpu: {{ .Values.connect.resources.limits.cpu | default "2" }}
              memory: {{ .Values.connect.resources.limits.memory | default "4Gi" }}
              {{ .Values.connect.executor.gpu.vendor }}: {{ .Values.connect.executor.gpu.count | quote }}
            requests:
              cpu: {{ .Values.connect.resources.requests.cpu | default "1" }}
              memory: {{ .Values.connect.resources.requests.memory | default "2Gi" }}
            {{- else }}
            {{- toYaml .Values.connect.resources | nindent 12 }}
            {{- end }}
---
apiVersion: v1
kind: Service
metadata:
  name: {{ include "spark-3.5.fullname" . }}-connect
  labels:
    app: spark-connect
    {{- include "spark-3.5.labels" . | nindent 4 }}
spec:
  type: {{ .Values.connect.service.type }}
  ports:
    - name: grpc
      port: {{ .Values.connect.service.port }}
      targetPort: 15002
    {{- if or (eq .Values.connect.backendMode "k8s") (eq .Values.connect.backendMode "standalone") }}
    - name: driver
      port: 7078
      targetPort: 7078
    - name: blockmanager
      port: 7079
      targetPort: 7079
    {{- end }}
    {{- if .Values.connect.metrics.enabled }}
    - name: metrics
      port: {{ .Values.connect.service.metricsPort }}
      targetPort: {{ .Values.connect.metrics.port }}
    {{- end }}
  selector:
    app: spark-connect
{{- end }}
