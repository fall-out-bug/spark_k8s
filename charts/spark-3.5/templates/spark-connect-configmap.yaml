{{- if .Values.connect.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "spark-3.5.fullname" . }}-connect-config
  labels:
    {{- include "spark-3.5.labels" . | nindent 4 }}
data:
  spark-properties.conf.template: |-
    {{- if eq .Values.connect.backendMode "local" }}
    # Spark Connect Server runs in local mode
    # Clients connect via Spark Connect, driver and executors in same JVM
    spark.master=local[*]
    spark.driver.bindAddress=0.0.0.0
    spark.driver.host={{ default (printf "%s-connect" (include "spark-3.5.fullname" .)) .Values.connect.driver.host }}
    {{- else if eq .Values.connect.backendMode "k8s" }}
    # Spark Connect Server runs on K8s
    # Driver in pod, executors on K8s cluster
    spark.master=k8s://https://kubernetes.default.svc:443
    spark.driver.bindAddress=0.0.0.0
    spark.driver.host=$(POD_IP)
    {{- else if eq .Values.connect.backendMode "standalone" }}
    # Spark Connect Server runs in standalone mode
    # Driver connects to standalone Spark master
    spark.master=spark://{{ include "spark-3.5.fullname" . }}-standalone-master:7077
    spark.driver.bindAddress=0.0.0.0
    spark.driver.host={{ default (printf "%s-connect" (include "spark-3.5.fullname" .)) .Values.connect.driver.host }}
    {{- end }}
    spark.driver.port={{ .Values.connect.driver.port }}
    spark.driver.blockManager.port={{ .Values.connect.driver.blockManagerPort }}

    {{- if ne .Values.connect.backendMode "standalone" }}
    # K8s executor config (for clients launching executors on K8s)
    spark.kubernetes.namespace={{ .Release.Namespace }}
    spark.kubernetes.executor.podTemplateFile=/tmp/spark-conf/executor-pod-template.yaml
    spark.kubernetes.executor.request.cores={{ .Values.connect.executor.cores }}
    spark.kubernetes.executor.limit.cores={{ .Values.connect.executor.coresLimit }}
    spark.kubernetes.executor.request.memory={{ .Values.connect.executor.memory }}
    spark.kubernetes.container.image={{ .Values.connect.image.repository }}:{{ .Values.connect.image.tag }}

    # Dynamic allocation
    spark.dynamicAllocation.enabled={{ .Values.connect.dynamicAllocation.enabled }}
    spark.dynamicAllocation.shuffleTracking.enabled=true
    spark.dynamicAllocation.minExecutors={{ .Values.connect.dynamicAllocation.minExecutors }}
    spark.dynamicAllocation.maxExecutors={{ .Values.connect.dynamicAllocation.maxExecutors }}
    {{- end }}
    spark.connect.grpc.binding.address=0.0.0.0
    spark.connect.grpc.binding.port=15002

    # Prometheus metrics
    {{- if .Values.connect.metrics.enabled }}
    spark.ui.prometheus.enabled=true
    spark.metrics.conf=*
    {
      "sink": {
        "prometheus": {
          "class": "org.apache.spark.metrics.sink.PrometheusServlet",
          "path": "metrics/prometheus"
        }
      }
    }
    {{- end }}

    # Ivy cache (for Maven dependencies)
    spark.jars.ivy=/tmp/spark-ivy-cache

    # Disable push-based shuffle (may conflict with PSS restricted)
    spark.shuffle.push.enabled=false
    spark.shuffle.sort.bypassMergeThreshold=0

    # Event log (for History Server)
    {{- if .Values.connect.eventLog.enabled }}
    spark.eventLog.enabled=true
    spark.eventLog.dir={{ .Values.connect.eventLog.dir }}
    {{- else }}
    spark.eventLog.enabled=false
    {{- end }}

    {{- if .Values.global.s3.enabled }}
    # S3 config
    spark.hadoop.fs.s3a.endpoint={{ .Values.global.s3.endpoint }}
    spark.hadoop.fs.s3a.access.key=${AWS_ACCESS_KEY_ID}
    spark.hadoop.fs.s3a.secret.key=${AWS_SECRET_ACCESS_KEY}
    spark.hadoop.fs.s3a.path.style.access={{ .Values.global.s3.pathStyleAccess }}
    {{- end }}

    {{- if .Values.connect.sparkConf }}
    # Extra Spark conf (user overrides)
    {{- range $key, $value := .Values.connect.sparkConf }}
    {{ $key }}={{ $value }}
    {{- end }}
    {{- end }}

    {{- if .Values.celeborn.enabled }}
    # Celeborn shuffle
    spark.shuffle.manager=org.apache.spark.shuffle.celeborn.RssShuffleManager
    spark.celeborn.master.endpoints={{ .Values.celeborn.masterEndpoints }}
    spark.celeborn.push.replicate.enabled=true
    spark.celeborn.client.spark.shuffle.writer=hash
    spark.celeborn.client.spark.fetch.throwsFetchFailure=true
    {{- end }}
{{- end }}
