# OpenShift preset: PSS restricted + SCC restricted
#
# This preset configures Spark 3.5 for OpenShift with:
# - Pod Security Standards (PSS) restricted profile
# - Security Context Constraints (SCC) restricted
# - UID 185 (matches default spark-k8s image)
# - External S3/PostgreSQL (no embedded MinIO/PostgreSQL)
#
# Usage:
#   helm install my-spark charts/spark-3.5 -f charts/spark-3.5/presets/openshift/restricted.yaml
#
# Requirements:
# - OpenShift cluster with SCC restricted available
# - SCC must allow UID 185 (or build image with --build-arg SPARK_UID=1000000000)
# - External S3-compatible storage
# - External PostgreSQL for Hive Metastore

# Global settings
global:
  s3:
    enabled: true
    endpoint: "https://s3.openshift.example.com"
    accessKey: ""  # Use existingSecret
    secretKey: ""
    existingSecret: "s3-credentials"
    sslEnabled: true

# Security settings for OpenShift
security:
  podSecurityStandards: true
  createNamespace: true
  pssProfile: restricted
  runAsUser: 185  # Matches spark-k8s image default (or build with SPARK_UID=1000000000 for OpenShift range)
  runAsGroup: 185
  fsGroup: 185
  readOnlyRootFilesystem: false

# RBAC
rbac:
  create: true
  serviceAccountName: "spark-35-openshift"

# Spark Connect Server
connect:
  enabled: true
  replicas: 1
  image:
    repository: spark-custom
    tag: "3.5.x"
    pullPolicy: IfNotPresent
  resources:
    requests:
      memory: "2Gi"
      cpu: "1"
    limits:
      memory: "4Gi"
      cpu: "2"
  backendMode: k8s
  driver:
    host: ""
    port: 7078
    blockManagerPort: 7079
  executor:
    cores: "1"
    coresLimit: "2"
    memory: "1Gi"
    memoryLimit: "2Gi"
  dynamicAllocation:
    enabled: true
    minExecutors: 0
    maxExecutors: 10
  eventLog:
    enabled: true
    dir: "s3a://spark-logs/events"

# Hive Metastore
hiveMetastore:
  enabled: true
  image:
    repository: apache/hive
    tag: "3.1.3"
    pullPolicy: IfNotPresent
  service:
    port: 9083
  warehouseDir: "s3a://warehouse/spark-35"
  metastoreWarehouseDir: "file:/tmp/hive-warehouse"
  database:
    name: "metastore_spark35"
  postgresql:
    enabled: false  # Use external PostgreSQL
    host: ""  # Set to external PostgreSQL host
    port: 5432
    database: "metastore_spark35"
    username: ""
    password: ""
    persistence:
      enabled: false
  resources:
    requests:
      memory: "512Mi"
      cpu: "200m"
    limits:
      memory: "2Gi"
      cpu: "1000m"

# History Server
historyServer:
  enabled: true
  image:
    repository: spark-custom
    tag: "3.5.x"
    pullPolicy: IfNotPresent
  logDirectory: "s3a://spark-logs/events"
  service:
    type: ClusterIP
    port: 18080
  resources:
    requests:
      memory: "512Mi"
      cpu: "200m"
    limits:
      memory: "2Gi"
      cpu: "1000m"

# Core infrastructure (disabled - use external)
core:
  minio:
    enabled: false  # Use external S3
  postgresql:
    enabled: false  # Use external
  hiveMetastore:
    enabled: false  # Use external

# Disable other components
sparkStandalone:
  enabled: false

jupyter:
  enabled: false

# OpenShift Routes (for external access)
routes:
  enabled: true
  tlsTerminationType: "edge"
  wildcardPolicy: "None"
  hosts:
    historyServer: ""
    jupyter: ""
    sparkConnect: ""

# Monitoring (Prometheus Operator + Grafana)
monitoring:
  serviceMonitor:
    enabled: true
    interval: "30s"
    scrapeTimeout: "10s"
  podMonitor:
    enabled: true
    interval: "30s"
    scrapeTimeout: "10s"
  grafanaDashboards:
    enabled: true
    namespace: ""

ingress:
  enabled: false

celeborn:
  enabled: false

sparkOperator:
  enabled: false
