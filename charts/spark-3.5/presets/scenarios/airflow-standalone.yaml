# Scenario Preset: Airflow + Spark Standalone (Direct)
#
# This preset deploys Spark Standalone cluster for direct spark-submit access from Airflow.
# No Spark Connect intermediary - Airflow submits jobs directly to Standalone master.
#
# Architecture:
#   Airflow --spark-submit--> Standalone Master:7077 --> Workers
#
# Use Cases:
# - Data engineers who want pipeline orchestration with Airflow
# - Production ETL workflows with fixed executor allocation
# - Scheduled Spark jobs via Airflow DAGs
# - Direct spark-submit without Connect overhead
#
# Components Enabled:
# - Spark Standalone Master (port 7077)
# - Spark Standalone Workers (1 replica by default)
#
# Prerequisites:
# - Airflow must be deployed separately
# - Airflow SparkHook or SparkSubmitOperator connects to: spark://<release>-standalone-master:7077
#
# Usage:
#   helm install scenario2 charts/spark-3.5 -n spark-35-airflow-sa \
#     -f charts/spark-3.5/presets/scenarios/airflow-standalone.yaml
#
# Airflow DAG Example (SparkSubmitOperator):
#   from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
#   SparkSubmitOperator(
#       task_id='spark_job',
#       application='hdfs://path/to/job.py',
#       conn_id='spark_default',
#       conf={
#           'spark.master': 'spark://scenario2-spark-35-standalone-master:7077',
#           'spark.executor.memory': '1g',
#           'spark.executor.cores': '1',
#       }
#   )
#
# Airflow Connection Setup:
#   Conn Id: spark_default
#   Conn Type: Spark
#   Host: scenario2-spark-35-standalone-master
#   Port: 7077
#   Extra: {"queue": "root.default"}

# Core Infrastructure (minimal - Airflow manages storage)
spark-base:
  enabled: true
  minio:
    enabled: false
  postgresql:
    enabled: false

# RBAC for Spark pods (shared with Airflow)
rbac:
  create: true
  serviceAccountName: "spark"

# Spark Standalone Cluster (direct access)
sparkStandalone:
  enabled: true
  serviceAccountName: "spark"
  image:
    repository: spark-custom
    tag: "3.5.x"
    pullPolicy: IfNotPresent
  master:
    enabled: true
    service:
      type: ClusterIP
      rpcPort: 7077
      uiPort: 8080
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1"
  worker:
    enabled: true
    replicas: 2
    resources:
      requests:
        memory: "2Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "2"
    sparkConf:
      spark.worker.memory: "2g"
      spark.worker.cores: "2"

# Disable Spark Connect (not needed for direct spark-submit)
connect:
  enabled: false

# Disable Jupyter (not needed for Airflow scenario)
jupyter:
  enabled: false

# Disable other components
hiveMetastore:
  enabled: false

historyServer:
  enabled: false

ingress:
  enabled: false

security:
  podSecurityStandards: false
