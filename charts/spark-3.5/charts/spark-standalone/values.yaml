# Spark Standalone Cluster Configuration
# For batch ETL workloads with Airflow orchestration

# Spark Master Configuration
master:
  enabled: true
  image:
    repository: spark-custom
    tag: "3.5.7"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    rpcPort: 7077
    uiPort: 8080
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1"
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Spark Master configuration options
  sparkConf: {}
  # spark.master.ui.port: "8080"
  # spark.master.rest.enabled: "true"

# Spark Worker Configuration
worker:
  enabled: true
  replicas: 2
  image:
    repository: spark-custom
    tag: "3.5.7"
    pullPolicy: IfNotPresent
  resources:
    requests:
      memory: "2Gi"
      cpu: "500m"
    limits:
      memory: "4Gi"
      cpu: "2"
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # Spark Worker configuration options
  sparkConf:
    spark.worker.memory: "2g"
    spark.worker.cores: "2"
  # Worker directories
  workDir: "/tmp/spark-worker"

# Airflow Orchestration
airflow:
  enabled: true
  image:
    repository: spark-k8s/airflow
    tag: "2.11.0"
    pullPolicy: IfNotPresent

  # Airflow Webserver
  webserver:
    replicas: 1
    service:
      type: ClusterIP
      port: 8080
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1"
    nodeSelector: {}
    tolerations: []

  # Airflow Scheduler
  scheduler:
    replicas: 1
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "500m"
    nodeSelector: {}
    tolerations: []

  # Airflow PostgreSQL Database
  postgresql:
    enabled: true
    image:
      repository: postgres
      tag: "15-alpine"
      pullPolicy: IfNotPresent
    auth:
      database: "airflow"
      username: "airflow"
      password: "airflow123"
    service:
      port: 5432
    persistence:
      enabled: true
      size: "5Gi"
      storageClass: ""
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"

  # Airflow Configuration
  config:
    core:
      dags_folder: "/opt/airflow/dags"
      load_examples: "False"
      executor: "LocalExecutor"
    webserver:
      expose_config: "True"
      rbac: "True"
    scheduler:
      catchup_by_default: "False"

  # DAG files to include
  dags:
    - spark_etl_example.py
    - spark_streaming_example.py

  # Airflow Variables (set at startup)
  variables:
    spark_image: "spark-custom:3.5.7"
    spark_namespace: "spark"
    s3_endpoint: "http://minio:9000"
    s3_access_key: "minioadmin"
    s3_secret_key: "minioadmin"

  # Connections (set at startup)
  connections: []
  # - conn_id: spark_k8s
  #   conn_type: spark
  #   host: k8s://https://kubernetes.default.svc

# Service Account
serviceAccount:
  create: true
  name: "spark-standalone"

# RBAC
rbac:
  create: true

# Security
security:
  podSecurityStandards: false
  runAsUser: 185
  runAsGroup: 185
  fsGroup: 185
