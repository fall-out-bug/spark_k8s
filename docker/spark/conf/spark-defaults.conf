# Spark defaults configuration
# Optimized for Kubernetes/OpenShift with S3, Hive Metastore

# ============================================
# Local/Standalone Configuration (for Docker Compose)
# For K8s deployment, set via environment or command line
# ============================================
spark.master                                    local[*]

# ============================================
# S3A Configuration for MinIO (AWS SDK v2)
# Hadoop 3.4.1 with AWS SDK v2
# ============================================
spark.hadoop.fs.s3a.impl                        org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint                    http://minio:9000
spark.hadoop.fs.s3a.access.key                  minioadmin
spark.hadoop.fs.s3a.secret.key                  minioadmin
spark.hadoop.fs.s3a.path.style.access           true
spark.hadoop.fs.s3a.connection.ssl.enabled      false

# AWS SDK v2 specific settings
spark.hadoop.fs.s3a.aws.credentials.provider    org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

# Performance tuning
spark.hadoop.fs.s3a.fast.upload                 true
spark.hadoop.fs.s3a.fast.upload.buffer          bytebuffer
spark.hadoop.fs.s3a.multipart.size              104857600
spark.hadoop.fs.s3a.multipart.threshold         104857600
spark.hadoop.fs.s3a.socket.send.buffer          65536
spark.hadoop.fs.s3a.socket.recv.buffer          65536
spark.hadoop.fs.s3a.threads.max                 64
spark.hadoop.fs.s3a.connection.maximum          100
spark.hadoop.fs.s3a.attempts.maximum            10
spark.hadoop.fs.s3a.retry.limit                 5

# S3A committer for better write performance
spark.hadoop.fs.s3a.committer.name              magic
spark.hadoop.fs.s3a.committer.magic.enabled     true
spark.sql.sources.commitProtocolClass           org.apache.spark.internal.io.cloud.PathOutputCommitProtocol
spark.sql.parquet.output.committer.class        org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter

# ============================================
# Hive Metastore Configuration
# ============================================
spark.sql.catalogImplementation                 hive
spark.sql.warehouse.dir                         s3a://warehouse/spark-warehouse
spark.hadoop.hive.metastore.warehouse.dir       s3a://warehouse/spark-warehouse

# Hive Metastore Thrift service (standalone metastore)
# spark.hive.metastore.uris                     thrift://hive-metastore:9083

# For embedded metastore with PostgreSQL backend
spark.hadoop.javax.jdo.option.ConnectionURL          jdbc:postgresql://postgres:5432/metastore
spark.hadoop.javax.jdo.option.ConnectionDriverName   org.postgresql.Driver
spark.hadoop.javax.jdo.option.ConnectionUserName     spark
spark.hadoop.javax.jdo.option.ConnectionPassword     spark

# Metastore schema initialization
spark.hadoop.datanucleus.autoCreateSchema       true
spark.hadoop.datanucleus.fixedDatastore         false
spark.hadoop.datanucleus.schema.autoCreateAll   true

# ============================================
# Spark Connect Server
# ============================================
spark.connect.grpc.binding.port                 15002
spark.connect.grpc.arrow.maxBatchSize           4194304
spark.connect.grpc.maxInboundMessageSize        134217728

# ============================================
# Session Idle Timeout (auto-kill inactive sessions)
# ============================================
# Kill sessions idle for more than 30 minutes
spark.connect.session.timeout                   30m
spark.connect.execute.reattachable              true

# Spark SQL session timeout
spark.sql.session.timeout                       1800000

# ============================================
# Spark UI and History Server
# ============================================
spark.eventLog.enabled                          true
spark.eventLog.dir                              /tmp/spark-events
spark.eventLog.compress                         true
spark.history.fs.logDirectory                   /tmp/spark-events
spark.history.ui.port                           18080

spark.ui.enabled                                true
spark.ui.port                                   4040
spark.ui.reverseProxy                           true
spark.ui.reverseProxyUrl                        /spark-ui

# ============================================
# Memory and Executor Configuration
# ============================================
spark.driver.memory                             2g
spark.driver.cores                              1
spark.executor.memory                           4g
spark.executor.cores                            2
spark.executor.instances                        2

# Dynamic allocation (disabled in local mode, enable for K8s)
spark.dynamicAllocation.enabled                 false
# spark.dynamicAllocation.shuffleTracking.enabled true
# spark.dynamicAllocation.minExecutors            0
# spark.dynamicAllocation.maxExecutors            10
# spark.dynamicAllocation.initialExecutors        1
# spark.dynamicAllocation.executorIdleTimeout     60s
# spark.dynamicAllocation.schedulerBacklogTimeout 1s

# ============================================
# Serialization and Performance
# ============================================
spark.serializer                                org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max                 1024m
spark.sql.shuffle.partitions                    200
spark.sql.adaptive.enabled                      true
spark.sql.adaptive.coalescePartitions.enabled   true
spark.sql.adaptive.skewJoin.enabled             true

# Arrow for pandas/PySpark integration
spark.sql.execution.arrow.pyspark.enabled       true
spark.sql.execution.arrow.pyspark.fallback.enabled true

# ============================================
# Network Configuration
# ============================================
spark.network.timeout                           300s
spark.executor.heartbeatInterval                30s
spark.rpc.askTimeout                            300s

# ============================================
# Security (for OpenShift / K8s)
# These settings are ignored in local mode
# ============================================
# spark.kubernetes.executor.podNamePrefix         spark-exec
# spark.kubernetes.driver.podNamePrefix           spark-driver
# spark.kubernetes.authenticate.driver.serviceAccountName spark
# spark.kubernetes.authenticate.executor.serviceAccountName spark
