{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming + Kafka\n",
    "Real-time data processing with Spark and Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom spark_config import get_spark_session\n\n# Note: Spark Streaming requires local or cluster mode, not Spark Connect\nspark = get_spark_session(\n    app_name=\"SparkStreamingDemo\",\n    local_mode=True,  # Use local mode for streaming\n    extra_configs={\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.7\"\n    }\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = os.environ.get('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')\n",
    "INPUT_TOPIC = 'events'\n",
    "OUTPUT_TOPIC = 'processed-events'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read streaming data from Kafka\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"subscribe\", INPUT_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON events\n",
    "from pyspark.sql.functions import from_json, col, window, count, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Define schema for incoming events\n",
    "event_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Parse JSON\n",
    "events_df = kafka_df \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"event\")) \\\n",
    "    .select(\"event.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowed Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowed aggregation - count events per type in 1-minute windows\n",
    "windowed_counts = events_df \\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"1 minute\", \"30 seconds\"),\n",
    "        col(\"event_type\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"event_count\"),\n",
    "        avg(\"value\").alias(\"avg_value\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Console (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to console for debugging\n",
    "# query = windowed_counts \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"update\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"truncate\", \"false\") \\\n",
    "#     .start()\n",
    "# \n",
    "# query.awaitTermination(60)  # Run for 60 seconds\n",
    "# query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "# Prepare output for Kafka\n",
    "output_df = windowed_counts \\\n",
    "    .select(\n",
    "        col(\"event_type\").alias(\"key\"),\n",
    "        to_json(struct(\n",
    "            col(\"window.start\").alias(\"window_start\"),\n",
    "            col(\"window.end\").alias(\"window_end\"),\n",
    "            col(\"event_type\"),\n",
    "            col(\"event_count\"),\n",
    "            col(\"avg_value\")\n",
    "        )).alias(\"value\")\n",
    "    )\n",
    "\n",
    "# Write to Kafka\n",
    "# query = output_df \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "#     .option(\"topic\", OUTPUT_TOPIC) \\\n",
    "#     .option(\"checkpointLocation\", \"s3a://checkpoints/streaming/windowed-counts\") \\\n",
    "#     .outputMode(\"update\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to S3 (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write raw events to S3 as Parquet\n",
    "# query = events_df \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"parquet\") \\\n",
    "#     .option(\"path\", \"s3a://streaming-output/events\") \\\n",
    "#     .option(\"checkpointLocation\", \"s3a://checkpoints/streaming/events\") \\\n",
    "#     .partitionBy(\"event_type\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .trigger(processingTime=\"1 minute\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to produce test events to Kafka\n",
    "def produce_test_events(bootstrap_servers, topic, n_events=100):\n",
    "    \"\"\"Produce test events to Kafka topic.\"\"\"\n",
    "    from kafka import KafkaProducer\n",
    "    import json\n",
    "    import random\n",
    "    from datetime import datetime\n",
    "    import uuid\n",
    "    \n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    \n",
    "    event_types = ['click', 'view', 'purchase', 'signup']\n",
    "    \n",
    "    for i in range(n_events):\n",
    "        event = {\n",
    "            'event_id': str(uuid.uuid4()),\n",
    "            'event_type': random.choice(event_types),\n",
    "            'user_id': f'user_{random.randint(1, 100)}',\n",
    "            'value': random.uniform(1, 1000),\n",
    "            'timestamp': datetime.utcnow().isoformat()\n",
    "        }\n",
    "        producer.send(topic, value=event)\n",
    "    \n",
    "    producer.flush()\n",
    "    print(f\"Produced {n_events} events to {topic}\")\n",
    "\n",
    "# Uncomment to produce test events\n",
    "# produce_test_events(KAFKA_BOOTSTRAP_SERVERS, INPUT_TOPIC, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}