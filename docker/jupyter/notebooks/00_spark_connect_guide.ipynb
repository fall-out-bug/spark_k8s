{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Connect: Полное руководство\n",
    "\n",
    "## Что такое Spark Connect?\n",
    "\n",
    "**Spark Connect** — это новый клиент-серверный протокол в Apache Spark 3.4+, который позволяет:\n",
    "\n",
    "- Подключаться к удалённому Spark кластеру через легковесный клиент\n",
    "- Не запускать JVM на клиентской машине\n",
    "- Использовать один Spark кластер для множества пользователей\n",
    "- Автоматически управлять ресурсами и сессиями\n",
    "\n",
    "### Архитектура\n",
    "\n",
    "```\n",
    "┌─────────────────┐     gRPC      ┌─────────────────────────────┐\n",
    "│   JupyterLab    │ ────────────► │   Spark Connect Server      │\n",
    "│   (Python)      │               │   (Driver + Executors)      │\n",
    "│                 │ ◄──────────── │                             │\n",
    "│   pyspark       │   Arrow       │   ┌─────┐ ┌─────┐ ┌─────┐  │\n",
    "│   (lightweight) │               │   │Exec │ │Exec │ │Exec │  │\n",
    "└─────────────────┘               │   └─────┘ └─────┘ └─────┘  │\n",
    "                                  └─────────────────────────────┘\n",
    "                                              │\n",
    "                                              ▼\n",
    "                                  ┌─────────────────────────────┐\n",
    "                                  │   S3 / Hive Metastore       │\n",
    "                                  └─────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Преимущества перед Livy/sparkmagic:\n",
    "\n",
    "| Критерий | Livy | Spark Connect |\n",
    "|----------|------|---------------|\n",
    "| Протокол | HTTP REST | gRPC (бинарный) |\n",
    "| Скорость | Медленнее | Быстрее |\n",
    "| Поддержка | Устарел | Официальный |\n",
    "| Arrow | Нет | Да |\n",
    "| pandas API | Нет | Да |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Подключение к Spark Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pyspark.sql import SparkSession\n\n# Способ 1: Через переменную окружения SPARK_REMOTE (рекомендуется)\n# Переменная уже установлена: SPARK_REMOTE=sc://spark-connect:15002\n\nspark = SparkSession.builder \\\n    .appName(\"MySparkApp\") \\\n    .getOrCreate()\n\nprint(f\"Spark version: {spark.version}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Способ 2: Явное указание remote URL\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"MySparkApp\") \\\n",
    "#     .remote(\"sc://spark-connect:15002\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Способ 3: Используя хелпер из spark_config\n",
    "from spark_config import get_spark_session\n",
    "\n",
    "# spark = get_spark_session(app_name=\"MyApp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Базовые операции с DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataFrame из Python данных\n",
    "data = [\n",
    "    (\"Alice\", \"Engineering\", 75000),\n",
    "    (\"Bob\", \"Engineering\", 80000),\n",
    "    (\"Charlie\", \"Sales\", 60000),\n",
    "    (\"Diana\", \"Sales\", 65000),\n",
    "    (\"Eve\", \"HR\", 55000),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"department\", \"salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Схема DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтрация и группировка\n",
    "from pyspark.sql.functions import avg, count, max, min\n",
    "\n",
    "dept_stats = df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employees\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\")\n",
    ")\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. SQL запросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Регистрируем DataFrame как временную таблицу\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# SQL запрос\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT department, \n",
    "           AVG(salary) as avg_salary,\n",
    "           COUNT(*) as count\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    HAVING AVG(salary) > 60000\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Работа с S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запись в S3 (Parquet)\n",
    "df.write.mode(\"overwrite\").parquet(\"s3a://warehouse/demo/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение из S3\n",
    "df_from_s3 = spark.read.parquet(\"s3a://warehouse/demo/employees\")\n",
    "df_from_s3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Другие форматы\n",
    "# CSV\n",
    "# df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"s3a://warehouse/demo/employees_csv\")\n",
    "\n",
    "# JSON\n",
    "# df.write.mode(\"overwrite\").json(\"s3a://warehouse/demo/employees_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. pandas API on Spark (бывший Koalas)\n",
    "\n",
    "Используйте привычный pandas-синтаксис для работы с большими данными!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "# Создание pandas-on-Spark DataFrame\n",
    "psdf = ps.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'department': ['Eng', 'Eng', 'Sales', 'Sales', 'HR'],\n",
    "    'salary': [75000, 80000, 60000, 65000, 55000],\n",
    "    'years': [3, 5, 2, 4, 1]\n",
    "})\n",
    "\n",
    "psdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-like операции работают на кластере!\n",
    "\n",
    "# Фильтрация\n",
    "high_earners = psdf[psdf['salary'] > 65000]\n",
    "print(\"High earners:\")\n",
    "print(high_earners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy\n",
    "dept_summary = psdf.groupby('department').agg({\n",
    "    'salary': ['mean', 'sum'],\n",
    "    'years': 'mean'\n",
    "})\n",
    "print(dept_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение файлов с pandas API\n",
    "# psdf = ps.read_parquet(\"s3a://warehouse/demo/employees\")\n",
    "# psdf = ps.read_csv(\"s3a://raw-data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конвертация между типами\n",
    "\n",
    "# pandas-on-Spark → Spark DataFrame\n",
    "spark_df = psdf.to_spark()\n",
    "\n",
    "# Spark DataFrame → pandas-on-Spark\n",
    "psdf_back = spark_df.pandas_api()\n",
    "\n",
    "# pandas-on-Spark → pandas (осторожно с большими данными!)\n",
    "pandas_df = psdf.to_pandas()\n",
    "\n",
    "print(f\"Type: {type(pandas_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Работа с Hive таблицами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание managed таблицы в Hive\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список таблиц\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение таблицы\n",
    "emp_df = spark.table(\"employees\")\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание партиционированной таблицы\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"department\") \\\n",
    "    .saveAsTable(\"employees_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описание таблицы\n",
    "spark.sql(\"DESCRIBE EXTENDED employees_partitioned\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Интеграция с pandas через Arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Создание pandas DataFrame\n",
    "pandas_data = pd.DataFrame({\n",
    "    'id': range(1000),\n",
    "    'value': [i * 2 for i in range(1000)]\n",
    "})\n",
    "\n",
    "# Быстрая конвертация через Arrow\n",
    "spark_df = spark.createDataFrame(pandas_data)\n",
    "print(f\"Rows: {spark_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обратная конвертация (также через Arrow)\n",
    "result_pandas = spark_df.filter(\"value > 500\").toPandas()\n",
    "print(f\"Filtered rows: {len(result_pandas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Полезные советы\n",
    "\n",
    "### Отладка\n",
    "```python\n",
    "# Посмотреть план выполнения\n",
    "df.explain(True)\n",
    "\n",
    "# Кэширование для повторных операций\n",
    "df.cache()\n",
    "df.unpersist()  # освободить\n",
    "```\n",
    "\n",
    "### Оптимизация\n",
    "```python\n",
    "# Партиционирование при записи\n",
    "df.repartition(10).write.parquet(\"...\")\n",
    "\n",
    "# Coalesce для уменьшения партиций (без shuffle)\n",
    "df.coalesce(1).write.parquet(\"...\")\n",
    "```\n",
    "\n",
    "### Таймауты сессий\n",
    "- Неактивные сессии автоматически завершаются через **30 минут**\n",
    "- При потере соединения можно переподключиться к существующей сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Информация о сессии\nprint(f\"Spark version: {spark.version}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Завершение работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Явное завершение сессии (опционально - сессия автоматически завершится по таймауту)\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ссылки\n",
    "\n",
    "- [Spark Connect Documentation](https://spark.apache.org/docs/latest/spark-connect-overview.html)\n",
    "- [pandas API on Spark](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html)\n",
    "- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}