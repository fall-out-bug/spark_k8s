{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# NYC Yellow Taxi Data Analysis\n\nАнализ данных NYC Yellow Taxi 2025 года с использованием Spark Connect.\n\n**Данные:** ~653 MB, 10 месяцев (январь-октябрь 2025), ~40 млн поездок  \n**Источник:** [NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n\n---\n\n## Web UI\n\n### Docker Compose\n| Сервис | URL | Описание |\n|--------|-----|----------|\n| **Jupyter** | http://localhost:8888 | This notebook |\n| **Spark UI** | http://localhost:4040 | Jobs, Stages, Storage, SQL |\n| **Spark History** | http://localhost:18080 | История завершенных приложений |\n| **MinIO Console** | http://localhost:9001 | S3 bucket browser (minioadmin/minioadmin) |\n\n### Kubernetes\n| Сервис | Команда / URL | Описание |\n|--------|---------------|----------|\n| **Jupyter** | http://localhost:30888 | NodePort |\n| **Spark UI** | `kubectl port-forward -n spark svc/spark-connect 4040:4040` | Затем http://localhost:4040 |\n| **Spark History** | `kubectl port-forward -n spark svc/spark-history-server 18080:18080` | Затем http://localhost:18080 |\n| **MinIO Console** | `kubectl port-forward -n spark svc/minio 9001:9001` | Затем http://localhost:9001 |\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "# Setup - прямое подключение к Spark Connect\nimport os\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n# Подключение к Spark Connect серверу\nspark = SparkSession.builder \\\n    .appName(\"NYCTaxiAnalysis\") \\\n    .remote(os.environ.get('SPARK_REMOTE', 'sc://spark-connect:15002')) \\\n    .getOrCreate()\n\nprint(f\"Spark version: {spark.version}\")\nprint(f\"\\nSpark UI:\")\nprint(f\"  Docker Compose: http://localhost:4040\")\nprint(f\"  K8s: kubectl port-forward -n spark svc/spark-connect 4040:4040\")"
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## 1. Загрузка данных из S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение всех parquet файлов\n",
    "df = spark.read.parquet(\"s3a://raw-data/nyc-taxi/*.parquet\")\n",
    "\n",
    "# Кэширование для повторного использования\n",
    "df.cache()\n",
    "\n",
    "# Общее количество записей\n",
    "total_count = df.count()\n",
    "print(f\"Всего записей: {total_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Схема данных\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры данных\n",
    "df.select(\n",
    "    \"tpep_pickup_datetime\", \n",
    "    \"tpep_dropoff_datetime\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\", \n",
    "    \"fare_amount\", \n",
    "    \"tip_amount\", \n",
    "    \"total_amount\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-header",
   "metadata": {},
   "source": [
    "## 2. Базовая статистика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Общая статистика\n",
    "stats = df.select(\n",
    "    F.count(\"*\").alias(\"total_trips\"),\n",
    "    F.round(F.avg(\"trip_distance\"), 2).alias(\"avg_distance_miles\"),\n",
    "    F.round(F.avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "    F.round(F.avg(\"tip_amount\"), 2).alias(\"avg_tip\"),\n",
    "    F.round(F.avg(\"total_amount\"), 2).alias(\"avg_total\"),\n",
    "    F.round(F.sum(\"total_amount\") / 1_000_000, 2).alias(\"total_revenue_millions\")\n",
    ")\n",
    "\n",
    "stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "describe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Статистика по числовым колонкам\n",
    "df.select(\"trip_distance\", \"fare_amount\", \"tip_amount\", \"total_amount\") \\\n",
    "    .summary(\"count\", \"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-header",
   "metadata": {},
   "source": [
    "## 3. Анализ по месяцам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-agg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Агрегация по месяцам\n",
    "monthly = df.withColumn(\"month\", F.month(\"tpep_pickup_datetime\")) \\\n",
    "    .groupBy(\"month\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trips\"),\n",
    "        F.round(F.sum(\"total_amount\") / 1_000_000, 2).alias(\"revenue_M\"),\n",
    "        F.round(F.avg(\"trip_distance\"), 2).alias(\"avg_distance\"),\n",
    "        F.round(F.avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "        F.round(F.avg(\"tip_amount\"), 2).alias(\"avg_tip\")\n",
    "    ) \\\n",
    "    .orderBy(\"month\")\n",
    "\n",
    "monthly.show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "monthly_pd = monthly.toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Поездки по месяцам\n",
    "axes[0].bar(monthly_pd['month'], monthly_pd['trips'] / 1_000_000, color='steelblue')\n",
    "axes[0].set_xlabel('Месяц')\n",
    "axes[0].set_ylabel('Поездки (млн)')\n",
    "axes[0].set_title('NYC Yellow Taxi: Поездки по месяцам (2025)')\n",
    "axes[0].set_xticks(range(1, 11))\n",
    "\n",
    "# Выручка по месяцам\n",
    "axes[1].bar(monthly_pd['month'], monthly_pd['revenue_M'], color='green')\n",
    "axes[1].set_xlabel('Месяц')\n",
    "axes[1].set_ylabel('Выручка ($ млн)')\n",
    "axes[1].set_title('NYC Yellow Taxi: Выручка по месяцам (2025)')\n",
    "axes[1].set_xticks(range(1, 11))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-header",
   "metadata": {},
   "source": [
    "## 4. Анализ по часам дня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поездки по часам\n",
    "hourly = df.withColumn(\"hour\", F.hour(\"tpep_pickup_datetime\")) \\\n",
    "    .groupBy(\"hour\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trips\"),\n",
    "        F.round(F.avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "        F.round(F.avg(\"trip_distance\"), 2).alias(\"avg_distance\")\n",
    "    ) \\\n",
    "    .orderBy(\"hour\")\n",
    "\n",
    "hourly_pd = hourly.toPandas()\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(hourly_pd['hour'], hourly_pd['trips'] / 1_000_000, color='coral')\n",
    "plt.xlabel('Час дня')\n",
    "plt.ylabel('Поездки (млн)')\n",
    "plt.title('NYC Yellow Taxi: Распределение по часам дня')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.axvspan(7, 9, alpha=0.3, color='red', label='Утренний час пик')\n",
    "plt.axvspan(17, 19, alpha=0.3, color='red', label='Вечерний час пик')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dow-header",
   "metadata": {},
   "source": [
    "## 5. Анализ по дням недели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# День недели (1=Sunday, 7=Saturday в Spark)\n",
    "dow_names = {1: 'Вс', 2: 'Пн', 3: 'Вт', 4: 'Ср', 5: 'Чт', 6: 'Пт', 7: 'Сб'}\n",
    "\n",
    "by_dow = df.withColumn(\"dow\", F.dayofweek(\"tpep_pickup_datetime\")) \\\n",
    "    .groupBy(\"dow\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trips\"),\n",
    "        F.round(F.avg(\"total_amount\"), 2).alias(\"avg_total\"),\n",
    "        F.round(F.avg(\"tip_amount\"), 2).alias(\"avg_tip\")\n",
    "    ) \\\n",
    "    .orderBy(\"dow\")\n",
    "\n",
    "by_dow.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "payment-header",
   "metadata": {},
   "source": [
    "## 6. Типы оплаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Типы оплаты\n",
    "# 1=Credit card, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided\n",
    "\n",
    "payment = df.groupBy(\"payment_type\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trips\"),\n",
    "        F.round(F.sum(\"total_amount\") / 1_000_000, 2).alias(\"revenue_M\"),\n",
    "        F.round(F.avg(\"tip_amount\"), 2).alias(\"avg_tip\"),\n",
    "        F.round(F.avg(\"tip_amount\") / F.avg(\"fare_amount\") * 100, 1).alias(\"tip_pct\")\n",
    "    ) \\\n",
    "    .withColumn(\"payment_name\", \n",
    "        F.when(F.col(\"payment_type\") == 1, \"Credit Card\")\n",
    "         .when(F.col(\"payment_type\") == 2, \"Cash\")\n",
    "         .when(F.col(\"payment_type\") == 3, \"No Charge\")\n",
    "         .when(F.col(\"payment_type\") == 4, \"Dispute\")\n",
    "         .otherwise(\"Other\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"trips\"))\n",
    "\n",
    "payment.select(\"payment_name\", \"trips\", \"revenue_M\", \"avg_tip\", \"tip_pct\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "locations-header",
   "metadata": {},
   "source": [
    "## 7. Топ локаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Топ 10 точек посадки\n",
    "top_pickups = df.groupBy(\"PULocationID\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trips\"),\n",
    "        F.round(F.sum(\"total_amount\"), 0).alias(\"total_revenue\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"trips\")) \\\n",
    "    .limit(10)\n",
    "\n",
    "print(\"Топ 10 точек посадки (PULocationID):\")\n",
    "top_pickups.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-routes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Топ маршрутов\n",
    "top_routes = df.groupBy(\"PULocationID\", \"DOLocationID\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trips\"),\n",
    "        F.round(F.avg(\"trip_distance\"), 2).alias(\"avg_dist\"),\n",
    "        F.round(F.avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "        F.round(F.avg(\"total_amount\"), 2).alias(\"avg_total\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"trips\")) \\\n",
    "    .limit(15)\n",
    "\n",
    "print(\"Топ 15 маршрутов (PU -> DO):\")\n",
    "top_routes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sql-header",
   "metadata": {},
   "source": [
    "## 8. SQL аналитика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sql-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Регистрация временной таблицы\n",
    "df.createOrReplaceTempView(\"taxi_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sql-time-periods",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ по периодам дня\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN hour(tpep_pickup_datetime) BETWEEN 6 AND 9 THEN '1. Утренний час пик'\n",
    "        WHEN hour(tpep_pickup_datetime) BETWEEN 10 AND 15 THEN '2. День'\n",
    "        WHEN hour(tpep_pickup_datetime) BETWEEN 16 AND 19 THEN '3. Вечерний час пик'\n",
    "        WHEN hour(tpep_pickup_datetime) BETWEEN 20 AND 23 THEN '4. Вечер'\n",
    "        ELSE '5. Ночь'\n",
    "    END as time_period,\n",
    "    COUNT(*) as trips,\n",
    "    ROUND(AVG(trip_distance), 2) as avg_distance,\n",
    "    ROUND(AVG(fare_amount), 2) as avg_fare,\n",
    "    ROUND(AVG(tip_amount), 2) as avg_tip,\n",
    "    ROUND(SUM(total_amount) / 1000000, 2) as revenue_M\n",
    "FROM taxi_trips\n",
    "WHERE fare_amount > 0\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sql-distance-buckets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распределение по дистанции\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN trip_distance < 1 THEN '< 1 mile'\n",
    "        WHEN trip_distance < 3 THEN '1-3 miles'\n",
    "        WHEN trip_distance < 5 THEN '3-5 miles'\n",
    "        WHEN trip_distance < 10 THEN '5-10 miles'\n",
    "        WHEN trip_distance < 20 THEN '10-20 miles'\n",
    "        ELSE '20+ miles'\n",
    "    END as distance_bucket,\n",
    "    COUNT(*) as trips,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 1) as pct,\n",
    "    ROUND(AVG(fare_amount), 2) as avg_fare,\n",
    "    ROUND(AVG(tip_amount), 2) as avg_tip\n",
    "FROM taxi_trips\n",
    "WHERE trip_distance > 0 AND fare_amount > 0\n",
    "GROUP BY 1\n",
    "ORDER BY MIN(trip_distance)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sql-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поездки в/из аэропортов (Airport_fee > 0)\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CASE WHEN Airport_fee > 0 THEN 'Airport Trip' ELSE 'Regular Trip' END as trip_type,\n",
    "    COUNT(*) as trips,\n",
    "    ROUND(AVG(trip_distance), 2) as avg_distance,\n",
    "    ROUND(AVG(fare_amount), 2) as avg_fare,\n",
    "    ROUND(AVG(total_amount), 2) as avg_total,\n",
    "    ROUND(AVG(tip_amount), 2) as avg_tip\n",
    "FROM taxi_trips\n",
    "GROUP BY 1\n",
    "ORDER BY trips DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "window-header",
   "metadata": {},
   "source": [
    "## 9. Оконные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кумулятивная выручка по месяцам\n",
    "monthly_cumulative = df.withColumn(\"month\", F.month(\"tpep_pickup_datetime\")) \\\n",
    "    .groupBy(\"month\") \\\n",
    "    .agg(F.round(F.sum(\"total_amount\") / 1_000_000, 2).alias(\"revenue_M\")) \\\n",
    "    .withColumn(\"cumulative_revenue_M\", \n",
    "        F.sum(\"revenue_M\").over(Window.orderBy(\"month\"))\n",
    "    ) \\\n",
    "    .withColumn(\"month_over_month_pct\",\n",
    "        F.round(\n",
    "            (F.col(\"revenue_M\") - F.lag(\"revenue_M\").over(Window.orderBy(\"month\"))) / \n",
    "            F.lag(\"revenue_M\").over(Window.orderBy(\"month\")) * 100, 1\n",
    "        )\n",
    "    ) \\\n",
    "    .orderBy(\"month\")\n",
    "\n",
    "monthly_cumulative.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ранжирование локаций по месяцам\n",
    "location_monthly = df.withColumn(\"month\", F.month(\"tpep_pickup_datetime\")) \\\n",
    "    .groupBy(\"month\", \"PULocationID\") \\\n",
    "    .agg(F.count(\"*\").alias(\"trips\")) \\\n",
    "    .withColumn(\"rank\", \n",
    "        F.rank().over(Window.partitionBy(\"month\").orderBy(F.desc(\"trips\")))\n",
    "    ) \\\n",
    "    .filter(F.col(\"rank\") <= 3) \\\n",
    "    .orderBy(\"month\", \"rank\")\n",
    "\n",
    "print(\"Топ-3 локации по месяцам:\")\n",
    "location_monthly.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dgih1i7maxe",
   "source": "## 10. pandas API on Spark\n\npandas API on Spark (бывший Koalas) позволяет использовать привычный pandas синтаксис для распределенных вычислений.\n\n### ⚠️ Ограничение Spark Connect с TimestampNTZType\n\nВ Spark Connect есть известная проблема: `ps.read_parquet()` и `df.pandas_api()` вызывают `AssertionError` при наличии колонок типа `TimestampNTZType`.\n\n**Пример ошибки:**\n```\nAssertionError: [InternalField(dtype=datetime64[us], struct_field=StructField('tpep_pickup_datetime', TimestampNTZType(), True))...]\n```\n\n**Проблемные колонки в NYC Taxi:** `tpep_pickup_datetime`, `tpep_dropoff_datetime`\n\n### Workaround: исключение timestamp колонок\n\n**Вариант 1: Выбор числовых колонок**\n```python\n# Читаем через Spark DataFrame (работает со всеми типами)\ndf = spark.read.parquet(\"s3a://raw-data/nyc-taxi/*.parquet\")\n\n# Выбираем только числовые колонки\nnumeric_cols = ['trip_distance', 'fare_amount', 'tip_amount', 'total_amount', 'payment_type']\ndf_numeric = df.select(numeric_cols)\n\n# Конвертируем в pandas-on-Spark\npsdf = df_numeric.pandas_api()\n```\n\n**Вариант 2: Извлечение компонентов даты до конвертации**\n```python\nfrom pyspark.sql import functions as F\n\n# Извлекаем час, день недели и т.д. как числа\ndf_with_time = df.select(\n    F.hour(\"tpep_pickup_datetime\").alias(\"pickup_hour\"),\n    F.dayofweek(\"tpep_pickup_datetime\").alias(\"pickup_dow\"),\n    F.month(\"tpep_pickup_datetime\").alias(\"pickup_month\"),\n    \"trip_distance\", \"fare_amount\", \"total_amount\"\n)\n\n# Теперь можно конвертировать\npsdf = df_with_time.pandas_api()\n```\n\n**Вариант 3: Для анализа с датами - использовать Spark DataFrame API**\n```python\n# Группировка по дате через Spark (работает без ограничений)\ndaily = df.groupBy(F.to_date(\"tpep_pickup_datetime\").alias(\"date\")) \\\n    .agg(F.count(\"*\").alias(\"trips\"))\n\n# Результат агрегации уже без TimestampNTZ - можно конвертировать\ndaily_psdf = daily.pandas_api()\n```\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qottyeyz3w",
   "source": "import pyspark.pandas as ps\n\n# Настройка для лучшей производительности\nps.set_option('compute.default_index_type', 'distributed')\nps.set_option('compute.ops_on_diff_frames', True)\n\n# Вариант 1: Только числовые колонки\nnumeric_cols = [\n    'VendorID', 'passenger_count', 'trip_distance', 'RatecodeID',\n    'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount',\n    'extra', 'mta_tax', 'tip_amount', 'tolls_amount', \n    'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee'\n]\ndf_numeric = df.select(numeric_cols)\nprint(f\"Вариант 1: {len(numeric_cols)} числовых колонок (без timestamp)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "el6ioz5bq2",
   "source": "# Вариант 2: Извлечение компонентов даты как числа\ndf_with_time = df.select(\n    F.hour(\"tpep_pickup_datetime\").alias(\"pickup_hour\"),\n    F.dayofweek(\"tpep_pickup_datetime\").alias(\"pickup_dow\"),\n    F.month(\"tpep_pickup_datetime\").alias(\"pickup_month\"),\n    F.to_date(\"tpep_pickup_datetime\").cast(\"string\").alias(\"pickup_date\"),  # дата как строка\n    \"trip_distance\", \"fare_amount\", \"tip_amount\", \"total_amount\", \"payment_type\"\n)\nprint(\"Вариант 2: timestamp -> числовые компоненты (hour, dow, month) + date как string\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3qu6p3q2fj2",
   "source": "# Конвертация в pandas-on-Spark (используем вариант 2 с временными компонентами)\npsdf = df_with_time.pandas_api()\n\nprint(f\"Тип: {type(psdf)}\")\nprint(f\"Колонки: {list(psdf.columns)}\")\nprint()\npsdf.head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6tyxot39wx3",
   "source": "# describe() - статистика по всем 40M+ записям, распределенно\npsdf[['trip_distance', 'fare_amount', 'tip_amount', 'total_amount']].describe()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wgljwislzxh",
   "source": "# Теперь можно использовать временные компоненты в pandas-стиле!\n\n# Анализ по часам (pandas groupby)\nhourly_stats = psdf.groupby('pickup_hour').agg({\n    'total_amount': 'mean',\n    'trip_distance': 'mean',\n    'tip_amount': 'mean'\n}).round(2)\n\nprint(\"Средние показатели по часам дня:\")\nhourly_stats",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "tuuhavjnlz",
   "source": "# Анализ по дню недели (1=Вс, 2=Пн, ..., 7=Сб)\ndow_stats = psdf.groupby('pickup_dow').agg({\n    'total_amount': ['mean', 'count'],\n    'tip_amount': 'mean'\n})\n\nprint(\"Статистика по дням недели:\")\ndow_stats",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gkjjebsgsqb",
   "source": "# Анализ по дате (pickup_date как строка работает!)\ndaily_revenue = psdf.groupby('pickup_date')['total_amount'].sum().sort_index()\n\nprint(\"Выручка по дням (первые 10):\")\ndaily_revenue.head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "q4y36eyvyz",
   "source": "# Фильтрация и статистика - pandas синтаксис\nprint(\"describe() - статистика по 40M+ записям:\")\npsdf[['trip_distance', 'fare_amount', 'tip_amount', 'total_amount']].describe()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gbhjg0sv73k",
   "source": "# Фильтрация - pandas синтаксис, распределенное выполнение\nmorning_rush = psdf[(psdf['pickup_hour'] >= 7) & (psdf['pickup_hour'] <= 9)]\nevening_rush = psdf[(psdf['pickup_hour'] >= 17) & (psdf['pickup_hour'] <= 19)]\n\nprint(f\"Утренний час пик (7-9): {len(morning_rush):,} поездок\")\nprint(f\"Вечерний час пик (17-19): {len(evening_rush):,} поездок\")\nprint()\nprint(\"Сравнение:\")\nprint(f\"  Утро - средняя сумма: ${morning_rush['total_amount'].mean():.2f}\")\nprint(f\"  Вечер - средняя сумма: ${evening_rush['total_amount'].mean():.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": "## 11. Сохранение результатов в S3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение месячной агрегации\n",
    "monthly.write.mode(\"overwrite\").parquet(\"s3a://warehouse/aggregated/nyc-taxi-monthly\")\n",
    "print(\"Saved: s3a://warehouse/aggregated/nyc-taxi-monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-hourly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение почасовой агрегации\n",
    "hourly.write.mode(\"overwrite\").parquet(\"s3a://warehouse/aggregated/nyc-taxi-hourly\")\n",
    "print(\"Saved: s3a://warehouse/aggregated/nyc-taxi-hourly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-partitioned",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение партиционированных данных (sample)\n",
    "# Берем 1% данных для примера партиционирования\n",
    "df_sample = df.sample(fraction=0.01) \\\n",
    "    .withColumn(\"year\", F.year(\"tpep_pickup_datetime\")) \\\n",
    "    .withColumn(\"month\", F.month(\"tpep_pickup_datetime\"))\n",
    "\n",
    "df_sample.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"s3a://warehouse/processed/nyc-taxi-partitioned\")\n",
    "\n",
    "print(\"Saved: s3a://warehouse/processed/nyc-taxi-partitioned (partitioned by year/month)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ui-header",
   "metadata": {},
   "source": "## 12. Работа с Spark UI\n\n### Доступ к Spark UI\n\n**Docker Compose:** http://localhost:4040\n\n**Kubernetes:**\n```bash\nkubectl port-forward -n spark svc/spark-connect 4040:4040\n# Затем открыть http://localhost:4040\n```\n\n### Вкладки Spark UI\n\n| Вкладка | Описание |\n|---------|----------|\n| **Jobs** | Список всех jobs и их статус |\n| **Stages** | Детали stages (tasks, input/output size, shuffle) |\n| **Storage** | Кэшированные RDD/DataFrames |\n| **Environment** | Конфигурация Spark |\n| **Executors** | Метрики executors (memory, GC, tasks) |\n| **SQL** | Query plans для SQL/DataFrame операций |\n\n### Полезные метрики в SQL вкладке:\n- **Duration** - время выполнения запроса\n- **Rows output** - количество строк на каждом этапе\n- **Data size** - объем данных\n- **Physical Plan** - план выполнения (кликните на query)\n\n### Spark History Server\n\n**Docker Compose:** http://localhost:18080\n\n**Kubernetes:**\n```bash\nkubectl port-forward -n spark svc/spark-history-server 18080:18080\n# Затем открыть http://localhost:18080\n```\n\nПосле завершения приложения (`spark.stop()`) можно просмотреть историю выполненных jobs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просмотр плана выполнения\n",
    "complex_query = df.filter(F.col(\"fare_amount\") > 10) \\\n",
    "    .groupBy(F.month(\"tpep_pickup_datetime\").alias(\"month\")) \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trips\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_total\")\n",
    "    ) \\\n",
    "    .orderBy(\"month\")\n",
    "\n",
    "# Логический план\n",
    "print(\"=== Logical Plan ===\")\n",
    "complex_query.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explain-extended",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расширенный план (физический)\n",
    "print(\"=== Physical Plan ===\")\n",
    "complex_query.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tips-header",
   "metadata": {},
   "source": "## 13. Tips: Spark Connect vs sparkmagic\n\n### Основные отличия\n\n| Аспект | sparkmagic | Spark Connect |\n|--------|------------|---------------|\n| Протокол | Livy REST API | gRPC |\n| Latency | Высокая (HTTP) | Низкая (gRPC) |\n| Сессия | Через Livy сервер | Прямое подключение |\n| Magic commands | `%%spark`, `%%sql` | Нативный Python |\n\n### Эквивалентные операции\n\n**sparkmagic:**\n```python\n%%spark\ndf = spark.read.parquet(\"s3a://...\")\ndf.show()\n```\n\n**Spark Connect:**\n```python\ndf = spark.read.parquet(\"s3a://...\")\ndf.show()\n```\n\n**sparkmagic SQL:**\n```python\n%%sql\nSELECT * FROM table\n```\n\n**Spark Connect SQL:**\n```python\nspark.sql(\"SELECT * FROM table\").show()\n```\n\n### Ограничения Spark Connect\n\nНекоторые операции недоступны в Spark Connect:\n- `df.rdd` - RDD API не поддерживается\n- `spark.sparkContext` - SparkContext недоступен\n- UDF через `@udf` - используйте `spark.udf.register()`\n- Некоторые методы `df.pandas_api()` - используйте `pyspark.pandas` напрямую\n\n### Автоматическое подключение (как в sparkmagic)\n\nДля автоматического создания сессии при запуске ноутбука, см. `/home/jupyter/.ipython/profile_default/startup/00-spark.py`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "to-pandas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конвертация в pandas для локальной работы\n",
    "# ВНИМАНИЕ: загружает данные на клиент, используйте для небольших результатов\n",
    "\n",
    "# Безопасно - небольшой агрегат\n",
    "monthly_pd = monthly.toPandas()\n",
    "print(f\"Тип: {type(monthly_pd)}\")\n",
    "monthly_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка кэша\n",
    "df.unpersist()\n",
    "print(\"Cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stop",
   "metadata": {},
   "outputs": [],
   "source": "# Завершение сессии\n# Docker Compose: после этого приложение появится в History Server (http://localhost:18080)\nspark.stop()\nprint(\"Session stopped.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}