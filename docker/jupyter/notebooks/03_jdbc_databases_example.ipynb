{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с JDBC базами данных\n",
    "PostgreSQL, Oracle, Vertica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spark_config import get_spark_session, get_jdbc_url\n",
    "\n",
    "spark = get_spark_session(app_name=\"JDBCDemo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL connection\n",
    "pg_url = get_jdbc_url('postgres', host='postgres', port=5432, database='spark_db')\n",
    "pg_properties = {\n",
    "    \"user\": os.environ.get('PG_USER', 'spark'),\n",
    "    \"password\": os.environ.get('PG_PASSWORD', 'spark'),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "print(f\"PostgreSQL URL: {pg_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from PostgreSQL\n",
    "# df_pg = spark.read.jdbc(url=pg_url, table=\"my_table\", properties=pg_properties)\n",
    "# df_pg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to PostgreSQL\n",
    "# df.write.jdbc(url=pg_url, table=\"new_table\", mode=\"overwrite\", properties=pg_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle connection\n",
    "oracle_url = get_jdbc_url('oracle', host='oracle-db', port=1521, database='ORCL', service='ORCL')\n",
    "oracle_properties = {\n",
    "    \"user\": os.environ.get('ORACLE_USER', 'system'),\n",
    "    \"password\": os.environ.get('ORACLE_PASSWORD', 'oracle'),\n",
    "    \"driver\": \"oracle.jdbc.OracleDriver\"\n",
    "}\n",
    "\n",
    "print(f\"Oracle URL: {oracle_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Oracle with predicate pushdown\n",
    "# df_oracle = spark.read.jdbc(\n",
    "#     url=oracle_url,\n",
    "#     table=\"(SELECT * FROM orders WHERE order_date > DATE '2024-01-01') subq\",\n",
    "#     properties=oracle_properties\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertica connection\n",
    "vertica_url = get_jdbc_url('vertica', host='vertica-host', port=5433, database='analytics')\n",
    "vertica_properties = {\n",
    "    \"user\": os.environ.get('VERTICA_USER', 'dbadmin'),\n",
    "    \"password\": os.environ.get('VERTICA_PASSWORD', ''),\n",
    "    \"driver\": \"com.vertica.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "print(f\"Vertica URL: {vertica_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Vertica with partitioning for parallel reads\n",
    "# df_vertica = spark.read.jdbc(\n",
    "#     url=vertica_url,\n",
    "#     table=\"fact_sales\",\n",
    "#     column=\"id\",\n",
    "#     lowerBound=0,\n",
    "#     upperBound=1000000,\n",
    "#     numPartitions=10,\n",
    "#     properties=vertica_properties\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Универсальная функция чтения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_db(spark, db_type, table, partition_column=None, num_partitions=10, **conn_params):\n",
    "    \"\"\"\n",
    "    Universal function to read from different databases.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        db_type: 'postgres', 'oracle', or 'vertica'\n",
    "        table: table name or subquery\n",
    "        partition_column: column for parallel reads\n",
    "        num_partitions: number of partitions\n",
    "        **conn_params: host, port, database, user, password\n",
    "    \"\"\"\n",
    "    drivers = {\n",
    "        'postgres': 'org.postgresql.Driver',\n",
    "        'oracle': 'oracle.jdbc.OracleDriver',\n",
    "        'vertica': 'com.vertica.jdbc.Driver'\n",
    "    }\n",
    "    \n",
    "    url = get_jdbc_url(db_type, conn_params['host'], conn_params['port'], conn_params['database'])\n",
    "    properties = {\n",
    "        'user': conn_params['user'],\n",
    "        'password': conn_params['password'],\n",
    "        'driver': drivers[db_type]\n",
    "    }\n",
    "    \n",
    "    reader = spark.read\n",
    "    \n",
    "    if partition_column:\n",
    "        # Get bounds for partitioning\n",
    "        bounds_df = spark.read.jdbc(\n",
    "            url=url,\n",
    "            table=f\"(SELECT MIN({partition_column}) as min_val, MAX({partition_column}) as max_val FROM {table}) bounds\",\n",
    "            properties=properties\n",
    "        ).collect()[0]\n",
    "        \n",
    "        return reader.jdbc(\n",
    "            url=url,\n",
    "            table=table,\n",
    "            column=partition_column,\n",
    "            lowerBound=bounds_df['min_val'],\n",
    "            upperBound=bounds_df['max_val'],\n",
    "            numPartitions=num_partitions,\n",
    "            properties=properties\n",
    "        )\n",
    "    else:\n",
    "        return reader.jdbc(url=url, table=table, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
