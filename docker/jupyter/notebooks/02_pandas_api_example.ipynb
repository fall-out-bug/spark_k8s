{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark pandas API (бывший Koalas)\n",
    "Pandas-like интерфейс для работы с большими данными через Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_config import get_spark_session\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = get_spark_session(app_name=\"PandasAPIDemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas API on Spark\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "\n",
    "# Set default index type for better performance\n",
    "ps.set_option('compute.default_index_type', 'distributed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame - same syntax as pandas!\n",
    "psdf = ps.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'age': [34, 45, 29, 52, 38],\n",
    "    'city': ['Moscow', 'SPb', 'Moscow', 'SPb', 'Moscow'],\n",
    "    'salary': [100000, 120000, 90000, 150000, 110000]\n",
    "})\n",
    "\n",
    "psdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas-like operations - all work on distributed data!\n",
    "\n",
    "# Filtering\n",
    "adults = psdf[psdf['age'] > 35]\n",
    "print(\"People over 35:\")\n",
    "print(adults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy - works on cluster!\n",
    "city_stats = psdf.groupby('city').agg({\n",
    "    'salary': ['mean', 'sum', 'count'],\n",
    "    'age': 'mean'\n",
    "})\n",
    "print(\"Stats by city:\")\n",
    "print(city_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from S3\n",
    "# psdf = ps.read_csv(\"s3a://bucket/data.csv\")\n",
    "# psdf = ps.read_parquet(\"s3a://bucket/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to regular Spark DataFrame if needed\n",
    "spark_df = psdf.to_spark()\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to regular pandas (be careful with large data!)\n",
    "pandas_df = psdf.to_pandas()\n",
    "print(type(pandas_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
