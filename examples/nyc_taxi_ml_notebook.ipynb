{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi ML Pipeline - Jupyter + Spark Connect\n",
    "\n",
    "## End-to-End ML Pipeline with EDA, Feature Selection, and Model Training\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Spark Connect** connection for distributed data processing\n",
    "2. **EDA** - Exploratory data analysis with visualizations\n",
    "3. **Feature Selection** - Correlation analysis and feature importance\n",
    "4. **Model Training** - sklearn models with proper train/test split\n",
    "5. **Model Testing** - Cross-validation and performance metrics\n",
    "6. **Metrics Export** - Push metrics to Prometheus Pushgateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics export\n",
    "import requests\n",
    "from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Spark Connect\n",
    "    'spark_connect_host': os.environ.get('SPARK_CONNECT_HOST', 'scenario1-spark-35-connect'),\n",
    "    'spark_connect_port': os.environ.get('SPARK_CONNECT_PORT', '15002'),\n",
    "    \n",
    "    # MinIO\n",
    "    'minio_endpoint': os.environ.get('S3_ENDPOINT', 'http://minio.spark-infra.svc.cluster.local:9000'),\n",
    "    'minio_access_key': os.environ.get('AWS_ACCESS_KEY_ID', 'minioadmin'),\n",
    "    'minio_secret_key': os.environ.get('AWS_SECRET_ACCESS_KEY', 'minioadmin'),\n",
    "    \n",
    "    # Prometheus\n",
    "    'pushgateway_url': os.environ.get('PUSHGATEWAY_URL', 'http://prometheus-pushgateway.spark-operations:9091'),\n",
    "    \n",
    "    # ML\n",
    "    'test_size': 0.2,\n",
    "    'cv_folds': 5,\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # Experiment\n",
    "    'experiment_name': 'nyc_taxi_revenue_prediction',\n",
    "    'model_version': datetime.now().strftime('%Y%m%d_%H%M'),\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    if 'secret' not in k.lower():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark Connect Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session via Spark Connect\n",
    "connect_url = f\"sc://{CONFIG['spark_connect_host']}:{CONFIG['spark_connect_port']}\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .remote(connect_url) \\\n",
    "    .appName(f\"nyc-taxi-ml-{CONFIG['model_version']}\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", CONFIG['minio_endpoint']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", CONFIG['minio_access_key']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", CONFIG['minio_secret_key']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark App ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw taxi data from MinIO\n",
    "raw_path = \"s3a://nyc-taxi/raw/*.parquet\"\n",
    "\n",
    "print(f\"Loading data from {raw_path}...\")\n",
    "df_raw = spark.read.parquet(raw_path)\n",
    "\n",
    "# Record metrics\n",
    "total_records = df_raw.count()\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "\n",
    "# Schema overview\n",
    "print(\"\\nSchema:\")\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "print(\"Sample data:\")\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality: filter invalid records\n",
    "df_clean = df_raw.filter(\n",
    "    (F.col(\"trip_distance\") > 0) &\n",
    "    (F.col(\"trip_distance\") < 100) &  # Remove outliers\n",
    "    (F.col(\"fare_amount\") > 0) &\n",
    "    (F.col(\"total_amount\") > 0) &\n",
    "    (F.col(\"tpep_pickup_datetime\") >= \"2023-01-01\") &\n",
    "    (F.col(\"tpep_pickup_datetime\") < \"2025-01-01\")\n",
    ")\n",
    "\n",
    "clean_records = df_clean.count()\n",
    "print(f\"Clean records: {clean_records:,} ({100*clean_records/total_records:.1f}% of raw)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily statistics for EDA\n",
    "daily_stats = df_clean \\\n",
    "    .withColumn(\"pickup_date\", F.to_date(\"tpep_pickup_datetime\")) \\\n",
    "    .groupBy(\"pickup_date\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trip_count\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "        F.avg(\"fare_amount\").alias(\"avg_fare\")\n",
    "    ) \\\n",
    "    .orderBy(\"pickup_date\") \\\n",
    "    .toPandas()\n",
    "\n",
    "print(f\"Daily stats: {len(daily_stats)} days\")\n",
    "daily_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Daily trip count\n",
    "axes[0, 0].plot(daily_stats['pickup_date'], daily_stats['trip_count'])\n",
    "axes[0, 0].set_title('Daily Trip Count')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Daily revenue\n",
    "axes[0, 1].plot(daily_stats['pickup_date'], daily_stats['total_revenue'])\n",
    "axes[0, 1].set_title('Daily Revenue')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Trip count distribution\n",
    "axes[1, 0].hist(daily_stats['trip_count'], bins=50, edgecolor='black')\n",
    "axes[1, 0].set_title('Trip Count Distribution')\n",
    "axes[1, 0].set_xlabel('Trips per Day')\n",
    "\n",
    "# Revenue vs Distance\n",
    "axes[1, 1].scatter(daily_stats['avg_distance'], daily_stats['total_revenue'], alpha=0.5)\n",
    "axes[1, 1].set_title('Revenue vs Avg Distance')\n",
    "axes[1, 1].set_xlabel('Avg Distance (miles)')\n",
    "axes[1, 1].set_ylabel('Total Revenue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/eda_plots.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US Holidays\n",
    "HOLIDAYS_2023 = [\"2023-01-01\", \"2023-01-16\", \"2023-02-20\", \"2023-05-29\", \"2023-06-19\",\n",
    "                 \"2023-07-04\", \"2023-09-04\", \"2023-10-09\", \"2023-11-11\", \"2023-11-23\", \"2023-12-25\"]\n",
    "HOLIDAYS_2024 = [\"2024-01-01\", \"2024-01-15\", \"2024-02-19\", \"2024-05-27\", \"2024-06-19\",\n",
    "                 \"2024-07-04\", \"2024-09-02\", \"2024-10-14\", \"2024-11-11\", \"2024-11-28\", \"2024-12-25\"]\n",
    "ALL_HOLIDAYS = HOLIDAYS_2023 + HOLIDAYS_2024\n",
    "\n",
    "def add_features(df):\n",
    "    \"\"\"Add temporal, geospatial, and derived features.\"\"\"\n",
    "    \n",
    "    # Temporal features\n",
    "    df = df \\\n",
    "        .withColumn(\"pickup_date\", F.to_date(\"tpep_pickup_datetime\")) \\\n",
    "        .withColumn(\"hour_of_day\", F.hour(\"tpep_pickup_datetime\")) \\\n",
    "        .withColumn(\"day_of_week\", F.dayofweek(\"tpep_pickup_datetime\")) \\\n",
    "        .withColumn(\"day_of_month\", F.dayofmonth(\"tpep_pickup_datetime\")) \\\n",
    "        .withColumn(\"month\", F.month(\"tpep_pickup_datetime\")) \\\n",
    "        .withColumn(\"year\", F.year(\"tpep_pickup_datetime\")) \\\n",
    "        .withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin([1, 7]), 1).otherwise(0)) \\\n",
    "        .withColumn(\"is_rush_hour\", F.when(\n",
    "            (F.col(\"hour_of_day\").between(7, 9)) | (F.col(\"hour_of_day\").between(17, 19)), 1\n",
    "        ).otherwise(0)) \\\n",
    "        .withColumn(\"is_holiday\", F.when(F.col(\"pickup_date\").isin(ALL_HOLIDAYS), 1).otherwise(0)) \\\n",
    "        .withColumn(\"time_of_day\", \n",
    "            F.when(F.col(\"hour_of_day\").between(6, 11), \"morning\")\n",
    "            .when(F.col(\"hour_of_day\").between(12, 17), \"afternoon\")\n",
    "            .when(F.col(\"hour_of_day\").between(18, 21), \"evening\")\n",
    "            .otherwise(\"night\"))\n",
    "    \n",
    "    # Borough mapping (simplified)\n",
    "    df = df.withColumn(\"pickup_borough\",\n",
    "        F.when(F.col(\"PULocationID\").between(1, 10), \"Staten Island\")\n",
    "         .when(F.col(\"PULocationID\").between(11, 50), \"Brooklyn\")\n",
    "         .when(F.col(\"PULocationID\").between(51, 150), \"Queens\")\n",
    "         .when(F.col(\"PULocationID\").between(151, 220), \"Manhattan\")\n",
    "         .when(F.col(\"PULocationID\").between(221, 265), \"Bronx\")\n",
    "         .otherwise(\"Unknown\"))\n",
    "    \n",
    "    # Trip metrics\n",
    "    df = df \\\n",
    "        .withColumn(\"trip_duration\", \n",
    "            (F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\")) / 60) \\\n",
    "        .withColumn(\"avg_speed\", \n",
    "            F.when(F.col(\"trip_duration\") > 0, \n",
    "                   F.col(\"trip_distance\") / (F.col(\"trip_duration\") / 60)).otherwise(0)) \\\n",
    "        .withColumn(\"is_airport\", F.when(F.col(\"PULocationID\").isin([1, 132, 138]), 1).otherwise(0))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply features\n",
    "df_features = add_features(df_clean)\n",
    "print(\"Features added successfully\")\n",
    "df_features.select(\"pickup_date\", \"hour_of_day\", \"day_of_week\", \"is_weekend\", \n",
    "                  \"pickup_borough\", \"trip_distance\", \"trip_duration\", \"avg_speed\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to daily borough level for ML\n",
    "df_daily = df_features \\\n",
    "    .groupBy(\"pickup_date\", \"pickup_borough\") \\\n",
    "    .agg(\n",
    "        F.sum(\"total_amount\").alias(\"daily_revenue\"),\n",
    "        F.count(\"*\").alias(\"daily_trips\"),\n",
    "        F.avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "        F.avg(\"trip_duration\").alias(\"avg_duration\"),\n",
    "        F.avg(\"avg_speed\").alias(\"avg_speed\"),\n",
    "        F.sum(\"is_weekend\").alias(\"is_weekend\"),\n",
    "        F.sum(\"is_holiday\").alias(\"is_holiday\"),\n",
    "        F.sum(\"is_airport\").alias(\"airport_trips\")\n",
    "    ) \\\n",
    "    .orderBy(\"pickup_date\", \"pickup_borough\")\n",
    "\n",
    "# Convert to pandas for ML\n",
    "df_ml = df_daily.toPandas()\n",
    "df_ml['pickup_date'] = pd.to_datetime(df_ml['pickup_date'])\n",
    "df_ml = df_ml.sort_values(['pickup_borough', 'pickup_date'])\n",
    "\n",
    "print(f\"ML dataset: {len(df_ml)} rows\")\n",
    "print(f\"Date range: {df_ml['pickup_date'].min()} to {df_ml['pickup_date'].max()}\")\n",
    "print(f\"Boroughs: {df_ml['pickup_borough'].unique().tolist()}\")\n",
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lag features and rolling averages\n",
    "def add_time_series_features(df, target_cols=['daily_revenue', 'daily_trips']):\n",
    "    \"\"\"Add time series features: lags and rolling statistics.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for borough in df['pickup_borough'].unique():\n",
    "        mask = df['pickup_borough'] == borough\n",
    "        borough_data = df[mask].sort_values('pickup_date')\n",
    "        \n",
    "        for col in target_cols:\n",
    "            # Lag features\n",
    "            df.loc[mask, f'{col}_lag1'] = borough_data[col].shift(1).values\n",
    "            df.loc[mask, f'{col}_lag7'] = borough_data[col].shift(7).values\n",
    "            \n",
    "            # Rolling statistics\n",
    "            df.loc[mask, f'{col}_ma7'] = borough_data[col].rolling(7, min_periods=1).mean().values\n",
    "            df.loc[mask, f'{col}_std7'] = borough_data[col].rolling(7, min_periods=1).std().values\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_ml = add_time_series_features(df_ml)\n",
    "df_ml = df_ml.dropna()\n",
    "\n",
    "print(f\"After adding time series features: {len(df_ml)} rows\")\n",
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for feature selection\n",
    "numeric_cols = df_ml.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_cols = ['daily_revenue', 'daily_trips']\n",
    "feature_cols = [c for c in numeric_cols if c not in exclude_cols and 'lag' not in c and 'ma' not in c and 'std' not in c]\n",
    "feature_cols += [c for c in numeric_cols if 'lag' in c or 'ma' in c]\n",
    "\n",
    "# Correlation with target\n",
    "correlations = df_ml[feature_cols + ['daily_revenue']].corr()['daily_revenue'].drop('daily_revenue')\n",
    "correlations = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Feature correlations with daily_revenue:\")\n",
    "print(correlations.head(15))\n",
    "\n",
    "# Select top features\n",
    "SELECTED_FEATURES = correlations.head(10).index.tolist()\n",
    "print(f\"\\nSelected features ({len(SELECTED_FEATURES)}): {SELECTED_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df_ml[SELECTED_FEATURES + ['daily_revenue', 'daily_trips']].corr(), \n",
    "            annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/correlation_heatmap.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training with Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "def prepare_training_data(df, borough, target='daily_revenue'):\n",
    "    \"\"\"Prepare train/test split for a specific borough.\"\"\"\n",
    "    borough_df = df[df['pickup_borough'] == borough].copy()\n",
    "    borough_df = borough_df.sort_values('pickup_date')\n",
    "    \n",
    "    # Time-based split (last 20% for test)\n",
    "    split_idx = int(len(borough_df) * 0.8)\n",
    "    \n",
    "    X = borough_df[SELECTED_FEATURES].values\n",
    "    y = borough_df[target].values\n",
    "    \n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler\n",
    "\n",
    "# Test with one borough\n",
    "borough = 'Manhattan'\n",
    "X_train, X_test, y_train, y_test, scaler = prepare_training_data(df_ml, borough)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Target range: [{y_train.min():.2f}, {y_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison with cross-validation\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model with cross-validation and test metrics.\"\"\"\n",
    "    \n",
    "    # Cross-validation on training data\n",
    "    cv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "    cv_rmse = np.sqrt(-cv_scores).mean()\n",
    "    \n",
    "    # Train on full training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Test metrics\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    test_mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mape': test_mape,\n",
    "        'test_r2': test_r2,\n",
    "        'model_obj': model\n",
    "    }\n",
    "\n",
    "# Compare models\n",
    "models_to_test = [\n",
    "    (GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42), 'GradientBoosting'),\n",
    "    (RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42), 'RandomForest'),\n",
    "    (Ridge(alpha=1.0), 'Ridge'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for model, name in models_to_test:\n",
    "    result = evaluate_model(model, X_train, y_train, X_test, y_test, name)\n",
    "    results.append(result)\n",
    "    print(f\"{name}: CV_RMSE={result['cv_rmse']:.2f}, Test_RMSE={result['test_rmse']:.2f}, \"\n",
    "          f\"Test_MAPE={result['test_mape']:.4f}, Test_R2={result['test_r2']:.4f}\")\n",
    "\n",
    "# Select best model\n",
    "best_result = min(results, key=lambda x: x['test_rmse'])\n",
    "print(f\"\\nBest model: {best_result['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final models for all boroughs\n",
    "trained_models = {}\n",
    "training_results = []\n",
    "\n",
    "for borough in df_ml['pickup_borough'].unique():\n",
    "    if borough == 'Unknown':\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nTraining models for {borough}...\")\n",
    "    \n",
    "    # Revenue model\n",
    "    X_train, X_test, y_train, y_test, scaler = prepare_training_data(df_ml, borough, 'daily_revenue')\n",
    "    \n",
    "    if len(X_train) < 50:\n",
    "        print(f\"  Skipping {borough} - insufficient data ({len(X_train)} samples)\")\n",
    "        continue\n",
    "    \n",
    "    model_revenue = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "    model_revenue.fit(X_train, y_train)\n",
    "    \n",
    "    pred_revenue = model_revenue.predict(X_test)\n",
    "    mape_rev = mean_absolute_percentage_error(y_test, pred_revenue)\n",
    "    rmse_rev = np.sqrt(mean_squared_error(y_test, pred_revenue))\n",
    "    \n",
    "    # Trips model\n",
    "    X_train_t, X_test_t, y_train_t, y_test_t, scaler_t = prepare_training_data(df_ml, borough, 'daily_trips')\n",
    "    \n",
    "    model_trips = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "    model_trips.fit(X_train_t, y_train_t)\n",
    "    \n",
    "    pred_trips = model_trips.predict(X_test_t)\n",
    "    mape_trips = mean_absolute_percentage_error(y_test_t, pred_trips)\n",
    "    rmse_trips = np.sqrt(mean_squared_error(y_test_t, pred_trips))\n",
    "    \n",
    "    trained_models[borough] = {\n",
    "        'revenue_model': model_revenue,\n",
    "        'trips_model': model_trips,\n",
    "        'scaler': scaler,\n",
    "        'features': SELECTED_FEATURES\n",
    "    }\n",
    "    \n",
    "    training_results.append({\n",
    "        'borough': borough,\n",
    "        'mape_revenue': mape_rev,\n",
    "        'rmse_revenue': rmse_rev,\n",
    "        'mape_trips': mape_trips,\n",
    "        'rmse_trips': rmse_trips,\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test)\n",
    "    })\n",
    "    \n",
    "    print(f\"  Revenue: MAPE={mape_rev:.4f}, RMSE={rmse_rev:.2f}\")\n",
    "    print(f\"  Trips: MAPE={mape_trips:.4f}, RMSE={rmse_trips:.2f}\")\n",
    "\n",
    "# Results summary\n",
    "results_df = pd.DataFrame(training_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=CONFIG['minio_endpoint'],\n",
    "    aws_access_key_id=CONFIG['minio_access_key'],\n",
    "    aws_secret_access_key=CONFIG['minio_secret_key']\n",
    ")\n",
    "\n",
    "# Save each borough's models\n",
    "for borough, models in trained_models.items():\n",
    "    model_data = {\n",
    "        'revenue_model': models['revenue_model'],\n",
    "        'trips_model': models['trips_model'],\n",
    "        'scaler': models['scaler'],\n",
    "        'features': models['features'],\n",
    "        'borough': borough,\n",
    "        'version': CONFIG['model_version'],\n",
    "        'trained_at': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    model_bytes = joblib.dumps(model_data)\n",
    "    key = f\"taxi-predictor/{CONFIG['model_version']}/{borough.lower().replace(' ', '_')}.joblib\"\n",
    "    \n",
    "    s3.put_object(Bucket='ml-models', Key=key, Body=model_bytes)\n",
    "    print(f\"Saved {borough} model to s3a://ml-models/{key}\")\n",
    "\n",
    "# Save training results\n",
    "results_df.to_csv('/tmp/training_results.csv', index=False)\n",
    "s3.put_object(\n",
    "    Bucket='ml-models', \n",
    "    Key=f\"taxi-predictor/{CONFIG['model_version']}/training_results.csv\",\n",
    "    Body=results_df.to_csv(index=False)\n",
    ")\n",
    "print(f\"\\nSaved training results to s3a://ml-models/taxi-predictor/{CONFIG['model_version']}/training_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Push Metrics to Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_metrics_to_prometheus(results_df, config):\n",
    "    \"\"\"Push training metrics to Prometheus Pushgateway.\"\"\"\n",
    "    registry = CollectorRegistry()\n",
    "    \n",
    "    # Create metrics\n",
    "    mape_revenue_gauge = Gauge(\n",
    "        'ml_training_mape_revenue',\n",
    "        'MAPE for revenue prediction',\n",
    "        ['borough', 'experiment', 'version'],\n",
    "        registry=registry\n",
    "    )\n",
    "    \n",
    "    mape_trips_gauge = Gauge(\n",
    "        'ml_training_mape_trips',\n",
    "        'MAPE for trips prediction',\n",
    "        ['borough', 'experiment', 'version'],\n",
    "        registry=registry\n",
    "    )\n",
    "    \n",
    "    rmse_revenue_gauge = Gauge(\n",
    "        'ml_training_rmse_revenue',\n",
    "        'RMSE for revenue prediction',\n",
    "        ['borough', 'experiment', 'version'],\n",
    "        registry=registry\n",
    "    )\n",
    "    \n",
    "    rmse_trips_gauge = Gauge(\n",
    "        'ml_training_rmse_trips',\n",
    "        'RMSE for trips prediction',\n",
    "        ['borough', 'experiment', 'version'],\n",
    "        registry=registry\n",
    "    )\n",
    "    \n",
    "    train_samples_gauge = Gauge(\n",
    "        'ml_training_samples',\n",
    "        'Number of training samples',\n",
    "        ['borough', 'experiment', 'version'],\n",
    "        registry=registry\n",
    "    )\n",
    "    \n",
    "    # Set values\n",
    "    for _, row in results_df.iterrows():\n",
    "        labels = {\n",
    "            'borough': row['borough'].lower().replace(' ', '_'),\n",
    "            'experiment': config['experiment_name'],\n",
    "            'version': config['model_version']\n",
    "        }\n",
    "        \n",
    "        mape_revenue_gauge.labels(**labels).set(row['mape_revenue'])\n",
    "        mape_trips_gauge.labels(**labels).set(row['mape_trips'])\n",
    "        rmse_revenue_gauge.labels(**labels).set(row['rmse_revenue'])\n",
    "        rmse_trips_gauge.labels(**labels).set(row['rmse_trips'])\n",
    "        train_samples_gauge.labels(**labels).set(row['train_samples'])\n",
    "    \n",
    "    # Push to gateway\n",
    "    try:\n",
    "        push_to_gateway(\n",
    "            config['pushgateway_url'],\n",
    "            job=f\"nyc_taxi_ml_{config['model_version']}\",\n",
    "            registry=registry\n",
    "        )\n",
    "        print(f\"Metrics pushed to {config['pushgateway_url']}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not push metrics: {e}\")\n",
    "        return False\n",
    "\n",
    "# Push metrics\n",
    "push_metrics_to_prometheus(results_df, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"NYC TAXI ML PIPELINE - NOTEBOOK SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nExperiment: {CONFIG['experiment_name']}\")\n",
    "print(f\"Version: {CONFIG['model_version']}\")\n",
    "print(f\"\\nData:\")\n",
    "print(f\"  - Raw records: {total_records:,}\")\n",
    "print(f\"  - Clean records: {clean_records:,}\")\n",
    "print(f\"  - ML dataset rows: {len(df_ml)}\")\n",
    "print(f\"  - Features: {len(SELECTED_FEATURES)}\")\n",
    "print(f\"\\nModels trained: {len(trained_models)} boroughs\")\n",
    "print(f\"\\nResults:\")\n",
    "print(results_df[['borough', 'mape_revenue', 'mape_trips', 'train_samples']].to_string(index=False))\n",
    "print(f\"\\nModels saved to: s3a://ml-models/taxi-predictor/{CONFIG['model_version']}/\")\n",
    "print(f\"\\nMetrics pushed to: {CONFIG['pushgateway_url']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"Spark session closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
