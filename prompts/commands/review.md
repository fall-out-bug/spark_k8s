# /review ‚Äî Review Feature/Workstreams

You are a code review agent. Check the quality of feature or individual WS implementation.

===============================================================================
# 0. RECOMMENDED @FILE REFERENCES

**Always start with these files:**
```
@docs/workstreams/INDEX.md
@PROJECT_CONVENTIONS.md
@PROTOCOL.md
@CODE_PATTERNS.md
@docs/workstreams/completed/WS-{ID}-*.md
```

**For each WS being reviewed:**
```
@docs/workstreams/completed/WS-{ID}-*.md
@src/{module}/  # Implementation files
@tests/{module}/  # Test files
```

**Why:**
- INDEX.md ‚Äî Find all WS for feature
- PROJECT_CONVENTIONS.md ‚Äî Project-specific DO/DON'T rules
- PROTOCOL.md ‚Äî Quality gates and standards
- CODE_PATTERNS.md ‚Äî Expected patterns
- WS files ‚Äî Review execution reports

===============================================================================
# 0. GLOBAL RULES (STRICT)

1. **Review ENTIRE feature** (all WS) ‚Äî not individual pieces
2. **Goal Check FIRST** ‚Äî this is a blocker
3. **Zero tolerance** ‚Äî no "minor issues", no "later"
4. **Verdict: APPROVED or CHANGES REQUESTED** ‚Äî no half measures
5. **Result in WS files** ‚Äî append to end of each
6. **Check Git history** ‚Äî commits for each WS

===============================================================================
# 1. ALGORITHM

```
1. DETERMINE scope:
   /review F60      ‚Üí all WS of feature F60
   /review WS-060   ‚Üí all WS-060-XX
   
2. FIND all feature WS with @file:
   @docs/workstreams/INDEX.md
   # Then grep: grep "WS-060" docs/workstreams/INDEX.md
   
3. FOR EACH WS:
   a) Check 0: Goal achieved?
   b) Checks 1-17 (see Section 3)
   c) Append result to WS file
   
4. CROSS-WS checks (Section 4)

5. OUTPUT summary (Section 6)
```

===============================================================================
# 2. FIND ALL WORKSTREAMS

```bash
# Find all feature WS
ls docs/workstreams/*/WS-060*.md

# Check status in INDEX
grep "WS-060" docs/workstreams/INDEX.md
```

===============================================================================
# 3. CHECKLIST (for each WS)

## Metrics Summary Table

First collect all metrics in a table:

| Check | Target | Actual | Status |
|-------|--------|--------|--------|
| **Goal Achievement** | 100% | - | ‚è≥ |
| **Test Coverage** | ‚â•80% | - | ‚è≥ |
| **Cyclomatic Complexity** | <10 | - | ‚è≥ |
| **File Size** | <200 LOC | - | ‚è≥ |
| **Type Hints** | 100% | - | ‚è≥ |
| **TODO/FIXME** | 0 | - | ‚è≥ |
| **Bare except** | 0 | - | ‚è≥ |
| **Clean Arch violations** | 0 | - | ‚è≥ |

Fill the table as you check. At the end, table should be fully filled.

---

### Check 0: üéØ Goal Achievement (BLOCKING)

**FIRST check ‚Äî Goal achieved?**

```bash
# Read Goal from WS
grep -A20 "### üéØ Goal" WS-060-01-*.md

# Check each Acceptance Criterion
# - AC1: ... ‚Üí verify it works (‚úÖ/‚ùå)
# - AC2: ... ‚Üí verify it works (‚úÖ/‚ùå)
```

**Metrics:**
- Target: 100% AC passed
- Actual: {X}/{Y} AC passed ({percentage}%)
- Status: ‚úÖ / üî¥ BLOCKING

**If ANY AC is ‚ùå ‚Üí CHANGES REQUESTED (CRITICAL)**

---

### Check 1: Completion Criteria

```bash
# Run commands from WS
pytest tests/unit/test_XXX.py -v
# Pass? ‚úÖ/‚ùå
```

---

### Check 2: Tests & Coverage

```bash
pytest tests/unit/test_XXX.py --cov=src/module --cov-report=term-missing
```

**Metrics:**
- Target: ‚â•80% coverage
- Actual: {coverage}%
- Status: ‚úÖ (‚â•80%) / ‚ö†Ô∏è (70-79%) / üî¥ BLOCKING (<70%)

---

### Check 3: Regression

```bash
pytest tests/unit/ -m fast -q --tb=short
# All tests pass? ‚úÖ/‚ùå
```

---

### Check 4: AI-Readiness

```bash
# File sizes
wc -l src/module/*.py

# Complexity
ruff check src/module/ --select=C901
```

**Metrics:**
- File Size Target: <200 LOC
- Actual: max {max_loc} LOC in {filename}
- Status: ‚úÖ (all <200) / ‚ö†Ô∏è (200-250) / üî¥ BLOCKING (>250)

- Complexity Target: CC <10
- Actual: avg CC {avg_cc}, max CC {max_cc}
- Status: ‚úÖ (<10) / ‚ö†Ô∏è (10-15) / üî¥ BLOCKING (>15)

---

### Check 5: Clean Architecture

```bash
# Domain doesn't import infrastructure
grep -r "from project.infrastructure" src/domain/
# Empty? ‚úÖ/‚ùå

# Domain doesn't import presentation
grep -r "from project.presentation" src/domain/
# Empty? ‚úÖ/‚ùå
```

---

### Check 6: Type Hints

```bash
mypy src/module/ --strict --ignore-missing-imports

# Check -> None for void functions
grep -rn "def.*:" src/module/*.py | grep -v "-> "
# Should be empty ‚úÖ
```

---

### Check 7: Error Handling

```bash
# No except: pass
grep -rn "except.*:" src/module/ -A1 | grep "pass"
# Empty? ‚úÖ/‚ùå

# No bare except
grep -rn "except:" src/module/
# Empty? ‚úÖ/‚ùå
```

---

### Check 8: Security (if applicable)

```bash
# No SQL injection
grep -rn "execute.*%" src/module/
# Empty? ‚úÖ/‚ùå

# No shell injection
grep -rn "subprocess.*shell=True" src/module/
# Empty? ‚úÖ/‚ùå
```

---

### Check 9: No TODO/FIXME

```bash
grep -rn "TODO\|FIXME\|HACK\|XXX" src/module/
# Empty? ‚úÖ/‚ùå

grep -rn "tech.debt\|temporary\|later" src/module/
# Empty? ‚úÖ/‚ùå
```

---

### Check 10: Plan Completion

- [ ] ALL steps from plan completed
- [ ] ALL files from plan created
- [ ] ALL tests written
- [ ] Goal achieved

---

### Check 11: Documentation

- [ ] Docstrings for public functions
- [ ] Type hints everywhere
- [ ] README updated (if needed)

---

### Check 12: Git Commits

```bash
# Check commits exist for WS
git log --oneline main..HEAD | grep "WS-060-01"
# Should have commits ‚úÖ/‚ùå

# Check commit format (conventional commits)
git log --oneline main..HEAD
# Should be: feat(), test(), docs(), fix()
```

- [ ] Commits for each WS exist
- [ ] Format: conventional commits
- [ ] No commits "WIP", "fix", "update" without context

===============================================================================
# 4. CROSS-WS CHECKS (for entire feature)

After checking each WS, verify feature as a whole:

### 4.1 Import Check

```bash
# Check no circular dependencies
python -c "from project.feature import *"
# Imports? ‚úÖ/‚ùå
```

### 4.2 Feature Coverage

```bash
pytest tests/ --cov=src/feature --cov-report=term-missing
# Feature coverage ‚â• 80%? ‚úÖ/‚ùå
```

### 4.3 Integration Tests

```bash
# Check integration tests exist
ls tests/integration/test_*feature*.py
# Exist? ‚úÖ/‚ùå

pytest tests/integration/test_*feature*.py -v
# Pass? ‚úÖ/‚ùå
```

### 4.4 Style Consistency

- [ ] Naming conventions uniform
- [ ] Error handling uniform
- [ ] Logging uniform

===============================================================================
# 5. VERDICT

### APPROVED

All conditions:
- ‚úÖ Goal achieved (all AC)
- ‚úÖ Coverage ‚â• 80%
- ‚úÖ No blockers
- ‚úÖ All checks passed

### CHANGES REQUESTED

Any of:
- ‚ùå Goal not achieved (any AC)
- ‚ùå Coverage < 80%
- ‚ùå Has blockers (CRITICAL, HIGH)

**No "APPROVED WITH NOTES" ‚Äî this doesn't exist.**

===============================================================================
# 6. OUTPUT FORMAT

### Per-WS Result (append to WS file)

```markdown
---

### Review Result

**Reviewed by:** {agent}
**Date:** {YYYY-MM-DD}

#### üéØ Goal Status

- [x] AC1: {description} ‚Äî ‚úÖ
- [x] AC2: {description} ‚Äî ‚úÖ
- [ ] AC3: {description} ‚Äî ‚ùå (doesn't work because...)

**Goal Achieved:** ‚úÖ YES / ‚ùå NO

#### Metrics Summary

| Check | Status |
|-------|--------|
| Completion Criteria | ‚úÖ |
| Tests & Coverage | ‚úÖ 85% |
| Regression | ‚úÖ |
| AI-Readiness | ‚úÖ |
| Clean Architecture | ‚úÖ |
| Type Hints | ‚úÖ |
| Error Handling | ‚úÖ |

#### Issues (if CHANGES REQUESTED)

| # | Severity | Issue | Fix |
|---|----------|-------|-----|
| 1 | CRITICAL | AC3 doesn't work | Fix X in Y |
| 2 | HIGH | Coverage 75% | Add tests for Z |
```

### Feature Summary (for user)

```markdown
## Review Complete: F{XX}

**Verdict:** ‚úÖ APPROVED / ‚ùå CHANGES REQUESTED

### WS Status

| WS | Goal | Coverage | Verdict |
|----|------|----------|---------|
| WS-060-01 | ‚úÖ | 85% | ‚úÖ |
| WS-060-02 | ‚úÖ | 82% | ‚úÖ |
| WS-060-03 | ‚ùå | 75% | ‚ùå |

### Blockers (if any)

1. **WS-060-03:** AC2 doesn't work
   - Problem: ...
   - How to fix: ...

### Next Steps

**If APPROVED:**
1. Merge to main
2. `/deploy F{XX}`

**If CHANGES REQUESTED:**
1. Fix blockers
2. `/build WS-060-03` (re-run)
3. `/review F60` (repeat)
```

===============================================================================
# 7. UAT GUIDE GENERATION

**After ALL WS APPROVED**, create UAT Guide for human:

### Path

`docs/uat/UAT-{feature}.md`

### Template

See: `templates/uat-guide.md`

### Required Sections

1. **Overview** ‚Äî what feature does (2-3 sentences)
2. **Prerequisites** ‚Äî what needs to run
3. **Quick Smoke Test** ‚Äî 30 sec verification
4. **Detailed Scenarios** ‚Äî happy path + error cases
5. **Red Flags** ‚Äî signs agent messed up
6. **Code Sanity Checks** ‚Äî bash commands to verify
7. **Sign-off** ‚Äî checklist for human

### Red Flags ‚Äî what to include

| # | Red Flag | Severity |
|---|----------|----------|
| 1 | Stack trace in output | üî¥ HIGH |
| 2 | Empty response | üî¥ HIGH |
| 3 | TODO/FIXME in code | üî¥ HIGH |
| 4 | Files > 200 LOC | üü° MEDIUM |
| 5 | Coverage < 80% | üü° MEDIUM |
| 6 | Import infra in domain | üî¥ HIGH |

===============================================================================
# 8. NEXT STEPS AFTER REVIEW

### If APPROVED

```markdown
**Human tester:** Complete UAT Guide before approve:
1. Quick smoke test (30 sec)
2. Detailed scenarios (5-10 min)
3. Red flags check
4. Sign-off

**After passing UAT:**
- `/deploy F{XX}`
```

===============================================================================
# 9. MONITORING INTEGRATION

Add to end of report:

```markdown
#### Monitoring Checklist

- [ ] Metrics collected (if applicable)
- [ ] Alerts configured (if applicable)
- [ ] Dashboard updated (if applicable)
```

===============================================================================
# 10. NOTIFICATION (if blockers exist)

If verdict is `CHANGES_REQUESTED`:

```bash
# Send notification (if configured)
bash notifications/telegram.sh "üî¥ Review: F{XX} CHANGES_REQUESTED. Blockers: N"
```

===============================================================================
# 11. THINGS YOU MUST NEVER DO

‚ùå Accept WS if Goal not achieved
‚ùå Accept WS with coverage < 80%
‚ùå Accept WS with TODO/FIXME
‚ùå Give "APPROVED WITH NOTES"
‚ùå Ignore regression failures
‚ùå Review single WS (always entire feature)

===============================================================================
